<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Fangzh的个人博客 | 人工智能拯救世界</title>
  
  <subtitle>人工智能、人生感悟</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://fangzh.top/"/>
  <updated>2018-10-18T12:32:03.890Z</updated>
  <id>http://fangzh.top/</id>
  
  <author>
    <name>Fangzh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>吴恩达Coursera(DeepLearning.ai)笔记和作业汇总帖</title>
    <link href="http://fangzh.top/2018/dl-ai-summary/"/>
    <id>http://fangzh.top/2018/dl-ai-summary/</id>
    <published>2018-10-18T12:01:05.000Z</published>
    <updated>2018-10-18T12:32:03.890Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>吴恩达Coursera(DeepLearning.ai)笔记和作业汇总。</p><a id="more"></a><p>历时一个多月终于把NG的五门课全部学完并且做了作业和笔记了。这里汇总一下：</p><h1 id="第一门课：神经网络和深度学习"><a href="#第一门课：神经网络和深度学习" class="headerlink" title="第一门课：神经网络和深度学习"></a>第一门课：神经网络和深度学习</h1><p>主要讲了神经网络的基本概念，以及机器学习的梯度下降法，向量化，而后进入了浅层和深层神经网络的实现。</p><ul><li>前两周太简单了，在之前的机器学习课上NG全部都讲过了，这里就不做了。</li><li><p>第三周：主要是浅层神经网络的实现</p><ul><li>笔记：<a href="http://fangzh.top/2018/2018091215/">浅层神经网络</a></li><li>作业：<a href="http://fangzh.top/2018/2018091216/">浅层神经网络</a></li></ul></li><li><p>第四周：深层神经网络的实现</p><ul><li>笔记：<a href="http://fangzh.top/2018/2018091316/">深层神经网络</a></li><li>作业：<a href="http://fangzh.top/2018/2018091318/">深层神经网络</a><h1 id="第二门课：改善神经网络"><a href="#第二门课：改善神经网络" class="headerlink" title="第二门课：改善神经网络"></a>第二门课：改善神经网络</h1>介绍了改善神经网络的方法，如正则化，超参数调节，优化算法等。</li></ul></li><li><p>第一周：训练集的划分、正则化、dropout</p><ul><li>笔记：<a href="http://fangzh.top/2018/20180901513/">深度学习的实践层面</a></li><li>作业：<a href="http://fangzh.top/2018/20180901515/">深度学习的实践层面</a></li></ul></li><li><p>第二周：Mini-batch、Momentum、RMS、Adam、学习率衰减</p><ul><li>笔记：<a href="http://fangzh.top/2018/2018091621/">优化算法</a></li><li>作业：<a href="http://fangzh.top/2018/2018091711/">优化算法</a></li></ul></li><li><p>第三周：超参数的调试、BatchNorm、softmax</p><ul><li>笔记：<a href="http://fangzh.top/2018/2018091720/">超参数调试</a></li><li>作业：<a href="http://fangzh.top/2018/2018091810/">超参数调试</a></li></ul></li></ul><h1 id="第三门课：结构化机器学习项目"><a href="#第三门课：结构化机器学习项目" class="headerlink" title="第三门课：结构化机器学习项目"></a>第三门课：结构化机器学习项目</h1><p>主要讲了机器学习中的一些策略。</p><ul><li>第一周：ML策略、正交化、优化指标、数据集的划分、偏差<ul><li>笔记：<a href="http://fangzh.top/2018/2018092016/">机器学习策略(1)</a></li></ul></li><li>第二周：误差分析、数据不同分布、迁移学习、多任务、端到端<ul><li>笔记：<a href="http://fangzh.top/2018/2018092017/">机器学习策略(2)</a></li></ul></li></ul><h1 id="第四门课：卷积神经网络"><a href="#第四门课：卷积神经网络" class="headerlink" title="第四门课：卷积神经网络"></a>第四门课：卷积神经网络</h1><p>主要讲了神经网络的在图像上的非常重要的应用，卷积神经网络。</p><ul><li><p>第一周：padding、步长、池化、卷积</p><ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-4-1/">卷积神经网络</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-4-1h/">卷积神经网络</a></li></ul></li><li><p>第二周：一些重要的神经网络结构，VGG、ResNet、Inception等</p><ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-4-2/">深度卷积网络实例探究</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-4-2h/">深度卷积网络实例探究</a></li></ul></li><li><p>第三周：目标检测、Bounding Box、IOU、NMS</p><ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-4-3/">目标检测</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-4-3h/">目标检测</a></li></ul></li><li><p>第四周：人脸识别和神经风格转换</p><ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-4-4/">人脸识别和神经风格转换</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-4-4h/">人脸识别和神经风格转换</a></li></ul></li></ul><h1 id="第五门课：序列模型"><a href="#第五门课：序列模型" class="headerlink" title="第五门课：序列模型"></a>第五门课：序列模型</h1><p>主要讲了神经网络在语言领域的应用，用RNN模型</p><ul><li>第一周：介绍了基本的RNN、GRU、LSTM<ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-5-1/">循环神经网络</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-5-1h1/">构建RNN</a>、[字符级生成恐龙名字][<a href="http://fangzh.top/2018/dl-ai-5-1h2/]、[LSTM生成爵士乐](http://fangzh.top/2018/dl-ai-5-1h3/)">http://fangzh.top/2018/dl-ai-5-1h2/]、[LSTM生成爵士乐](http://fangzh.top/2018/dl-ai-5-1h3/)</a></li></ul></li><li>第二周：自然语言处理与词嵌入<ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-5-2/">自然语言处理与词嵌入</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-5-2h/">词向量运算和emoji表情包</a></li></ul></li><li>第三周：序列模型和注意力机制<ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-5-3/">序列模型和注意力机制</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-5-3h/">机器翻译和触发关键字</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;吴恩达Coursera(DeepLearning.ai)笔记和作业汇总。&lt;/p&gt;
    
    </summary>
    
      <category term="汇总帖" scheme="http://fangzh.top/categories/%E6%B1%87%E6%80%BB%E5%B8%96/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(5-3) -- 序列模型和注意力机制</title>
    <link href="http://fangzh.top/2018/dl-ai-5-3h/"/>
    <id>http://fangzh.top/2018/dl-ai-5-3h/</id>
    <published>2018-10-18T10:39:15.000Z</published>
    <updated>2018-10-18T11:48:12.439Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>这周作业分为了两部分：</p><ul><li>机器翻译</li><li>触发关键字</li></ul><h1 id="Part1：机器翻译"><a href="#Part1：机器翻译" class="headerlink" title="Part1：机器翻译"></a>Part1：机器翻译</h1><p>你将建立一个将人类可读日期（“2009年6月25日”）转换为机器可读日期（“2009-06-25”）的神经机器翻译（NMT）模型。 你将使用注意力机制来执行此操作，这是模型序列中最尖端的一个序列。</p><p>你将创建的模型可用于从一种语言翻译为另一种语言，如从英语翻译为印地安语。 但是，语言翻译需要大量的数据集，并且通常需要几天的GPU训练。 在不使用海量数据的情况下，为了让你有机会尝试使用这些模型，我们使用更简单的“日期转换”任务。</p><p>网络以各种可能格式（例如“1958年8月29日”，“03/30/1968”，“1987年6月24日”）写成的日期作为输入，并将它们转换成标准化的机器可读的日期（例如“1958 -08-29“，”1968-03-30“，”1987-06-24“），让网络学习以通用机器可读格式YYYY-MM-DD输出日期。</p><ul><li>X: 经过处理的训练集中人类可读日期，其中每个字符都替换为其在human_vocab中映射到的索引。 每个日期用特殊字符进一步填充为Tx长度。 X.shape =（m，Tx）</li><li>Y: 经过处理的训练集中机器可读日期，其中每个字符都替换为其在machine_vocab中映射到的索引。 你应该有Y.shape =（m，Ty）。</li><li>Xoh：X的one-hot向量，Xoh.shape = (m，Tx，len(human_vocab))</li><li>Yoh：Y的one-hot向量，Yoh.shape = (m，Tx，len(machine_vocab))</li></ul><h2 id="采用注意力机制的机器翻译"><a href="#采用注意力机制的机器翻译" class="headerlink" title="采用注意力机制的机器翻译"></a>采用注意力机制的机器翻译</h2><p>定义一些layers</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defined shared layers as global variables</span></span><br><span class="line">repeator = RepeatVector(Tx)</span><br><span class="line">concatenator = Concatenate(axis=<span class="number">-1</span>)</span><br><span class="line">densor1 = Dense(<span class="number">10</span>, activation = <span class="string">"tanh"</span>)</span><br><span class="line">densor2 = Dense(<span class="number">1</span>, activation = <span class="string">"relu"</span>)</span><br><span class="line">activator = Activation(softmax, name=<span class="string">'attention_weights'</span>) <span class="comment"># We are using a custom softmax(axis = 1) loaded in this notebook</span></span><br><span class="line">dotor = Dot(axes = <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>然后根据a 和 s 得到context</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm6agwq9j20yk0vwaeg.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: one_step_attention</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_step_attention</span><span class="params">(a, s_prev)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights</span></span><br><span class="line"><span class="string">    "alphas" and the hidden states "a" of the Bi-LSTM.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)</span></span><br><span class="line"><span class="string">    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    context -- context vector, input of the next (post-attetion) LSTM cell</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states "a" (≈ 1 line)</span></span><br><span class="line">    s_prev = repeator(s_prev)</span><br><span class="line">    <span class="comment"># Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)</span></span><br><span class="line">    concat = concatenator([a, s_prev])</span><br><span class="line">    <span class="comment"># Use densor1 to propagate concat through a small fully-connected neural network to compute the "intermediate energies" variable e. (≈1 lines)</span></span><br><span class="line">    e = densor1(concat)</span><br><span class="line">    <span class="comment"># Use densor2 to propagate e through a small fully-connected neural network to compute the "energies" variable energies. (≈1 lines)</span></span><br><span class="line">    energies = densor2(e)</span><br><span class="line">    <span class="comment"># Use "activator" on "energies" to compute the attention weights "alphas" (≈ 1 line)</span></span><br><span class="line">    alphas = activator(energies)</span><br><span class="line">    <span class="comment"># Use dotor together with "alphas" and "a" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)</span></span><br><span class="line">    context = dotor([alphas, a])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure><p>实现model()</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm6apu4fj21d417g0zz.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_a = <span class="number">32</span></span><br><span class="line">n_s = <span class="number">64</span></span><br><span class="line">post_activation_LSTM_cell = LSTM(n_s, return_state = <span class="keyword">True</span>)</span><br><span class="line">output_layer = Dense(len(machine_vocab), activation=softmax)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Tx -- length of the input sequence</span></span><br><span class="line"><span class="string">    Ty -- length of the output sequence</span></span><br><span class="line"><span class="string">    n_a -- hidden state size of the Bi-LSTM</span></span><br><span class="line"><span class="string">    n_s -- hidden state size of the post-attention LSTM</span></span><br><span class="line"><span class="string">    human_vocab_size -- size of the python dictionary "human_vocab"</span></span><br><span class="line"><span class="string">    machine_vocab_size -- size of the python dictionary "machine_vocab"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- Keras model instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the inputs of your model with a shape (Tx,)</span></span><br><span class="line">    <span class="comment"># Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)</span></span><br><span class="line">    X = Input(shape=(Tx, human_vocab_size))</span><br><span class="line">    s0 = Input(shape=(n_s,), name=<span class="string">'s0'</span>)</span><br><span class="line">    c0 = Input(shape=(n_s,), name=<span class="string">'c0'</span>)</span><br><span class="line">    s = s0</span><br><span class="line">    c = c0</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize empty list of outputs</span></span><br><span class="line">    outputs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)</span></span><br><span class="line">    a = Bidirectional(LSTM(n_a, return_sequences=<span class="keyword">True</span>))(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Iterate for Ty steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(Ty):</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)</span></span><br><span class="line">        context = one_step_attention(a ,s)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.B: Apply the post-attention LSTM cell to the "context" vector.</span></span><br><span class="line">        <span class="comment"># Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)</span></span><br><span class="line">        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)</span></span><br><span class="line">        out = output_layer(s)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.D: Append "out" to the "outputs" list (≈ 1 line)</span></span><br><span class="line">        outputs.append(out)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)</span></span><br><span class="line">    model = Model(inputs=[X,s0,c0], outputs=outputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h1 id="Part2-Trigger-Word-Detection"><a href="#Part2-Trigger-Word-Detection" class="headerlink" title="Part2:Trigger Word Detection"></a>Part2:Trigger Word Detection</h1><p>做触发关键字的检测。</p><p>X: 这里把每一段音频分为了10s，而10s内细分为了5511个小的片段，也就是Tx = 5511</p><p>Y: Ty = 1375，每个y都是一个布尔值，用来记录有没有收到触发关键字。</p><h2 id="生成一个训练示例"><a href="#生成一个训练示例" class="headerlink" title="生成一个训练示例"></a>生成一个训练示例</h2><p>这里把样本分为了三种，背景音乐，正向的音频，反向的音频，合成训练示例：</p><ul><li>随机选择一个10秒的背景音频剪辑</li><li>随机将0-4个正向音频片段插入此10秒剪辑中</li><li>随机将0-2个反向音频片段插入此10秒剪辑中</li></ul><p>合成后类似这样：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69v62hj20jo08sac1.jpg" alt=""></p><p>定义一个随机插入片段起始和终点位置的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_time_segment</span><span class="params">(segment_ms)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    segment_ms -- the duration of the audio clip in ms ("ms" stands for "milliseconds")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    segment_time -- a tuple of (segment_start, segment_end) in ms</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    segment_start = np.random.randint(low=<span class="number">0</span>, high=<span class="number">10000</span>-segment_ms)   <span class="comment"># Make sure segment doesn't run past the 10sec background </span></span><br><span class="line">    segment_end = segment_start + segment_ms - <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (segment_start, segment_end)</span><br></pre></td></tr></table></figure><p>然后需要判断在别的片段插入的时候，有没有被占用:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: is_overlapping</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_overlapping</span><span class="params">(segment_time, previous_segments)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Checks if the time of a segment overlaps with the times of existing segments.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    segment_time -- a tuple of (segment_start, segment_end) for the new segment</span></span><br><span class="line"><span class="string">    previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    True if the time segment overlaps with any of the existing segments, False otherwise</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    segment_start, segment_end = segment_time</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 line)</span></span><br><span class="line">    <span class="comment"># Step 1: Initialize overlap as a "False" flag. (≈ 1 line)</span></span><br><span class="line">    overlap = <span class="keyword">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: loop over the previous_segments start and end times.</span></span><br><span class="line">    <span class="comment"># Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines)</span></span><br><span class="line">    <span class="keyword">for</span> previous_start, previous_end <span class="keyword">in</span> previous_segments:</span><br><span class="line">        <span class="keyword">if</span> segment_start &lt;= previous_end <span class="keyword">and</span> segment_end &gt;= previous_start:</span><br><span class="line">            overlap = <span class="keyword">True</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> overlap</span><br></pre></td></tr></table></figure><p>生成input音频片段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: insert_audio_clip</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_audio_clip</span><span class="params">(background, audio_clip, previous_segments)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Insert a new audio segment over the background noise at a random time step, ensuring that the </span></span><br><span class="line"><span class="string">    audio segment does not overlap with existing segments.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    background -- a 10 second background audio recording.  </span></span><br><span class="line"><span class="string">    audio_clip -- the audio clip to be inserted/overlaid. </span></span><br><span class="line"><span class="string">    previous_segments -- times where audio segments have already been placed</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    new_background -- the updated background audio</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the duration of the audio clip in ms</span></span><br><span class="line">    segment_ms = len(audio_clip)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    <span class="comment"># Step 1: Use one of the helper functions to pick a random time segment onto which to insert </span></span><br><span class="line">    <span class="comment"># the new audio clip. (≈ 1 line)</span></span><br><span class="line">    segment_time = get_random_time_segment(segment_ms)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep </span></span><br><span class="line">    <span class="comment"># picking new segment_time at random until it doesn't overlap. (≈ 2 lines)</span></span><br><span class="line">    <span class="keyword">while</span> is_overlapping(segment_time,previous_segments):</span><br><span class="line">        segment_time = get_random_time_segment(segment_ms)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Add the new segment_time to the list of previous_segments (≈ 1 line)</span></span><br><span class="line">    previous_segments.append(segment_time)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 4: Superpose audio segment and background</span></span><br><span class="line">    new_background = background.overlay(audio_clip, position = segment_time[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> new_background, segment_time</span><br></pre></td></tr></table></figure><p>生成y标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: insert_ones</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_ones</span><span class="params">(y, segment_end_ms)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update the label vector y. The labels of the 50 output steps strictly after the end of the segment </span></span><br><span class="line"><span class="string">    should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the</span></span><br><span class="line"><span class="string">    50 followinf labels should be ones.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    y -- numpy array of shape (1, Ty), the labels of the training example</span></span><br><span class="line"><span class="string">    segment_end_ms -- the end time of the segment in ms</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    y -- updated labels</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># duration of the background (in terms of spectrogram time-steps)</span></span><br><span class="line">    segment_end_y = int(segment_end_ms * Ty / <span class="number">10000.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add 1 to the correct index in the background label (y)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(segment_end_y+<span class="number">1</span>, segment_end_y+<span class="number">51</span>):</span><br><span class="line">        <span class="keyword">if</span> i &lt; Ty:</span><br><span class="line">            y[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: create_training_example</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_training_example</span><span class="params">(background, activates, negatives)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a training example with a given background, activates, and negatives.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    background -- a 10 second background audio recording</span></span><br><span class="line"><span class="string">    activates -- a list of audio segments of the word "activate"</span></span><br><span class="line"><span class="string">    negatives -- a list of audio segments of random words that are not "activate"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    x -- the spectrogram of the training example</span></span><br><span class="line"><span class="string">    y -- the label at each time step of the spectrogram</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the random seed</span></span><br><span class="line">    np.random.seed(<span class="number">18</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make background quieter</span></span><br><span class="line">    background = background - <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Initialize y (label vector) of zeros (≈ 1 line)</span></span><br><span class="line">    y = np.zeros((<span class="number">1</span>, Ty))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Initialize segment times as empty list (≈ 1 line)</span></span><br><span class="line">    previous_segments = []</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Select 0-4 random "activate" audio clips from the entire list of "activates" recordings</span></span><br><span class="line">    number_of_activates = np.random.randint(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">    random_indices = np.random.randint(len(activates), size=number_of_activates)</span><br><span class="line">    random_activates = [activates[i] <span class="keyword">for</span> i <span class="keyword">in</span> random_indices]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines)</span></span><br><span class="line">    <span class="comment"># Step 3: Loop over randomly selected "activate" clips and insert in background</span></span><br><span class="line">    <span class="keyword">for</span> random_activate <span class="keyword">in</span> random_activates:</span><br><span class="line">        <span class="comment"># Insert the audio clip on the background</span></span><br><span class="line">        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)</span><br><span class="line">        <span class="comment"># Retrieve segment_start and segment_end from segment_time</span></span><br><span class="line">        segment_start, segment_end = segment_time</span><br><span class="line">        <span class="comment"># Insert labels in "y"</span></span><br><span class="line">        y = insert_ones(y, segment_end)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Select 0-2 random negatives audio recordings from the entire list of "negatives" recordings</span></span><br><span class="line">    number_of_negatives = np.random.randint(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">    random_indices = np.random.randint(len(negatives), size=number_of_negatives)</span><br><span class="line">    random_negatives = [negatives[i] <span class="keyword">for</span> i <span class="keyword">in</span> random_indices]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines)</span></span><br><span class="line">    <span class="comment"># Step 4: Loop over randomly selected negative clips and insert in background</span></span><br><span class="line">    <span class="keyword">for</span> random_negative <span class="keyword">in</span> random_negatives:</span><br><span class="line">        <span class="comment"># Insert the audio clip on the background </span></span><br><span class="line">        background, _ = insert_audio_clip(background, random_negative, previous_segments)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Standardize the volume of the audio clip </span></span><br><span class="line">    background = match_target_amplitude(background, <span class="number">-20.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Export new training example </span></span><br><span class="line">    file_handle = background.export(<span class="string">"train"</span> + <span class="string">".wav"</span>, format=<span class="string">"wav"</span>)</span><br><span class="line">    print(<span class="string">"File (train.wav) was saved in your directory."</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get and plot spectrogram of the new recording (background with superposition of positive and negatives)</span></span><br><span class="line">    x = graph_spectrogram(<span class="string">"train.wav"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x, y</span><br></pre></td></tr></table></figure><p>实现model()</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm6amjp4j21hc1jkn36.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function creating the model's graph in Keras.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the model's input data (using Keras conventions)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- Keras model instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    X_input = Input(shape = input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: CONV layer (≈4 lines)</span></span><br><span class="line">    X = Conv1D(filters=<span class="number">196</span>,kernel_size=<span class="number">15</span>,strides=<span class="number">4</span>)(X_input)                                 <span class="comment"># CONV1D</span></span><br><span class="line">    X = BatchNormalization()(X)                                 <span class="comment"># Batch normalization</span></span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)                                 <span class="comment"># ReLu activation</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                 <span class="comment"># dropout (use 0.8)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: First GRU Layer (≈4 lines)</span></span><br><span class="line">    X = GRU(units = <span class="number">128</span>, return_sequences = <span class="keyword">True</span>)(X)                                 <span class="comment"># GRU (use 128 units and return the sequences)</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                  <span class="comment"># dropout (use 0.8)</span></span><br><span class="line">    X = BatchNormalization()(X)                                 <span class="comment"># Batch normalization</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Second GRU Layer (≈4 lines)</span></span><br><span class="line">    X = GRU(units = <span class="number">128</span>, return_sequences = <span class="keyword">True</span>)(X)                                 <span class="comment"># GRU (use 128 units and return the sequences)</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                  <span class="comment"># dropout (use 0.8)</span></span><br><span class="line">    X = BatchNormalization()(X)                                 <span class="comment"># Batch normalization</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                 <span class="comment"># dropout (use 0.8)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 4: Time-distributed dense layer (≈1 line)</span></span><br><span class="line">    X = TimeDistributed(Dense(<span class="number">1</span>, activation = <span class="string">"sigmoid"</span>))(X) <span class="comment"># time distributed  (sigmoid)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    model = Model(inputs = X_input, outputs = X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>这里载入预训练好的模型，不需要自己训练那么久了，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = load_model(<span class="string">'./models/tr_model.h5'</span>)</span><br><span class="line">opt = Adam(lr=<span class="number">0.0001</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, decay=<span class="number">0.01</span>)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=opt, metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line">model.fit(X, Y, batch_size = <span class="number">5</span>, epochs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;这周作业分为了两部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;机器翻译&lt;/li&gt;
&lt;li&gt;触发关键字&lt;/
      
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(5-3) -- 序列模型和注意力机制</title>
    <link href="http://fangzh.top/2018/dl-ai-5-3/"/>
    <id>http://fangzh.top/2018/dl-ai-5-3/</id>
    <published>2018-10-18T10:39:10.000Z</published>
    <updated>2018-10-18T11:48:09.236Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><h1 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h1><p><strong>sequence to sequence 模型：</strong></p><p>sequence to sequence 模型最为常见的就是机器翻译，假如这里我们要将法语翻译成英文。</p><p>对于机器翻译的序列对序列模型，如果我们拥有大量的句子语料，则可以得到一个很有效的机器翻译模型。模型的前部分使用一个编码网络来对输入的法语句子进行编码，后半部分则使用一个解码网络来生成对应的英文翻译。网络结构如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69m7lkj20ky0aggm7.jpg" alt=""></p><p>还有输入图像，输出描述图片的句子的：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69wi4xj20n00br0up.jpg" alt=""></p><h1 id="挑选最可能的句子"><a href="#挑选最可能的句子" class="headerlink" title="挑选最可能的句子"></a>挑选最可能的句子</h1><p>机器翻译：条件语言模型</p><p>对于机器翻译来说和之前几节介绍的语言模型有很大的相似性但也有不同之处。</p><p>在语言模型中，我们通过估计句子的可能性，来生成新的句子。语言模型总是以零向量开始，也就是其第一个时间步的输入可以直接为零向量；</p><p>在机器翻译中，包含了编码网络和解码网络，其中解码网络的结构与语言模型的结构是相似的。机器翻译以句子中每个单词的一系列向量作为输入，所以相比语言模型来说，机器翻译可以称作条件语言模型，其输出的句子概率是相对于输入的条件概率。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69mfoyj20mu0addgm.jpg" alt=""></p><h1 id="集束搜索（Beam-search）"><a href="#集束搜索（Beam-search）" class="headerlink" title="集束搜索（Beam search）"></a>集束搜索（Beam search）</h1><p>Beam search 算法：</p><p>这里我们还是以法语翻译成英语的机器翻译为例：</p><ul><li><p>Step 1：对于我们的词汇表，我们将法语句子输入到编码网络中得到句子的编码，通过一个softmax层计算各个单词（词汇表中的所有单词）输出的概率值，通过设置集束宽度（beam width）的大小如3，我们则取前3个最大输出概率的单词，并保存起来。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69luhlj20mi0bvq3c.jpg" alt=""></p></li><li><p>Step 2：在第一步中得到的集束宽度的单词数，我们分别对第一步得到的每一个单词计算其与单词表中的所有单词组成词对的概率。并与第一步的概率相乘，得到第一和第二两个词对的概率。有3×10000个选择，（这里假设词汇表有10000个单词），最后再通过beam width大小选择前3个概率最大的输出对；</p></li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69x7nhj20mv0cpac9.jpg" alt=""></p><ul><li>Step 3~Step T：与Step2的过程是相似的，直到遇到句尾符号结束。</li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69pggbj20mo0cm759.jpg" alt=""></p><h1 id="集束搜索的改进"><a href="#集束搜索的改进" class="headerlink" title="集束搜索的改进"></a>集束搜索的改进</h1><p>上面的集束搜索有个问题，就是因为每一项的概率都很小，所以句子越长，概率越小，因此会倾向于选择比较短的句子，这样是不太好的。</p><p>首先，为了保证不会太小而导致数值下溢，先取对数，把连乘变成求和。</p><p>然后在前面加上一个系数</p><p>$$\frac{1}{T_{y}^{\alpha}}$$</p><p>当$\alpha$ 为 1 时，就表示概率为句子长度的平均；为0时，就表示没有系数；在这里一般取$\alpha = 0.7$</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69ohpnj20lp0camym.jpg" alt=""></p><p>集束搜索讨论：</p><p>Beam width：B的选择，B越大考虑的情况越多，但是所需要进行的计算量也就相应的越大。在常见的产品系统中，一般设置B = 10，而更大的值（如100，1000，…）则需要对应用的领域和场景进行选择。</p><p>相比于算法范畴中的搜索算法像BFS或者DFS这些精确的搜索算法，Beam Search 算法运行的速度很快，但是不能保证找到目标准确的最大值。</p><h1 id="集束搜索的误差分析"><a href="#集束搜索的误差分析" class="headerlink" title="集束搜索的误差分析"></a>集束搜索的误差分析</h1><p>集束搜索算法是一种近似搜索算法，也被称为启发式搜索算法。而不是一种精确的搜索。</p><p>如果我们的集束搜素算法出现错误了要怎么办呢？如何确定是算法出现了错误还是模型出现了错误呢？此时集束搜索算法的误差分析就显示出了作用。</p><p>模型分为两个部分：</p><ul><li>RNN 部分：编码网络 + 解码网络</li><li>Beam Search 部分：选取最大的几个值</li></ul><h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p>计算人类翻译的概率P(y∗|x)以及模型翻译的概率P(ŷ |x)</p><ul><li><p>P(y∗|x) &gt; P(ŷ |x)：Beam search算法选择了ŷ ，但是y∗ 却得到了更高的概率，所以Beam search 算法出错了；</p></li><li><p>P(y∗|x) &lt;= P(ŷ |x) 的情况：翻译结果y∗相比ŷ 要更好，但是RNN模型却预测P(y∗|x)</p></li></ul><h1 id="Bleu-得分（选修）"><a href="#Bleu-得分（选修）" class="headerlink" title="Bleu 得分（选修）"></a>Bleu 得分（选修）</h1><p>PASS</p><h1 id="注意力模型直观理解"><a href="#注意力模型直观理解" class="headerlink" title="注意力模型直观理解"></a>注意力模型直观理解</h1><p>之前我们的翻译模型分为编码网络和解码网络，先记忆整个句子再翻译，这对于较短的句子效果不错，但是对于很长的句子，翻译结果就会变差。</p><p>回想当我们人类翻译长句子时，都是一部分一部分的翻译，翻译每个部分的时候也会顾及到该部分周围上下文对其的影响。同理，引入注意力机制，一部分一部分的翻译，每次翻译时给该部分及上下文不同的注意力权重以及已经译出的部分，直至翻译出整个句子。</p><h1 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h1><p>以一个双向的RNN模型来对法语进行翻译，得到相应的英语句子。其中的每个RNN单元均是LSTM或者GRU单元。</p><p>对于双向RNN，通过前向和后向的传播，可以得到每个时间步的前向激活值和反向激活值，我们用一个符号来表示前向和反向激活值的组合。 </p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69oymvj205l01i742.jpg" alt=""></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69ohpnj20lp0camym.jpg" alt=""></p><p>然后得到每个输入单词的注意力权重：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69pircj20qx01w0sr.jpg" alt=""></p><p>计算公式为：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69rhu3j206q02sdfn.jpg" alt=""></p><p>这里的$e^{&lt;t,t^{\prime}>}$则是通过一层神经网络来进行计算得到的，其值取决于输出RNN中前一步的激活值$s^{&lt;t-1>}$和输入RNN当前步的激活值$a^{&lt;t^{\prime}>}$。我们可以通过训练这个小的神经网络模型，使用反向传播算法来学习一个对应的关系函数。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69rsmuj20lr0b23zg.jpg" alt=""></p><h1 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h1><p>语音识别就是将一段音频转化为相应文本。</p><p>之前用音位来识别，现在 end-to-end 模型中已经不需要音位了，但是需要大量的数据常见的语音数据大小为300h、3000h或者更大。</p><h4 id="注意力模型的语音识别"><a href="#注意力模型的语音识别" class="headerlink" title="注意力模型的语音识别"></a>注意力模型的语音识别</h4><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69x4ttj20l40b774l.jpg" alt=""></p><h4 id="CTC-损失函数的语音识别"><a href="#CTC-损失函数的语音识别" class="headerlink" title="CTC 损失函数的语音识别"></a>CTC 损失函数的语音识别</h4><p>另外一种效果较好的就是使用CTC损失函数的语音识别模型（CTC，Connectionist temporal classification）</p><p>模型会有很多个输入和输出，对于一个10s的语音片段，我们就能够得到1000个特征的输入片段，而往往我们的输出仅仅是几个单词。</p><p>在CTC损失函数中，允许RNN模型输出有重复的字符和插入空白符的方式，强制使得我们的输出和输入的大小保持一致。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69s111j20kn0bdjs1.jpg" alt=""></p><h1 id="触发字检测"><a href="#触发字检测" class="headerlink" title="触发字检测"></a>触发字检测</h1><p>触发字检测：关键词语音唤醒。</p><p>一种可以简单应用的触发字检测算法，就是使用RNN模型，将音频信号进行声谱图转化得到图像特征或者使用音频特征，输入到RNN中作为我们的输入。而输出的标签，我们可以以触发字前的输出都标记为0，触发字后的输出则标记为1。</p><p>一种简单应用的触发字检测算法，就是使用RNN模型，将音频信号进行声谱图转化音频特征，输入到RNN中作为我们的输入。而输出的标签，非触发字的输出都标记为0，触发字的输出则标记为1。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69uuhqj20lr0bfdh5.jpg" alt=""></p><p>上面方法的缺点就是0、1标签的不均衡，0比1多很多。一种简单粗暴的方法就是在触发字及其之后多个目标标签都标记为1，在一定程度上可以提高系统的精确度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;基础模型&quot;&gt;&lt;a href=&quot;#基础模型&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(5-2) -- 自然语言处理与词嵌入(NLP and Word Embeddings)</title>
    <link href="http://fangzh.top/2018/dl-ai-5-2h/"/>
    <id>http://fangzh.top/2018/dl-ai-5-2h/</id>
    <published>2018-10-18T09:00:21.000Z</published>
    <updated>2018-10-18T10:28:16.868Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>本周作业分为两部分：</p><ul><li>词向量运算</li><li>emoji表情包</li></ul><a id="more"></a><h1 id="Part1-词向量运算"><a href="#Part1-词向量运算" class="headerlink" title="Part1:词向量运算"></a>Part1:词向量运算</h1><p>由于词嵌入的训练计算量庞大切耗费时间长，绝大部分机器学习人员都会导入一个预训练的词嵌入模型。</p><p>本作业中，我们使用50维的 Glove 向量来表示词。导入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words, word_to_vec_map = read_glove_vecs(<span class="string">'data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure><ul><li>words: 词典中的词集合</li><li>word_to_vec_map: 表示单词到向量映射的map。</li></ul><p>one-hot向量不擅长表示向量相似度(内积为0), Glove 向量包含了单词更多的信息，下面看看如何使用 Glove 向量计算相似度。</p><p>$$\text{CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta)$$</p><p>分子表示两个向量的内积，分母是向量的模的乘积，θθ表示向量夹角，向量越近夹角越小，cos 值越大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: cosine_similarity</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine_similarity</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Cosine similarity reflects the degree of similariy between u and v</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        u -- a word vector of shape (n,)          </span></span><br><span class="line"><span class="string">        v -- a word vector of shape (n,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cosine_similarity -- the cosine similarity between u and v defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    distance = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Compute the dot product between u and v (≈1 line)</span></span><br><span class="line">    dot = np.dot(u,v)</span><br><span class="line">    <span class="comment"># Compute the L2 norm of u (≈1 line)</span></span><br><span class="line">    norm_u = np.sqrt(np.dot(u,u))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the L2 norm of v (≈1 line)</span></span><br><span class="line">    norm_v = np.sqrt(np.dot(v,v))</span><br><span class="line">    <span class="comment"># Compute the cosine similarity defined by formula (1) (≈1 line)</span></span><br><span class="line">    cosine_similarity = dot / (norm_u * norm_v)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cosine_similarity</span><br></pre></td></tr></table></figure><h2 id="单词类比推理"><a href="#单词类比推理" class="headerlink" title="单词类比推理"></a>单词类比推理</h2><p>类比推理任务中需要实现”a is to b as c is to __” 比如”man is to woman as king is to queen”。我们需要找到单词 d,使得”e_b−e_a ≈ e_d−e_c”<br>也就是两组的差向量应该相似(仍然用 cos 来衡量)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: complete_analogy</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_analogy</span><span class="params">(word_a, word_b, word_c, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs the word analogy task as explained above: a is to b as c is to ____. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_a -- a word, string</span></span><br><span class="line"><span class="string">    word_b -- a word, string</span></span><br><span class="line"><span class="string">    word_c -- a word, string</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary that maps words to their corresponding vectors. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert words to lower case</span></span><br><span class="line">    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Get the word embeddings v_a, v_b and v_c (≈1-3 lines)</span></span><br><span class="line">    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    words = word_to_vec_map.keys()</span><br><span class="line">    max_cosine_sim = <span class="number">-100</span>              <span class="comment"># Initialize max_cosine_sim to a large negative number</span></span><br><span class="line">    best_word = <span class="keyword">None</span>                   <span class="comment"># Initialize best_word with None, it will help keep track of the word to output</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over the whole word vector set</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:        </span><br><span class="line">        <span class="comment"># to avoid best_word being one of the input words, pass on them.</span></span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> [word_a, word_b, word_c] :</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        <span class="comment"># Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)</span></span><br><span class="line">        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the cosine_sim is more than the max_cosine_sim seen so far,</span></span><br><span class="line">            <span class="comment"># then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)</span></span><br><span class="line">        <span class="keyword">if</span> cosine_sim &gt; max_cosine_sim:</span><br><span class="line">            max_cosine_sim = cosine_sim</span><br><span class="line">            best_word = w</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> best_word</span><br></pre></td></tr></table></figure><h2 id="消除词向量偏见-可选"><a href="#消除词向量偏见-可选" class="headerlink" title="消除词向量偏见 (可选)"></a>消除词向量偏见 (可选)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neutralize</span><span class="params">(word, g, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Removes the bias of "word" by projecting it on the space orthogonal to the bias axis. </span></span><br><span class="line"><span class="string">    This function ensures that gender neutral words are zero in the gender subspace.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        word -- string indicating the word to debias</span></span><br><span class="line"><span class="string">        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)</span></span><br><span class="line"><span class="string">        word_to_vec_map -- dictionary mapping words to their corresponding vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        e_debiased -- neutralized word vector representation of the input "word"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line)</span></span><br><span class="line">    e = word_to_vec_map[word]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute e_biascomponent using the formula give above. (≈ 1 line)</span></span><br><span class="line">    e_biascomponent = np.dot(e, g) / np.square(np.linalg.norm(g)) * g</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Neutralize e by substracting e_biascomponent from it </span></span><br><span class="line">    <span class="comment"># e_debiased should be equal to its orthogonal projection. (≈ 1 line)</span></span><br><span class="line">    e_debiased = e - e_biascomponent</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e_debiased</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equalize</span><span class="params">(pair, bias_axis, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Debias gender specific words by following the equalize method described in the figure above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    pair -- pair of strings of gender specific words to debias, e.g. ("actress", "actor") </span></span><br><span class="line"><span class="string">    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their corresponding vectors</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    e_1 -- word vector corresponding to the first word</span></span><br><span class="line"><span class="string">    e_2 -- word vector corresponding to the second word</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines)</span></span><br><span class="line">    w1, w2 = pair</span><br><span class="line">    e_w1, e_w2 = word_to_vec_map[w1, w2]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)</span></span><br><span class="line">    mu = (e_w1 + e_w2) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)</span></span><br><span class="line">    mu_B = np.dot(mu, bias_axis) / np.square(np.linalg.norm(bias_axis)) * bias_axis</span><br><span class="line">    mu_orth = mu - mu_B</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)</span></span><br><span class="line">    e_w1B = np.dot(e_w1, bias_axis) / np.square(np.linalg.norm(bias_axis)) * bias_axis</span><br><span class="line">    e_w2B = np.dot(e_w2, bias_axis) / np.square(np.linalg.norm(bias_axis)) * bias_axis</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)</span></span><br><span class="line">    corrected_e_w1B = np.sqrt(np.abs(<span class="number">1</span>-np.sum(mu_orth**<span class="number">2</span>))) * (e_w1B - mu_B)/np.linalg.norm(e_w1-mu_orth-mu_B)</span><br><span class="line">    corrected_e_w2B = np.sqrt(np.abs(<span class="number">1</span>-np.sum(mu_orth**<span class="number">2</span>))) * (e_w2B - mu_B)/np.linalg.norm(e_w2-mu_orth-mu_B)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)</span></span><br><span class="line">    e1 = corrected_e_w1B + mu_orth</span><br><span class="line">    e2 = corrected_e_w2B + mu_orth</span><br><span class="line">                                                                </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e1, e2</span><br></pre></td></tr></table></figure><h1 id="Part2-Emojify"><a href="#Part2-Emojify" class="headerlink" title="Part2:Emojify!"></a>Part2:Emojify!</h1><p>你有没有想过让你的短信更具表现力？ emojifier APP将帮助你做到这一点。 所以不是写下”Congratulations on the promotion! Lets get coffee and talk. Love you!” emojifier可以自动转换为 “Congratulations on the promotion! ? Lets get coffee and talk. ☕️ Love you! ❤️”</p><p>另外，如果你对emojis不感兴趣，但有朋友向你发送了使用太多表情符号的疯狂短信，你还可以使用emojifier来回复他们。</p><p>你将实现一个模型，输入一个句子（“Let’s go see the baseball game tonight!”），并找到最适合这个句子的表情符号（⚾️）。 在许多表情符号界面中，您需要记住❤️是”heart”符号而不是”love”符号。 但是使用单词向量，你会发现即使你的训练集只将几个单词明确地与特定的表情符号相关联，你的算法也能够将测试集中相关的单词概括并关联到相同的表情符号上，即使这些词没有出现在训练集中。这使得即使使用小型训练集，你也可以建立从句子到表情符号的精确分类器映射。</p><p>在本练习中，您将从使用词嵌入的基本模型（Emojifier-V1）开始，然后构建进一步整合LSTM的更复杂的模型（Emojifier-V2）。</p><h2 id="先用average试试"><a href="#先用average试试" class="headerlink" title="先用average试试"></a>先用average试试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sentence_to_avg</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_to_avg</span><span class="params">(sentence, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word</span></span><br><span class="line"><span class="string">    and averages its value into a single vector encoding the meaning of the sentence.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    sentence -- string, one training example from X</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Split sentence into list of lower case words (≈ 1 line)</span></span><br><span class="line">    words = sentence.lower().split()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the average word vector, should have the same shape as your word vectors.</span></span><br><span class="line">    avg = np.zeros(word_to_vec_map[words[<span class="number">0</span>]].shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: average the word vectors. You can loop over the words in the list "words".</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        avg += word_to_vec_map[w]</span><br><span class="line">    avg = avg / len(words)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> avg</span><br></pre></td></tr></table></figure><h2 id="再用RNN"><a href="#再用RNN" class="headerlink" title="再用RNN"></a>再用RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, word_to_vec_map, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">400</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Model to train word vector representations in numpy.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, numpy array of sentences as strings, of shape (m, 1)</span></span><br><span class="line"><span class="string">    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    learning_rate -- learning_rate for the stochastic gradient descent algorithm</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    pred -- vector of predictions, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    W -- weight matrix of the softmax layer, of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">    b -- bias of the softmax layer, of shape (n_y,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define number of training examples</span></span><br><span class="line">    m = Y.shape[<span class="number">0</span>]                          <span class="comment"># number of training examples</span></span><br><span class="line">    n_y = <span class="number">5</span>                                 <span class="comment"># number of classes  </span></span><br><span class="line">    n_h = <span class="number">50</span>                                <span class="comment"># dimensions of the GloVe vectors </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters using Xavier initialization</span></span><br><span class="line">    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)</span><br><span class="line">    b = np.zeros((n_y,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Convert Y to Y_onehot with n_y classes</span></span><br><span class="line">    Y_oh = convert_to_one_hot(Y, C = n_y) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_iterations):                       <span class="comment"># Loop over the number of iterations</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                                <span class="comment"># Loop over the training examples</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">            <span class="comment"># Average the word vectors of the words from the i'th training example</span></span><br><span class="line">            avg = sentence_to_avg(X[i], word_to_vec_map)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagate the avg through the softmax layer</span></span><br><span class="line">            z = np.dot(W, avg) + b</span><br><span class="line">            a = softmax(z)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost using the i'th training label's one hot representation and "A" (the output of the softmax)</span></span><br><span class="line">            cost = -np.sum(Y_oh[i] * np.log(a))</span><br><span class="line">            <span class="comment">### END CODE HERE ###</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute gradients </span></span><br><span class="line">            dz = a - Y_oh[i]</span><br><span class="line">            dW = np.dot(dz.reshape(n_y,<span class="number">1</span>), avg.reshape(<span class="number">1</span>, n_h))</span><br><span class="line">            db = dz</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters with Stochastic Gradient Descent</span></span><br><span class="line">            W = W - learning_rate * dW</span><br><span class="line">            b = b - learning_rate * db</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch: "</span> + str(t) + <span class="string">" --- cost = "</span> + str(cost))</span><br><span class="line">            pred = predict(X, Y, W, b, word_to_vec_map)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pred, W, b</span><br></pre></td></tr></table></figure><h2 id="Emojifier-V2-Using-LSTMs-in-Keras"><a href="#Emojifier-V2-Using-LSTMs-in-Keras" class="headerlink" title="Emojifier-V2: Using LSTMs in Keras:"></a>Emojifier-V2: Using LSTMs in Keras:</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sentences_to_indices</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)</span></span><br><span class="line">    X_indices = np.zeros((m, max_len))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words =X[i].lower().split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] = word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: pretrained_embedding_layer</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    vocab_len = len(word_to_index) + <span class="number">1</span>                  <span class="comment"># adding 1 to fit Keras embedding (requirement)</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]      <span class="comment"># define dimensionality of your GloVe word vectors (= 50)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)</span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. </span></span><br><span class="line">    embedding_layer = Embedding(vocab_len,emb_dim, trainable=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure><h1 id="Building-the-Emojifier-V2"><a href="#Building-the-Emojifier-V2" class="headerlink" title="Building the Emojifier-V2"></a>Building the Emojifier-V2</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: Emojify_V2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Emojify_V2</span><span class="params">(input_shape, word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function creating the Emojify-v2 model's graph.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the input, usually (max_len,)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a model instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).</span></span><br><span class="line">    sentence_indices = Input(shape= input_shape, dtype=<span class="string">'int32'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span></span><br><span class="line">    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate sentence_indices through your embedding layer, you get back the embeddings</span></span><br><span class="line">    embeddings = embedding_layer(sentence_indices)   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a batch of sequences.</span></span><br><span class="line">    X = LSTM(<span class="number">128</span>, return_sequences=<span class="keyword">True</span>)(embeddings)</span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X trough another LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a single hidden state, not a batch of sequences.</span></span><br><span class="line">    X = LSTM(<span class="number">128</span>, return_sequences=<span class="keyword">False</span>)(X)</span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span></span><br><span class="line">    X =  Dense(<span class="number">5</span>, activation=<span class="string">'softmax'</span>)(X)</span><br><span class="line">    <span class="comment"># Add a softmax activation</span></span><br><span class="line">    X = Activation(<span class="string">'softmax'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create Model instance which converts sentence_indices into X.</span></span><br><span class="line">    model = Model(inputs=sentence_indices ,outputs=X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本周作业分为两部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;词向量运算&lt;/li&gt;
&lt;li&gt;emoji表情包&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(5-2) -- 自然语言处理与词嵌入(NLP and Word Embeddings)</title>
    <link href="http://fangzh.top/2018/dl-ai-5-2/"/>
    <id>http://fangzh.top/2018/dl-ai-5-2/</id>
    <published>2018-10-18T09:00:17.000Z</published>
    <updated>2018-10-18T10:28:14.008Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>本周主要讲了NLP和词嵌入的问题。</p><a id="more"></a><h1 id="词汇表征"><a href="#词汇表征" class="headerlink" title="词汇表征"></a>词汇表征</h1><p>在前面学习的内容中，我们表征词汇是直接使用英文单词来进行表征的，但是对于计算机来说，是无法直接认识单词的。为了让计算机能够能更好地理解我们的语言，建立更好的语言模型，我们需要将词汇进行表征。下面是几种不同的词汇表征方式：</p><p><strong>one-hot 表征：</strong></p><p>在前面的一节课程中，已经使用过了one-hot表征的方式对模型字典中的单词进行表征，对应单词的位置用1表示，其余位置用0表示，如下图所示： </p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv3hd7j20n30cuaaw.jpg" alt=""></p><p>one-hot表征的缺点：这种方法将每个词孤立起来，使得模型对相关词的泛化能力不强。每个词向量之间的距离都一样，乘积均为0，所以无法获取词与词之间的相似性和关联性。</p><p><strong>特征表征：词嵌入</strong></p><p>用不同的特征来对各个词汇进行表征，相对与不同的特征，不同的单词均有不同的值。如下例所示：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv3tlfj20nc0crwfj.jpg" alt=""></p><p>这样差不多的词汇就会聚在一起：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv3trej20mf0bl3z7.jpg" alt=""></p><h1 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h1><p>Word Embeddings对不同单词进行了实现了特征化的表示，那么如何将这种表示方法应用到自然语言处理的应用中呢？</p><p>以下图为例，该图表示的是输入一段话，判断出人名。通过学习判断可以知道<strong>orange farmer</strong>指的应该是人，所以其对应的主语<strong>Sally Johnson</strong>就应该是人名了，所以其对应位置输出为1。</p><p>那如果把<strong>orange</strong>换成<strong>apple</strong>呢？通过词嵌入算法可以知道二者词性类似，而且后面跟着<strong>farmer</strong>，所以也能确认<strong>Robert Lin</strong>是人名。</p><p>我们继续替换，我们将<strong>apple farmer</strong>替换成不太常见的<strong>durian cultivator(榴莲繁殖员)</strong>。此时词嵌入中可能并没有<strong>durian</strong>这个词，<strong>cultivator</strong>也是不常用的词汇。这个时候怎么办呢？我们可以用到迁移学习。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv40grj20n40cqt9g.jpg" alt=""></p><ol><li><p>学习含有大量文本语料库的词嵌入(一般含有10亿到1000亿单词)，或者下载预训练好的词嵌入</p></li><li><p>将学到的词嵌入迁移到相对较小规模的训练集(例如10万词汇)，这个时候就能体现出相比于使&gt; 用one hot表示法，使用词嵌入的优势了。如果是使用one hot，那么每个单词是1×100000表&gt; 示，而用词嵌入后，假设特征维度是300，那么只需要使用 1×300的向量表示即可。</p></li><li><p>(可选) 这一步骤就是对新的数据进行fine-tune。</p></li></ol><p>词嵌入和人脸编码之间有很奇妙的联系。在人脸识别领域，我们会将人脸图片预编码成不同的编码向量，以表示不同的人脸，进而在识别的过程中使用编码来进行比对识别。词嵌入则和人脸编码有一定的相似性。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv4sacj20mv0aq75r.jpg" alt=""></p><p>但是不同的是，对于人脸识别，我们可以将任意一个<strong>没有见过的人脸照片</strong>输入到我们构建的网络中，则可输出一个对应的人脸编码。而在词嵌入模型中，所有词汇的编码是在一个<strong>固定的词汇表</strong>中进行学习单词的编码以及其之间的关系的。</p><h1 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h1><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv4zr5j20mz0cvt9u.jpg" alt=""></p><p>可以得到 man to woman ，正如 King to Queen。</p><p>可以通过词嵌入，计算词之间的距离，从而实现类比。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv6rr1j20mt0crjs7.jpg" alt=""></p><p>关于词相似度的计算，可以使用余弦公式。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv6r3rj209i0380sl.jpg" alt=""></p><p>当然也可以使用距离公式：</p><p>$$||u - v||^2$$</p><h1 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h1><p>如下图示，左边是词嵌入矩阵，每一列表示该单词的特征向量，每一行表示所有单词在某一特征上的值的大小，这个矩阵用$E$表示，假设其维度是<strong>(300,10000)</strong>。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv88frj20n10b0dgw.jpg" alt=""></p><p>在原来的one-hot中每个词是维度为10000的向量，而现在在嵌入矩阵中，每个词变成了维度为300的向量。</p><h1 id="学习词嵌入"><a href="#学习词嵌入" class="headerlink" title="学习词嵌入"></a>学习词嵌入</h1><p>下图展示了预测单词的方法，即给出缺少一个单词的句子：</p><p>“<strong>I want a glass of orange ___</strong>”</p><p>计算方法是将已知单词的特征向量都作为输入数据送到神经网络中去，然后经过一系列计算到达 Softmax分类层，在该例中输出节点数为10000个。经过计算<strong>juice</strong>概率最高，所以预测为</p><p>“<strong>I want a glass of orange juice</strong>”</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv86vmj20n30cpgmv.jpg" alt=""></p><p>在这个训练模式中，是通过全部的单词去预测最后一个单词然后反向传播更新词嵌表E</p><p>假设要预测的单词为W，词嵌表仍然为E，需要注意的是训练词嵌表和预测W是两个不同的任务。</p><p>如果任务是预测W，最佳方案是使用W前面n个单词构建语境。</p><p>如果任务是训练E，除了使用W前全部单词还可以通过：前后各4个单词、前面单独的一个词、前面语境中随机的一个词（这个方式也叫做 <strong>Skip Gram</strong> 算法），这些方法都能提供很好的结果。</p><h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>“<strong>word2vec</strong>” 是指将词语word 变成向量vector 的过程，这一过程通常通过浅层的神经网络完成，例如<strong>CBOW</strong>或者<strong>skip gram</strong>，这一过程同样可以视为构建词嵌表E的过程”。</p><h2 id="Skip-grams"><a href="#Skip-grams" class="headerlink" title="Skip-grams"></a><strong>Skip-grams</strong></h2><p>下图详细的展示了<strong>Skip-grams</strong>。即先假设<strong>Context(上下文)</strong>是<strong>orange</strong>，而<strong>Target(预测词)</strong>则是通过设置窗口值得到的，例如设置为紧邻的后一个单词，此时<strong>Target</strong>则为<strong>juice</strong>，设置其他窗口值可以得到其他预测词。</p><p>注意这个过程是用来构建<strong>词嵌表</strong>的，而不是为了真正的去预测，所以如果预测效果不好并不用担心。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv8qmmj20mr0bq74j.jpg" alt=""></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv9e21j20mx0cljs7.jpg" alt=""></p><p>上面在使用Softmax的时候有一个很明显的问题，那就是计算量过于繁琐，所以为了解决计算量大的问题，提出了如下图所示的方法，即<strong>Hierachical Softmax(分层的Softmax)</strong></p><p>简单的来说就是通过使用二叉树的形式来减少运算量。</p><p>例如一些常见的单词，如<strong>the</strong>、<strong>of</strong>等就可以在很浅的层次得到，而像<strong>durian</strong>这种少用的单词则在较深的层次得到。</p><h1 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h1><p>对于skip gram model而言，还要解决的一个问题是如何取样（选择）有效的随机词 c 和目标词 t 呢？如果真的按照自然随机分布的方式去选择，可能会大量重复的选择到出现次数频率很高的单词比如说“the, of, a, it, I, …” 重复的训练这样的单词没有特别大的意义。</p><p>如何有效的去训练选定的词如 orange 呢？在设置训练集时可以通过“<strong>负取样</strong>”的方法, 下表中第一行是通过和上面一<br>样的窗口法得到的“正”（1）结果，其他三行是从字典中随机得到的词语，结果为“负”（0）。通过这样的负取样法<br>可以更有效地去训练<strong>skip gram model</strong>.</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv964kj208a082dfz.jpg" alt=""></p><p>负取样的个数<strong>k</strong>由数据量的大小而定，上述例子中为4. 实际中数据量大则 <strong>k = 2 ~ 5</strong>，数据量小则可以相对大一些<strong>k = 5 ~ 20</strong></p><p>通过负取样，我们的神经网络训练从softmax预测每个词出现的频率变成了<strong>经典binary logistic regression</strong>问题，概率公式用 <strong>sigmoid</strong> 代替 <strong>softmax</strong>从而大大提高了速度。</p><p>选词概率的经验公式：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv9hyfj208p0400sl.jpg" alt=""></p><h1 id="GloVe词向量"><a href="#GloVe词向量" class="headerlink" title="GloVe词向量"></a>GloVe词向量</h1><p><strong>GloVe(Global vectors for word representation)</strong>虽然不想<strong>Word2Vec</strong>模型那样流行，但是它也有自身的优点，即简单。</p><p>这里就不介绍了，看不太懂。</p><h1 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h1><p>情感分类就是通过一段文本来判断这个文本中的内容是否喜欢其所讨论的内容，这是NLP中最重要的模块之一。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuvb16tj20n00craap.jpg" alt=""></p><p>可以看到下图中的模型先将评语中各个单词通过 <strong>词嵌表(数据量一般比较大，例如有100Billion的单词数)</strong> 转化成对应的特征向量，然后对所有的单词向量<strong>做求和</strong>或者<strong>做平均</strong>，然后构建Softmax分类器，最后输出星级评级。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuvedrsj20mv0cmaat.jpg" alt=""></p><p>但是上面的模型存在一个问题，一般而言如果评语中有像”<strong>good、excellent</strong>“这样的单词，一般都是星级评分较高的评语，但是该模型对下面这句评语就显得无能为力了：</p><p>“<strong>Completely lacking in good taste, good service, and good ambience.</strong>”</p><p>之所以上面的模型存在那样的缺点，就是因为它没有把单词的时序考虑进去，所以我们可以使用RNN构建模型来解决这种问题。</p><p>另外使用RNN模型还有另一个好处，假设测试集中的评语是这样的</p><p>“<strong>Completely absent of good taste, good service, and good ambience.</strong>”</p><p>该评语只是将<strong>lacking in</strong>替换成了<strong>absent of</strong>，而且我们即使假设<strong>absent</strong>并没有出现在训练集中，但是因为词嵌表很庞大，所以词嵌表中包含<strong>absent</strong>，所以算法依旧可以知道<strong>absent</strong>和<strong>lacking</strong>有相似之处，最后输出的结果也依然可以保持正确。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuvbtcqj20n20d4jrv.jpg" alt=""></p><h1 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a><strong>词嵌入除偏</strong></h1><p>现如今机器学习已经被用到了很多领域，例如银行贷款决策，简历筛选。但是因为机器是向人们学习，所以好的坏的都会学到，例如他也会学到一些偏见或者歧视。</p><p>如下图示</p><p>当说到<strong>Man：程序员</strong>的时候，算法得出<strong>Woman：家庭主妇</strong>，这显然存在偏见。</p><p>又如<strong>Man：Doctor</strong>，算法认为<strong>Woman：Nurse</strong>。这显然也存在其实和偏见。</p><p>上面提到的例子都是性别上的歧视，词嵌入也会反映出年龄歧视、性取向歧视以及种族歧视等等。</p><p>人类在这方面已经做的不对了，所以机器应当做出相应的调整来减少歧视。</p><p><strong>消除偏见的方法：</strong></p><ul><li>定义偏见的方向：如性别 <ul><li>对大量性别相对的词汇进行相减并求平均：$e_{he}−e_{she}、e_{male}−e_{female}$⋯；</li><li>通过平均后的向量，则可以得到一个或多个偏见趋势相关的维度，以及大量不相关的维度；</li></ul></li><li><p>中和化：对每一个定义不明确的词汇，进行偏见的处理，如像doctor、babysitter这类词；通过减小这些词汇在得到的偏见趋势维度上值的大小；</p></li><li><p>均衡：将如gradmother和gradfather这种对称词对调整至babysitter这类词汇平衡的位置上，使babysitter这类词汇处于一个中立的位置，进而消除偏见。</p></li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuvp5goj20ts0fndmf.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本周主要讲了NLP和词嵌入的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(5-1)-- 循环神经网络（Recurrent Neural Networks）（3）</title>
    <link href="http://fangzh.top/2018/dl-ai-5-1h3/"/>
    <id>http://fangzh.top/2018/dl-ai-5-1h3/</id>
    <published>2018-10-18T08:20:36.000Z</published>
    <updated>2018-10-18T08:40:34.982Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>第三个作业是用LSTM来生成爵士乐。</p><a id="more"></a><h1 id="Part3-Improvise-a-Jazz-Solo-with-an-LSTM-Network"><a href="#Part3-Improvise-a-Jazz-Solo-with-an-LSTM-Network" class="headerlink" title="Part3:Improvise a Jazz Solo with an LSTM Network"></a>Part3:Improvise a Jazz Solo with an LSTM Network</h1><p>我们已经对音乐数据做了预处理，以”values”来表示。可以非正式地将每个”value”看作一个音符，它包含音高和持续时间。 例如，如果您按下特定钢琴键0.5秒，那么您刚刚弹奏了一个音符。 在音乐理论中，”value” 实际上比这更复杂。 特别是，它还捕获了同时播放多个音符所需的信息。 例如，在播放音乐作品时，可以同时按下两个钢琴键（同时播放多个音符生成所谓的“和弦”）。 但是这里我们不需要关系音乐理论的细节。对于这个作业，你需要知道的是，我们获得一个”values”的数据集，并将学习一个RNN模型来生成一个序列的”values”。</p><p>我们的音乐生成系统将使用78个独特的值。</p><ul><li>X: 这是一个（m，Tx，78）维数组。 m 表示样本数量，Tx 表示时间步(也即序列的长度)，在每个时间步，输入是78个不同的可能值之一，表示为一个one-hot向量。 因此，例如，X [i，t，：]是表示第i个示例在时间t的值的one-hot向量。</li><li>Y: 与X基本相同，但向左（向前）移动了一步。 与恐龙分配类似，使用先前值预测下一个值，所以我们的序列模型将尝试预测给定的x⟨t⟩。 但是，Y中的数据被重新排序为维（Ty，m，78），其中Ty = Tx。 这种格式使得稍后进入LSTM更方便。</li><li>n_value: 数据集中独立”value”的个数，这里是78</li><li>indices_values: python 字典：key 是0-77，value 是特定音符</li></ul><p>模型结构如下：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgolo43mj21sm0ryq6x.jpg" alt=""></p><p>这里用了3个keras函数来定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reshapor = Reshape((<span class="number">1</span>, <span class="number">78</span>))                        <span class="comment"># Used in Step 2.B of djmodel(), below</span></span><br><span class="line">LSTM_cell = LSTM(n_a, return_state = <span class="keyword">True</span>)         <span class="comment"># Used in Step 2.C</span></span><br><span class="line">densor = Dense(n_values, activation=<span class="string">'softmax'</span>)     <span class="comment"># Used in Step 2.D</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: djmodel</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">djmodel</span><span class="params">(Tx, n_a, n_values)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Tx -- length of the sequence in a corpus</span></span><br><span class="line"><span class="string">    n_a -- the number of activations used in our model</span></span><br><span class="line"><span class="string">    n_values -- number of unique values in the music data </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a keras model with the </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define the input of your model with a shape </span></span><br><span class="line">    X = Input(shape=(Tx, n_values))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define s0, initial hidden state for the decoder LSTM</span></span><br><span class="line">    a0 = Input(shape=(n_a,), name=<span class="string">'a0'</span>)</span><br><span class="line">    c0 = Input(shape=(n_a,), name=<span class="string">'c0'</span>)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    <span class="comment"># Step 1: Create empty list to append the outputs while you iterate (≈1 line)</span></span><br><span class="line">    outputs = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Loop</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(Tx):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2.A: select the "t"th time step vector from X. </span></span><br><span class="line">        x = Lambda(<span class="keyword">lambda</span> x: X[:,t,:])(X)</span><br><span class="line">        <span class="comment"># Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)</span></span><br><span class="line">        x = reshapor(x)</span><br><span class="line">        <span class="comment"># Step 2.C: Perform one step of the LSTM_cell</span></span><br><span class="line">        a, _, c = LSTM_cell(x, initial_state=[a, c])</span><br><span class="line">        <span class="comment"># Step 2.D: Apply densor to the hidden state output of LSTM_Cell</span></span><br><span class="line">        out = densor(a)</span><br><span class="line">        <span class="comment"># Step 2.E: add the output to "outputs"</span></span><br><span class="line">        outputs.append(out)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Create model instance</span></span><br><span class="line">    model = Model(inputs=[X, a0, c0], outputs=outputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = djmodel(Tx = 30 , n_a = 64, n_values = 78)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=opt, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = 60</span><br><span class="line">a0 = np.zeros((m, n_a))</span><br><span class="line">c0 = np.zeros((m, n_a))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit([X, a0, c0], list(Y), epochs=100)</span><br></pre></td></tr></table></figure><h2 id="生成音乐的模型"><a href="#生成音乐的模型" class="headerlink" title="生成音乐的模型"></a>生成音乐的模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: music_inference_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">music_inference_model</span><span class="params">(LSTM_cell, densor, n_values = <span class="number">78</span>, n_a = <span class="number">64</span>, Ty = <span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Uses the trained "LSTM_cell" and "densor" from model() to generate a sequence of values.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    LSTM_cell -- the trained "LSTM_cell" from model(), Keras layer object</span></span><br><span class="line"><span class="string">    densor -- the trained "densor" from model(), Keras layer object</span></span><br><span class="line"><span class="string">    n_values -- integer, umber of unique values</span></span><br><span class="line"><span class="string">    n_a -- number of units in the LSTM_cell</span></span><br><span class="line"><span class="string">    Ty -- integer, number of time steps to generate</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    inference_model -- Keras model instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input of your model with a shape </span></span><br><span class="line">    x0 = Input(shape=(<span class="number">1</span>, n_values))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define s0, initial hidden state for the decoder LSTM</span></span><br><span class="line">    a0 = Input(shape=(n_a,), name=<span class="string">'a0'</span>)</span><br><span class="line">    c0 = Input(shape=(n_a,), name=<span class="string">'c0'</span>)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line">    x = x0</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Create an empty list of "outputs" to later store your predicted values (≈1 line)</span></span><br><span class="line">    outputs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Loop over Ty and generate a value at every time step</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(Ty):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.A: Perform one step of LSTM_cell (≈1 line)</span></span><br><span class="line">        a, _, c = LSTM_cell(x, initial_state=[a, c])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)</span></span><br><span class="line">        out = densor(a)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2.C: Append the prediction "out" to "outputs". out.shape = (None, 78) (≈1 line)</span></span><br><span class="line">        outputs.append(out)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.D: Select the next value according to "out", and set "x" to be the one-hot representation of the</span></span><br><span class="line">        <span class="comment">#           selected value, which will be passed as the input to LSTM_cell on the next step. We have provided </span></span><br><span class="line">        <span class="comment">#           the line of code you need to do this. </span></span><br><span class="line">        x = Lambda(one_hot)(out)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 3: Create model instance with the correct "inputs" and "outputs" (≈1 line)</span></span><br><span class="line">    inference_model = Model(inputs=[x0, a0, c0], outputs=outputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> inference_model</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inference_model = music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 50)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_initializer = np.zeros((1, 1, 78))</span><br><span class="line">a_initializer = np.zeros((1, n_a))</span><br><span class="line">c_initializer = np.zeros((1, n_a))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict_and_sample</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_and_sample</span><span class="params">(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, </span></span></span><br><span class="line"><span class="function"><span class="params">                       c_initializer = c_initializer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Predicts the next value of values using the inference model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    inference_model -- Keras model instance for inference time</span></span><br><span class="line"><span class="string">    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation</span></span><br><span class="line"><span class="string">    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell</span></span><br><span class="line"><span class="string">    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated</span></span><br><span class="line"><span class="string">    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.</span></span><br><span class="line">    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])</span><br><span class="line">    <span class="comment"># Step 2: Convert "pred" into an np.array() of indices with the maximum probabilities</span></span><br><span class="line">    indices = np.argmax(pred, axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># Step 3: Convert indices to one-hot vectors, the shape of the results should be (1, )</span></span><br><span class="line">    results = to_categorical(indices, num_classes=x_initializer.shape[<span class="number">-1</span>])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> results, indices</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out_stream = generate_music(inference_model)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;第三个作业是用LSTM来生成爵士乐。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(5-1)-- 循环神经网络（Recurrent Neural Networks）（2）</title>
    <link href="http://fangzh.top/2018/dl-ai-5-1h2/"/>
    <id>http://fangzh.top/2018/dl-ai-5-1h2/</id>
    <published>2018-10-18T08:20:33.000Z</published>
    <updated>2018-10-18T08:40:00.090Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>作业2搭建了一个字符级的语言模型，来生成恐龙的名字。</p><a id="more"></a><h1 id="Part2-Character-level-language-model-Dinosaurus-land"><a href="#Part2-Character-level-language-model-Dinosaurus-land" class="headerlink" title="Part2:Character level language model - Dinosaurus land"></a>Part2:Character level language model - Dinosaurus land</h1><p>模型结构</p><ul><li>初始化参数</li><li>执行最优化循环<ul><li>计算前向传播的损失函数</li><li>计算反向传播的梯度及损失函数</li><li>剪裁梯度避免梯度爆炸</li><li>使用梯度更新梯度下降中的各参数</li></ul></li><li>返回学习到的参数</li></ul><p><strong>梯度裁剪</strong></p><p>确保不会梯度爆炸</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### GRADED FUNCTION: clip</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients, maxValue)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Clips the gradients' values between minimum and maximum.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    gradients -- a dictionary containing the gradients "dWaa", "dWax", "dWya", "db", "dby"</span></span><br><span class="line"><span class="string">    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    gradients -- a dictionary with the clipped gradients.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    dWaa, dWax, dWya, db, dby = gradients[<span class="string">'dWaa'</span>], gradients[<span class="string">'dWax'</span>], gradients[<span class="string">'dWya'</span>], gradients[<span class="string">'db'</span>], gradients[<span class="string">'dby'</span>]</span><br><span class="line">   </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)</span></span><br><span class="line">    <span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax, dWaa, dWya, db, dby]:</span><br><span class="line">        np.clip(gradient, <span class="number">-1</span> * maxValue, maxValue,out=gradient)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dWaa"</span>: dWaa, <span class="string">"dWax"</span>: dWax, <span class="string">"dWya"</span>: dWya, <span class="string">"db"</span>: db, <span class="string">"dby"</span>: dby&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p><strong>采样</strong></p><p>现在假设你的模型已经训练好了，你需要以此生成新的字母，过程如下:</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgol8ej9j21ks0lw0wq.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sample</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters, char_to_ix, seed)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Sample a sequence of characters according to a sequence of probability distributions output of the RNN</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. </span></span><br><span class="line"><span class="string">    char_to_ix -- python dictionary mapping each character to an index.</span></span><br><span class="line"><span class="string">    seed -- used for grading purposes. Do not worry about it.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    indices -- a list of length n containing the indices of the sampled characters.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters and relevant shapes from "parameters" dictionary</span></span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'b'</span>]</span><br><span class="line">    vocab_size = by.shape[<span class="number">0</span>]</span><br><span class="line">    n_a = Waa.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)</span></span><br><span class="line">    x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Step 1': Initialize a_prev as zeros (≈1 line)</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)</span></span><br><span class="line">    indices = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Idx is a flag to detect a newline character, we initialize it to -1</span></span><br><span class="line">    idx = <span class="number">-1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop over time-steps t. At each time-step, sample a character from a probability distribution and append </span></span><br><span class="line">    <span class="comment"># its index to "indices". We'll stop if we reach 50 characters (which should be very unlikely with a well </span></span><br><span class="line">    <span class="comment"># trained model), which helps debugging and prevents entering an infinite loop. </span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    newline_character = char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (idx != newline_character <span class="keyword">and</span> counter != <span class="number">50</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2: Forward propagate x using the equations (1), (2) and (3)</span></span><br><span class="line">        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)</span><br><span class="line">        z = np.dot(Wya, a) + by</span><br><span class="line">        y = softmax(z)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for grading purposes</span></span><br><span class="line">        np.random.seed(counter+seed) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 3: Sample the index of a character within the vocabulary from the probability distribution y</span></span><br><span class="line">        idx = np.random.choice(range(len(y)),p = y.ravel())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append the index to "indices"</span></span><br><span class="line">        indices.append(idx)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 4: Overwrite the input character as the one corresponding to the sampled index.</span></span><br><span class="line">        x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">        x[idx] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update "a_prev" to be "a"</span></span><br><span class="line">        a_prev = a</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for grading purposes</span></span><br><span class="line">        seed += <span class="number">1</span></span><br><span class="line">        counter +=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (counter == <span class="number">50</span>):</span><br><span class="line">        indices.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> indices</span><br></pre></td></tr></table></figure><h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><p>函数都已经给你了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: optimize</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Execute one step of the optimization to train the model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.</span></span><br><span class="line"><span class="string">    Y -- list of integers, exactly the same as X but shifted one index to the left.</span></span><br><span class="line"><span class="string">    a_prev -- previous hidden state.</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        b --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for the model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- value of the loss function (cross-entropy)</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        db -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dby -- Gradients of output bias vector, of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Forward propagate through time (≈1 line)</span></span><br><span class="line">    loss, cache = rnn_forward(X, Y, a_prev, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backpropagate through time (≈1 line)</span></span><br><span class="line">    gradients, a = rnn_backward(X, Y, parameters, cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Clip your gradients between -5 (min) and 5 (max) (≈1 line)</span></span><br><span class="line">    gradients = clip(gradients, <span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update parameters (≈1 line)</span></span><br><span class="line">    parameters = update_parameters(parameters, gradients, learning_rate)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss, gradients, a[len(X)<span class="number">-1</span>]</span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data, ix_to_char, char_to_ix, num_iterations = <span class="number">35000</span>, n_a = <span class="number">50</span>, dino_names = <span class="number">7</span>, vocab_size = <span class="number">27</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Trains the model and generates dinosaur names. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    data -- text corpus</span></span><br><span class="line"><span class="string">    ix_to_char -- dictionary that maps the index to a character</span></span><br><span class="line"><span class="string">    char_to_ix -- dictionary that maps a character to an index</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to train the model for</span></span><br><span class="line"><span class="string">    n_a -- number of units of the RNN cell</span></span><br><span class="line"><span class="string">    dino_names -- number of dinosaur names you want to sample at each iteration. </span></span><br><span class="line"><span class="string">    vocab_size -- number of unique characters found in the text, size of the vocabulary</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- learned parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve n_x and n_y from vocab_size</span></span><br><span class="line">    n_x, n_y = vocab_size, vocab_size</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(n_a, n_x, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize loss (this is required because we want to smooth our loss, don't worry about it)</span></span><br><span class="line">    loss = get_initial_loss(vocab_size, dino_names)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Build list of all dinosaur names (training examples).</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"dinos.txt"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        examples = f.readlines()</span><br><span class="line">    examples = [x.lower().strip() <span class="keyword">for</span> x <span class="keyword">in</span> examples]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle list of all dinosaur names</span></span><br><span class="line">    np.random.seed(<span class="number">0</span>)</span><br><span class="line">    np.random.shuffle(examples)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the hidden state of your LSTM</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use the hint above to define one training example (X,Y) (≈ 2 lines)</span></span><br><span class="line">        index = j % len(examples)</span><br><span class="line">        X = [<span class="keyword">None</span>] + [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> examples[index]]</span><br><span class="line">        Y = X[<span class="number">1</span>:] + [char_to_ix[<span class="string">'\n'</span>]]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform one optimization step: Forward-prop -&gt; Backward-prop -&gt; Clip -&gt; Update parameters</span></span><br><span class="line">        <span class="comment"># Choose a learning rate of 0.01</span></span><br><span class="line">        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate=<span class="number">0.01</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use a latency trick to keep the loss smooth. It happens here to accelerate the training.</span></span><br><span class="line">        loss = smooth(loss, curr_loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Every 2000 Iteration, generate "n" characters thanks to sample() to check if the model is learning properly</span></span><br><span class="line">        <span class="keyword">if</span> j % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            </span><br><span class="line">            print(<span class="string">'Iteration: %d, Loss: %f'</span> % (j, loss) + <span class="string">'\n'</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># The number of dinosaur names to print</span></span><br><span class="line">            seed = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> range(dino_names):</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Sample indices and print them</span></span><br><span class="line">                sampled_indices = sample(parameters, char_to_ix, seed)</span><br><span class="line">                print_sample(sampled_indices, ix_to_char)</span><br><span class="line">                </span><br><span class="line">                seed += <span class="number">1</span>  <span class="comment"># To get the same result for grading purposed, increment the seed by one. </span></span><br><span class="line">      </span><br><span class="line">            print(<span class="string">'\n'</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作业2搭建了一个字符级的语言模型，来生成恐龙的名字。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(5-1)-- 循环神经网络（Recurrent Neural Networks）（1）</title>
    <link href="http://fangzh.top/2018/dl-ai-5-1h1/"/>
    <id>http://fangzh.top/2018/dl-ai-5-1h1/</id>
    <published>2018-10-18T02:26:56.000Z</published>
    <updated>2018-10-18T08:39:55.761Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>本周作业分为三部分：</p><ul><li>手动建一个RNN模型</li><li>搭建一个字符级的语言模型来生成恐龙的名字</li><li>用LSTM生成爵士乐</li></ul><a id="more"></a><h1 id="Part1-Building-a-recurrent-neural-network-step-by-step"><a href="#Part1-Building-a-recurrent-neural-network-step-by-step" class="headerlink" title="Part1:Building a recurrent neural network - step by step"></a>Part1:Building a recurrent neural network - step by step</h1><p>来构建一个RNN的神经网络。</p><h2 id="1-Forward-propagation-for-the-basic-Recurrent-Neural-Network"><a href="#1-Forward-propagation-for-the-basic-Recurrent-Neural-Network" class="headerlink" title="1 - Forward propagation for the basic Recurrent Neural Network"></a>1 - Forward propagation for the basic Recurrent Neural Network</h2><p>先来进行前向传播的构建，要构建这个网络，先构建每个RNN的传播单元：</p><p><strong>RNN cell</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgoliv4pj21he0o2q6i.jpg" alt=""></p><ol><li>Compute the hidden state with tanh activation: $a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)$</li><li>Using your new hidden state $a^{\langle t \rangle}$, compute the prediction $\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y)$. We provided you a function: <code>softmax</code>.</li><li>Store $(a^{\langle t \rangle}, a^{\langle t-1 \rangle}, x^{\langle t \rangle}, parameters)$ in cache</li><li>Return $a^{\langle t \rangle}$ , $y^{\langle t \rangle}$ and cache</li></ol><p>We will vectorize over $m$ examples. Thus, $x^{\langle t \rangle}$ will have dimension $(n_x,m)$, and $a^{\langle t \rangle}$ will have dimension $(n_a,m)$. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: rnn_cell_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt, a_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a single forward step of the RNN-cell as described in Figure (2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈2 lines)</span></span><br><span class="line">    <span class="comment"># compute next activation state using the formula given above</span></span><br><span class="line">    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)</span><br><span class="line">    <span class="comment"># compute output of the current cell using the formula given above</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya, a_next) + by)    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values you need for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred, cache</span><br></pre></td></tr></table></figure><p><strong>RNN forward pass</strong></p><p>思路是：</p><ul><li>先把 a ,y_pred置为0</li><li>然后初始化a_next = a0</li><li>然后经过Tx个循环，求得每一步的a和y以及cache</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: rnn_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "caches" which will contain the list of all caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters["Wya"]</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wya"</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a" and "y" with zeros (≈2 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    y_pred = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next (≈1 line)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        <span class="comment"># Append "cache" to "caches" (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a, y_pred, caches</span><br></pre></td></tr></table></figure><h2 id="2-Long-Short-Term-Memory-LSTM-network"><a href="#2-Long-Short-Term-Memory-LSTM-network" class="headerlink" title="2 - Long Short-Term Memory (LSTM) network"></a>2 - Long Short-Term Memory (LSTM) network</h2><p>接下来构建一个LSTM的网络</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgolqlbvj21d80mqn24.jpg" alt=""></p><p><strong>遗忘门：</strong></p><p>假设我们正在阅读一段文字中的单词，并且希望使用LSTM来跟踪语法结构，例如主语是单数还是复数。 如果主语从单个单词变成复数单词，我们需要找到一种方法来摆脱先前存储的单数/复数状态的记忆值。</p><p>在LSTM中，遗忘门让我们做到这一点： </p><p>$$\Gamma_f^{\langle t \rangle} = \sigma(W_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f)$$</p><p><strong>更新门:</strong></p><p>一旦我们忘记所讨论的主题是单数的，我们需要找到一种方法来更新它，以反映新主题现在是复数。</p><p>$$\Gamma_u^{\langle t \rangle} = \sigma(W_u[a^{\langle t-1 \rangle}, x^] + b_u)$$ </p><p>所以两个门结合起来可以更新单元值：</p><p>$$ \tilde{c}^{\langle t \rangle} = \tanh(W_c[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c) $$</p><p>$$ c^{&lt;t>} = \Gamma_f^{&lt;t>} c^{&lt;t-1>} +  \Gamma_u ^{&lt;t>} \tilde {c}^{&lt;t>} $$</p><p><strong>输出门：</strong></p><p>为了决定输出，我们将使用以下两个公式：</p><p>$$ \Gamma_o^{\langle t \rangle}=  \sigma(W_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o)$$<br>$$ a^{\langle t \rangle} = \Gamma_o^{\langle t \rangle}* \tanh(c^{\langle t \rangle}) $$</p><p><strong>LSTM 单元</strong></p><ul><li>先将$a^{\langle t-1 \rangle}$ and $x^{\langle t \rangle}$连接在一起变成$concat = \begin{bmatrix} a^{\langle t-1 \rangle} \ x^{\langle t \rangle} \end{bmatrix}$</li><li>计算以上的6个公式</li><li>然后预测输出y</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: lstm_cell_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt, a_prev, c_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc --  Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_next -- next memory state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),</span></span><br><span class="line"><span class="string">          c stands for the memory value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wf = parameters[<span class="string">"Wf"</span>]</span><br><span class="line">    bf = parameters[<span class="string">"bf"</span>]</span><br><span class="line">    Wi = parameters[<span class="string">"Wi"</span>]</span><br><span class="line">    bi = parameters[<span class="string">"bi"</span>]</span><br><span class="line">    Wc = parameters[<span class="string">"Wc"</span>]</span><br><span class="line">    bc = parameters[<span class="string">"bc"</span>]</span><br><span class="line">    Wo = parameters[<span class="string">"Wo"</span>]</span><br><span class="line">    bo = parameters[<span class="string">"bo"</span>]</span><br><span class="line">    Wy = parameters[<span class="string">"Wy"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Concatenate a_prev and xt (≈3 lines)</span></span><br><span class="line">    concat = np.zeros((n_x + n_a, m))</span><br><span class="line">    concat[: n_a, :] = a_prev  </span><br><span class="line">    concat[n_a :, :] = xt </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)</span></span><br><span class="line">    ft = sigmoid(np.dot(Wf, concat) + bf)</span><br><span class="line">    it = sigmoid(np.dot(Wi, concat) + bi)</span><br><span class="line">    cct = np.tanh(np.dot(Wc, concat) + bc)</span><br><span class="line">    c_next = ft * c_prev + it * cct</span><br><span class="line">    ot = sigmoid(np.dot(Wo, concat) + bo)</span><br><span class="line">    a_next = ot * np.tanh(c_next)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute prediction of the LSTM cell (≈1 line)</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wy, a_next) + by)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure><p><strong>Forward pass for LSTM</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgolirp8j21bs0bcwgj.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: lstm_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc -- Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize "caches", which will track the list of all the caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">'Wy'</span>].shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize "a", "c" and "y" with zeros (≈3 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    c = np.zeros((n_a, m, T_x))</span><br><span class="line">    y = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next and c_next (≈2 lines)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros((n_a, m))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, c_next, yt, cache = lstm_cell_forward(x[:, :, t], a_next, c_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y[:,:,t] = yt</span><br><span class="line">        <span class="comment"># Save the value of the next cell state (≈1 line)</span></span><br><span class="line">        c[:,:,t]  = c_next</span><br><span class="line">        <span class="comment"># Append the cache into caches (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a, y, c, caches</span><br></pre></td></tr></table></figure><h2 id="3-Backpropagation-in-recurrent-neural-networks"><a href="#3-Backpropagation-in-recurrent-neural-networks" class="headerlink" title="3 - Backpropagation in recurrent neural networks"></a>3 - Backpropagation in recurrent neural networks</h2><p>接下来是RNN的反向传播，但是一般框架都会帮我们实现，这里看看就好了。公式也比较复杂。</p><p><strong>RNN backward pass</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward pass for the RNN-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradient of loss with respect to next hidden state</span></span><br><span class="line"><span class="string">    cache -- python dictionary containing useful values (output of rnn_cell_forward())</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradients of input data, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from cache</span></span><br><span class="line">    (a_next, a_prev, xt, parameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from parameters</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># compute the gradient of tanh with respect to a_next (≈1 line)</span></span><br><span class="line">    dtanh = (<span class="number">1</span> - a_next**<span class="number">2</span>) * da_next</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of the loss with respect to Wax (≈2 lines)</span></span><br><span class="line">    dxt = np.dot(Wax.T, dtanh)</span><br><span class="line">    dWax = np.dot(dtanh, xt.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient with respect to Waa (≈2 lines)</span></span><br><span class="line">    da_prev = np.dot(Waa.T, dtanh)</span><br><span class="line">    dWaa = np.dot(dtanh, a_prev.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient with respect to b (≈1 line)</span></span><br><span class="line">    dba = np.sum(dtanh, keepdims=<span class="keyword">True</span>, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa, <span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for a RNN over an entire sequence of input data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple containing information from the forward pass (rnn_forward)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches (≈2 lines)</span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, a0, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes (≈2 lines)</span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes (≈6 lines)</span></span><br><span class="line">    dx = np.zeros((n_x, m, T_x))</span><br><span class="line">    dWax = np.zeros((n_a, n_x))</span><br><span class="line">    dWaa = np.zeros((n_a, n_a))</span><br><span class="line">    dba = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    da0 = np.zeros((n_a, m))</span><br><span class="line">    da_prevt = np.zeros((n_a, m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop through all the time steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute gradients at time step t. Choose wisely the "da_next" and the "cache" to use in the backward propagation step. (≈1 line)</span></span><br><span class="line">        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])</span><br><span class="line">        <span class="comment"># Retrieve derivatives from gradients (≈ 1 line)</span></span><br><span class="line">        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[<span class="string">"dxt"</span>], gradients[<span class="string">"da_prev"</span>], gradients[<span class="string">"dWax"</span>], gradients[<span class="string">"dWaa"</span>], gradients[<span class="string">"dba"</span>]</span><br><span class="line">        <span class="comment"># Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)</span></span><br><span class="line">        dx[:, :, t] = dxt</span><br><span class="line">        dWax += dWaxt</span><br><span class="line">        dWaa += dWaat</span><br><span class="line">        dba += dbat</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) </span></span><br><span class="line">    da0 = da_prevt</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa,<span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p><strong>LSTM backward pass</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_backward</span><span class="params">(da_next, dc_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for the LSTM-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradients of next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    dc_next -- Gradients of next cell state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    cache -- cache storing information from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from xt's and a_next's shape (≈2 lines)</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_a, m = a_next.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)</span></span><br><span class="line">    dot = da_next * np.tanh(c_next) * ot * (<span class="number">1</span>-ot)</span><br><span class="line">    dcct = (dc_next*it+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*it*da_next)*(<span class="number">1</span>-np.square(cct))</span><br><span class="line">    dit = (dc_next*cct+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*cct*da_next)*it*(<span class="number">1</span>-it)</span><br><span class="line">    dft = (dc_next*c_prev+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*c_prev*da_next)*ft*(<span class="number">1</span>-ft) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Code equations (7) to (10) (≈4 lines)</span></span><br><span class="line">    <span class="comment"># dit = None</span></span><br><span class="line">    <span class="comment"># dft = None</span></span><br><span class="line">    <span class="comment"># dot = None</span></span><br><span class="line">    <span class="comment"># dcct = None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute parameters related derivatives. Use equations (11)-(14) (≈8 lines)</span></span><br><span class="line">    dWf = np.dot(dft, np.concatenate((a_prev, xt), axis=<span class="number">0</span>).T)</span><br><span class="line">    dWi = np.dot(dit, np.concatenate((a_prev, xt), axis=<span class="number">0</span>).T)</span><br><span class="line">    dWc = np.dot(dcct, np.concatenate((a_prev, xt), axis=<span class="number">0</span>).T)</span><br><span class="line">    dWo = np.dot(dot, np.concatenate((a_prev, xt), axis=<span class="number">0</span>).T)</span><br><span class="line">    dbf = np.sum(dft, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dbi = np.sum(dit, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dbc = np.sum(dcct, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dbo = np.sum(dot, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines)</span></span><br><span class="line">    da_prev = np.dot(parameters[<span class="string">'Wf'</span>][:,:n_a].T, dft) + np.dot(parameters[<span class="string">'Wi'</span>][:,:n_a].T, dit) + np.dot(parameters[<span class="string">'Wc'</span>][:,:n_a].T, dcct) + np.dot(parameters[<span class="string">'Wo'</span>][:,:n_a].T, dot)</span><br><span class="line">    dc_prev = dc_next*ft + ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*ft*da_next</span><br><span class="line">    dxt = np.dot(parameters[<span class="string">'Wf'</span>][:,n_a:].T,dft)+np.dot(parameters[<span class="string">'Wi'</span>][:,n_a:].T,dit)+np.dot(parameters[<span class="string">'Wc'</span>][:,n_a:].T,dcct)+np.dot(parameters[<span class="string">'Wo'</span>][:,n_a:].T,dot) </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save gradients in dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dc_prev"</span>: dc_prev, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWi"</span>: dWi,<span class="string">"dbi"</span>: dbi,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- cache storing information from the forward pass (lstm_forward)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient of inputs, of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches.</span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes (≈2 lines)</span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes (≈12 lines)</span></span><br><span class="line">    dx = np.zeros((n_x, m, T_x))</span><br><span class="line">    da0 = np.zeros((n_a, m))</span><br><span class="line">    da_prevt = np.zeros((n_a, m))</span><br><span class="line">    dc_prevt = np.zeros((n_a, m))</span><br><span class="line">    dWf = np.zeros((n_a, n_a+n_x))</span><br><span class="line">    dWi = np.zeros((n_a, n_a+n_x))</span><br><span class="line">    dWc = np.zeros((n_a, n_a+n_x))</span><br><span class="line">    dWo = np.zeros((n_a, n_a+n_x))</span><br><span class="line">    dbf = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    dbi = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    dbc = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    dbo = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop back over the whole sequence</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute all gradients using lstm_cell_backward</span></span><br><span class="line">        gradients = lstm_cell_backward(da[:, :, t] + da_prevt, dc_prevt, caches[t])</span><br><span class="line">        <span class="comment"># Store or add the gradient to the parameters' previous step's gradient</span></span><br><span class="line">        dx[:,:,t] = gradients[<span class="string">'dxt'</span>]</span><br><span class="line">        dWf = dWf + gradients[<span class="string">'dWf'</span>]</span><br><span class="line">        dWi = dWi + gradients[<span class="string">'dWi'</span>]</span><br><span class="line">        dWc = dWc + gradients[<span class="string">'dWc'</span>]</span><br><span class="line">        dWo = dWo + gradients[<span class="string">'dWo'</span>]</span><br><span class="line">        dbf = dbf + gradients[<span class="string">'dbf'</span>]</span><br><span class="line">        dbi = dbi + gradients[<span class="string">'dbi'</span>]</span><br><span class="line">        dbc = dbc + gradients[<span class="string">'dbc'</span>]</span><br><span class="line">        dbo = dbo + gradients[<span class="string">'dbo'</span>]</span><br><span class="line">    <span class="comment"># Set the first activation's gradient to the backpropagated gradient da_prev.</span></span><br><span class="line">    da0 = gradients[<span class="string">'da_prev'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWi"</span>: dWi,<span class="string">"dbi"</span>: dbi,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本周作业分为三部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;手动建一个RNN模型&lt;/li&gt;
&lt;li&gt;搭建一个字符级的语言模型来生成恐龙的名字&lt;/li&gt;
&lt;li&gt;用LSTM生成爵士乐&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(5-1)-- 循环神经网络（Recurrent Neural Networks）</title>
    <link href="http://fangzh.top/2018/dl-ai-5-1/"/>
    <id>http://fangzh.top/2018/dl-ai-5-1/</id>
    <published>2018-10-18T02:26:52.000Z</published>
    <updated>2018-10-18T08:39:52.001Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>第五门课讲的是序列模型，主要是对RNN算法的应用，如GRU，LSTM算法，应用在词嵌入模型，情感分类，语音识别等领域。</p><p>第一周讲的是RNN的基本算法。</p><a id="more"></a><h1 id="序列模型的应用"><a href="#序列模型的应用" class="headerlink" title="序列模型的应用"></a>序列模型的应用</h1><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokszq4j20og0dkq54.jpg" alt=""></p><p>序列模型用在了很多的地方，如语音识别，音乐生成，情感分类，DNA序列分析，机器翻译，视频内容检测，名字检测等等。</p><h1 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h1><p>先讲一下NG在课程中主要用到的数学符号。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokftypj20o507kwes.jpg" alt=""></p><p>对于输入一个$x$的句子序列，可以细分为一个个的词，每一个词记为$x^{&lt;t>}$，对应的输出$y$记为$y^{&lt;t>}$</p><p>其中，输入x的序列长度为 $T_x$，输出$y$的序列长度为$T_y$</p><p>而针对很多个不同的序列，$X^{(i)&lt;t>}$表示第$i$个样本的第t的词。</p><p>那么如何用数学的形式表示这个$x^{&lt;t>}$呢？这里用到了one-hot编码，假设词表中一共有10000个词汇，那么$x^{&lt;t>}$就是一个长度为10000的向量，在这之中只有一个维度是1，其他都是0</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokgq4pj20o80dkjsc.jpg" alt=""></p><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>如果用传统的神经网络，经过一个N层的神经网络得到输出y。</p><p>效果并不是很好，因为：</p><ul><li>输入和输出在不同的样本中是可以不同长度的（每个句子可以有不同的长度）</li><li>这种朴素的神经网络结果并不能共享从文本不同位置所学习到的特征。（如卷积神经网络中学到的特征的快速地推广到图片其他位置）</li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokgsxfj20n40cfdgj.jpg" alt=""></p><p>所以循环神经网络采用每一个时间步来计算，输入一个$x^{&lt;t>}$和前面留下来的记忆$a^{&lt;t-1>}$，来得到这一层的输出$y^{&lt;t>}$和下一层的记忆$a^{&lt;t>}$</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokk8tvj20n108s0tp.jpg" alt=""></p><p>这里需要注意在零时刻，我们需要编造一个激活值，通常输入一个零向量，有的研究人员会使用随机的方法对该初始激活向量进行初始化。同时，上图中右边的循环神经网络的绘制结构与左边是等价的。</p><p>循环神经网络是从左到右扫描数据的，同时共享每个时间步的参数。</p><ul><li>$W_{ax}$管理从输入$x^{&lt;t>}$到隐藏层的连接，每个时间步都使用相同的$W_{ax}$，同下；</li><li>$W_{aa}$管理激活值$a^{&lt;t>}$到隐藏层的连接；</li><li>$W_{ya}$管理隐藏层到激活值$y^{&lt;t>}$的连接。</li></ul><p><strong>RNN的前向传播</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokk24pj20np0ckmya.jpg" alt=""></p><p>前向传播公式如图，这里可以把$W_{aa}，W_{ax}$合并成一项，为$W_a$，而后将$[a^{&lt;t-1>},x^{&lt;t>}]$合并成一项。</p><p><strong>RNN的反向传播</strong></p><p>定义一个loss function，然后倒回去计算。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgoksqpcj20nf0cxdhj.jpg" alt=""></p><h1 id="不同类型的RNN"><a href="#不同类型的RNN" class="headerlink" title="不同类型的RNN"></a>不同类型的RNN</h1><p>对于RNN，不同的问题需要不同的输入输出结构。</p><ul><li>One to many：如音乐生成，输入一个音乐类型或者空值，生成一段音乐</li><li>Many to one：如情感分类问题，输入某个序列，输出一个值来判断得分。</li><li>many to many（$T_x = T_y$）：输入和输出的序列长度相同</li><li>many to many（$T_x != T_y$）：如机器翻译这种，先输入一段，然后自己生成一段，输入和输出长度不一定相同的。</li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokh75rj20nm0d274y.jpg" alt=""></p><h1 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h1><p><strong>什么是语言模型？</strong></p><p>对于下面的例子，两句话有相似的发音，但是想表达的意义和正确性却不相同，如何让我们的构建的语音识别系统能够输出正确地给出想要的输出。也就是对于语言模型来说，从输入的句子中，评估各个句子中各个单词出现的可能性，进而给出整个句子出现的可能性。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokqwy9j20ln0b83yw.jpg" alt=""></p><p> <strong>使用RNN构建语言模型：</strong></p><ul><li>训练集：一个很大的语言文本语料库；</li><li>Tokenize：将句子使用字典库标记化；其中，未出现在字典库中的词使用“UNK”来表示；</li><li>第一步：使用零向量对输出进行预测，即预测第一个单词是某个单词的可能性；</li><li>第二步：通过前面的输入，逐步预测后面一个单词出现的概率；</li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokk6ckj20n70cx75i.jpg" alt=""></p><h1 id="对新序列采样"><a href="#对新序列采样" class="headerlink" title="对新序列采样"></a>对新序列采样</h1><p>当我们训练得到了一个模型之后，如果我们想知道这个模型学到了些什么，一个非正式的方法就是对新序列进行采样。具体方法如下：</p><p>在每一步输出$y$时，通常使用 softmax 作为激活函数，然后根据输出的分布，随机选择一个值，也就是对应的一个字或者英文单词。</p><p>然后将这个值作为下一个单元的x输入进去(即$x^{&lt;t>}=y^{&lt;t−1>}$), 直到我们输出了终结符，或者输出长度超过了提前的预设值n才停止采样。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokk4rpj20ic063jre.jpg" alt=""></p><h1 id="RNN的梯度消失"><a href="#RNN的梯度消失" class="headerlink" title="RNN的梯度消失"></a>RNN的梯度消失</h1><p>RNN存在一个梯度消失问题，如：</p><ul><li>The cat, which already ate ………..，was full；</li><li>The cats, which already ate ………..，were full.</li></ul><p>cat 和 cats要经过很长的一系列词汇后，才对应 was 和 were，但是我们在传递过程中$a^{&lt;t>}$很难记住前面这么多词汇的内容，往往只和前面最近几个词汇有关而已。</p><p>当然，也有可能是每一层的梯度都很大，导致的梯度爆炸问题，不过这个问题可以通过设置阈值来解决，关键是要解决梯度消失问题。我们知道一旦神经网络层次很多时，反向传播很难影响前面层次的参数。</p><h1 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU(Gated Recurrent Unit)"></a>GRU(Gated Recurrent Unit)</h1><p>那么如何解决梯度消失问题了，使用GRU单元可以有效的捕捉到更深层次的连接，来改善梯度消失问题。</p><p>原本的RNN单元如图：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokkehij20m00aiglw.jpg" alt=""></p><p>而GRU单元多了一个c（memory cell）变量，用来提供长期的记忆能力。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgoktgchj20n20cwjtf.jpg" alt=""></p><p>具体过程为：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokn137j20pg04m0t4.jpg" alt=""></p><p>完整的GRU还存在另一个门，用来控制$\bar c$和 $c^{&lt;t-1>}$之间的联系强弱：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokn4qrj20dp06maaj.jpg" alt=""></p><h1 id="LSTM-Long-short-term-memory"><a href="#LSTM-Long-short-term-memory" class="headerlink" title="LSTM(Long short-term memory)"></a>LSTM(Long short-term memory)</h1><p>GRU能够让我们在序列中学习到更深的联系，长短期记忆（long short-term memory, LSTM）对捕捉序列中更深层次的联系要比GRU更加有效。</p><p>GRU只有两个门，而LSTM有三个门，分别是更新门、遗忘门、输出门：$\Gamma_u,\Gamma_f, \Gamma_o$</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgoko16mj20n50c174n.jpg" alt="GRU和LSTM公式对比"></p><p>更新门：用来决定是否更新$\bar c^{&lt;t>}$</p><p>遗忘门：来决定是否遗忘上一个$c^{&lt;t-1>}$</p><p>输出门：来决定是否输出$c^{&lt;t>}$</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgolrcslj20wl0idjth.jpg" alt=""></p><h1 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h1><p>双向RNN（bidirectional RNNs）模型能够让我们在序列的某处，不仅可以获取之间的信息，还可以获取未来的信息。</p><p>对于下图的单向RNN的例子中，无论我们的RNN单元是基本的RNN单元，还是GRU，或者LSTM单元，对于例子中第三个单词”Teddy”很难判断是否是人名，仅仅使用前面的两个单词是不够的，需要后面的信息来进行判断，但是单向RNN就无法实现获取未来的信息。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokp4z4j20mn0boaas.jpg" alt=""></p><p>而双向RNN则可以解决单向RNN存在的弊端。在BRNN中，不仅有从左向右的前向连接层，还存在一个从右向左的反向连接层。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgolueabj20n20cwq4i.jpg" alt=""></p><h1 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h1><p>与深层的基本神经网络结构相似，深层RNNs模型具有多层的循环结构，但不同的是，在传统的神经网络中，我们可能会拥有很多层，几十层上百层，但是对与RNN来说，三层的网络结构就已经很多了，因为RNN存在时间的维度，所以其结构已经足够的庞大。如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokxta0j20mu0cuab9.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;第五门课讲的是序列模型，主要是对RNN算法的应用，如GRU，LSTM算法，应用在词嵌入模型，情感分类，语音识别等领域。&lt;/p&gt;
&lt;p&gt;第一周讲的是RNN的基本算法。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(4-4)-- 特殊应用:人脸识别和神经风格转换</title>
    <link href="http://fangzh.top/2018/dl-ai-4-4h/"/>
    <id>http://fangzh.top/2018/dl-ai-4-4h/</id>
    <published>2018-10-12T10:55:20.000Z</published>
    <updated>2018-10-12T12:46:39.456Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>本周作业分为了两个部分:</p><ul><li>人脸识别</li><li>风格迁移</li></ul><a id="more"></a><h1 id="Part1：人脸识别"><a href="#Part1：人脸识别" class="headerlink" title="Part1：人脸识别"></a>Part1：人脸识别</h1><p>训练FaceNet很不现实，所以模型已经都训练好了，我们只是学习一下loss函数，然后调用模型来进行简单的识别而已。</p><p>先计算triplet_loss函数，分为4步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: triplet_loss</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span><span class="params">(y_true, y_pred, alpha = <span class="number">0.2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the triplet loss as defined by formula (3)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.</span></span><br><span class="line"><span class="string">    y_pred -- python list containing three objects:</span></span><br><span class="line"><span class="string">            anchor -- the encodings for the anchor images, of shape (None, 128)</span></span><br><span class="line"><span class="string">            positive -- the encodings for the positive images, of shape (None, 128)</span></span><br><span class="line"><span class="string">            negative -- the encodings for the negative images, of shape (None, 128)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- real number, value of the loss</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    anchor, positive, negative = y_pred[<span class="number">0</span>], y_pred[<span class="number">1</span>], y_pred[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines)</span></span><br><span class="line">    <span class="comment"># Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1</span></span><br><span class="line">    pos_dist = tf.reduce_sum(tf.square(anchor - positive),axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1</span></span><br><span class="line">    neg_dist = tf.reduce_sum(tf.square(anchor - negative),axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># Step 3: subtract the two previous distances and add alpha.</span></span><br><span class="line">    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.</span></span><br><span class="line">    loss = tf.reduce_sum(tf.maximum(basic_loss, <span class="number">0.</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>进行单个人脸验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: verify</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verify</span><span class="params">(image_path, identity, database, model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function that verifies if the person on the "image_path" image is "identity".</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    image_path -- path to an image</span></span><br><span class="line"><span class="string">    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.</span></span><br><span class="line"><span class="string">    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).</span></span><br><span class="line"><span class="string">    model -- your Inception model instance in Keras</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dist -- distance between the image_path and the image of "identity" in the database.</span></span><br><span class="line"><span class="string">    door_open -- True, if the door should open. False otherwise.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)</span></span><br><span class="line">    encoding = img_to_encoding(image_path,model)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute distance with identity's image (≈ 1 line)</span></span><br><span class="line">    dist = np.linalg.norm(encoding-database[identity])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Open the door if dist &lt; 0.7, else don't open (≈ 3 lines)</span></span><br><span class="line">    <span class="keyword">if</span> dist &lt; <span class="number">0.7</span>:</span><br><span class="line">        print(<span class="string">"It's "</span> + str(identity) + <span class="string">", welcome home!"</span>)</span><br><span class="line">        door_open = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"It's not "</span> + str(identity) + <span class="string">", please go away"</span>)</span><br><span class="line">        door_open = <span class="keyword">False</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dist, door_open</span><br></pre></td></tr></table></figure><p>进行人脸识别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: who_is_it</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">who_is_it</span><span class="params">(image_path, database, model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements face recognition for the happy house by finding who is the person on the image_path image.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    image_path -- path to an image</span></span><br><span class="line"><span class="string">    database -- database containing image encodings along with the name of the person on the image</span></span><br><span class="line"><span class="string">    model -- your Inception model instance in Keras</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    min_dist -- the minimum distance between image_path encoding and the encodings from the database</span></span><br><span class="line"><span class="string">    identity -- string, the name prediction for the person on image_path</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 1: Compute the target "encoding" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)</span></span><br><span class="line">    encoding = img_to_encoding(image_path,model)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 2: Find the closest encoding ##</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "min_dist" to a large value, say 100 (≈1 line)</span></span><br><span class="line">    min_dist = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop over the database dictionary's names and encodings.</span></span><br><span class="line">    <span class="keyword">for</span> (name, db_enc) <span class="keyword">in</span> database.items():</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute L2 distance between the target "encoding" and the current "emb" from the database. (≈ 1 line)</span></span><br><span class="line">        dist = np.linalg.norm(encoding-database[name])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)</span></span><br><span class="line">        <span class="keyword">if</span> dist &lt; min_dist:</span><br><span class="line">            min_dist = dist</span><br><span class="line">            identity = name</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> min_dist &gt; <span class="number">0.7</span>:</span><br><span class="line">        print(<span class="string">"Not in the database."</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"it's "</span> + str(identity) + <span class="string">", the distance is "</span> + str(min_dist))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> min_dist, identity</span><br></pre></td></tr></table></figure><h1 id="Part2：风格迁移"><a href="#Part2：风格迁移" class="headerlink" title="Part2：风格迁移"></a>Part2：风格迁移</h1><p>模型也都是训练好的了，用的是VGG-19的网络。这里只是体验一下cost function的实现罢了。</p><p><strong>计算J_content(C,G)</strong></p><p>$$J_{content}(C,G) =  \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{(C)} - a^{(G)})^2 $$</p><p>在这过程中需要把三维的矩阵先展开成2维的矩阵进行计算（虽然不展开也是可以计算的，但是风格损失函数需要计算）</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2x5rgjj218g0kpaxc.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_content_cost</span><span class="params">(a_C, a_G)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the content cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C </span></span><br><span class="line"><span class="string">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_content -- scalar that you compute using equation 1 above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G (≈1 line)</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape a_C and a_G (≈2 lines)</span></span><br><span class="line">    a_C_unrolled = tf.reshape(a_C,[n_H * n_W, n_C])</span><br><span class="line">    a_G_unrolled = tf.reshape(a_G,[n_H * n_W, n_C])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute the cost with tensorflow (≈1 line)</span></span><br><span class="line">    J_content = tf.reduce_sum(tf.square(a_C_unrolled - a_G_unrolled)) / (n_H * n_W * n_C * <span class="number">4</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_content</span><br></pre></td></tr></table></figure><p><strong>计算J_style(S,G)</strong></p><p>需要把三维矩阵展开，然后转置，做矩阵乘法，才能得到相关系数矩阵</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2xlfa3j218g0epqm7.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: gram_matrix</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    A -- matrix of shape (n_C, n_H*n_W)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    GA -- Gram matrix of A, of shape (n_C, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈1 line)</span></span><br><span class="line">    GA = tf.matmul(A,tf.transpose(A))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> GA</span><br></pre></td></tr></table></figure><p>$$J_{style}^{[l]}(S,G) = \frac{1}{4 \times n_{C}^{2} \times (n_H \times n_W)^2} \sum_{i=1}^{n_C} \sum_{j=1}^{n_C} (G^{(S)}_{ij} - G^{(G)} _ {ij})^{2} $$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_layer_style_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_layer_style_cost</span><span class="params">(a_S, a_G)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S </span></span><br><span class="line"><span class="string">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G (≈1 line)</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)</span></span><br><span class="line">    a_S = tf.transpose(tf.reshape(a_S,[n_H*n_W, n_C]))</span><br><span class="line">    a_G = tf.transpose(tf.reshape(a_G,[n_H*n_W, n_C]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing gram_matrices for both images S and G (≈2 lines)</span></span><br><span class="line">    GS = gram_matrix(a_S)</span><br><span class="line">    GG = gram_matrix(a_G)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing the loss (≈1 line)</span></span><br><span class="line">    J_style_layer = <span class="number">1</span> / (<span class="number">4</span> * (n_C*n_W*n_H)**<span class="number">2</span>) * tf.reduce_sum(tf.square(tf.subtract(GS,GG)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_style_layer</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: total_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_cost</span><span class="params">(J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">40</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the total cost function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    J_content -- content cost coded above</span></span><br><span class="line"><span class="string">    J_style -- style cost coded above</span></span><br><span class="line"><span class="string">    alpha -- hyperparameter weighting the importance of the content cost</span></span><br><span class="line"><span class="string">    beta -- hyperparameter weighting the importance of the style cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    J -- total cost as defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈1 line)</span></span><br><span class="line">    J = alpha * J_content + beta * J_style</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">J = total_cost(J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">40</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_nn</span><span class="params">(sess, input_image, num_iterations = <span class="number">200</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize global variables (you need to run the session on the initializer)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run the noisy input image (initial generated image) through the model. Use assign().</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    generated_image = sess.run(model[<span class="string">'input'</span>].assign(input_image))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Run the session on the train_step to minimize the total cost</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the generated image by running the session on the current model['input']</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">        generated_image = sess.run(model[<span class="string">'input'</span>])</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print every 20 iteration.</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            Jt, Jc, Js = sess.run([J, J_content, J_style])</span><br><span class="line">            print(<span class="string">"Iteration "</span> + str(i) + <span class="string">" :"</span>)</span><br><span class="line">            print(<span class="string">"total cost = "</span> + str(Jt))</span><br><span class="line">            print(<span class="string">"content cost = "</span> + str(Jc))</span><br><span class="line">            print(<span class="string">"style cost = "</span> + str(Js))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save current generated image in the "/output" directory</span></span><br><span class="line">            save_image(<span class="string">"output/"</span> + str(i) + <span class="string">".png"</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save last generated image</span></span><br><span class="line">    save_image(<span class="string">'output/generated_image.jpg'</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated_image</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本周作业分为了两个部分:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人脸识别&lt;/li&gt;
&lt;li&gt;风格迁移&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(4-4)-- 特殊应用:人脸识别和神经风格转换</title>
    <link href="http://fangzh.top/2018/dl-ai-4-4/"/>
    <id>http://fangzh.top/2018/dl-ai-4-4/</id>
    <published>2018-10-12T10:55:15.000Z</published>
    <updated>2018-10-12T12:46:33.323Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>本周讲了CNN的两个特殊应用：人脸识别和神经风格转换。</p><a id="more"></a><h1 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h1><h2 id="Face-Verification-and-Face-Recognition"><a href="#Face-Verification-and-Face-Recognition" class="headerlink" title="Face Verification and Face Recognition"></a>Face Verification and Face Recognition</h2><p>人脸识别和人脸验证不一样。</p><p>人脸验证是输入一张图片，和这个人的ID或者名字，然后根据输入的图片判断这个人是不是对应这个ID，是个1对1的问题。</p><p>人脸识别是有K个人的数据库，然后输入一张人脸的图片，不确定他是哪一位，然后输出在K个人的数据库中对应的那个人，是1对K的问题。</p><p>所以人脸识别难度更高，而且精度要求更高，因为如果每张图片都是99%的精度，那么K个人就是K倍了，所以应该有99.9%以上的精度。</p><h2 id="One-shot-learning"><a href="#One-shot-learning" class="headerlink" title="One shot learning"></a>One shot learning</h2><p>人脸识别系统，通常都是只有一个人脸的样例，然后就能够成功的识别是不是这个人。这就是one shot learning，一次学习，单单通过一张照片就能识别这个人。</p><p>因此，在只有单个样本的情况下，并不能用之前的方法来实现这个识别系统。这里就需要有一个相似性函数。</p><p><strong>similarity函数：</strong></p><p>通过$d(img1,img2)$来表示两张图片的差异程度，如果d大于某个阈值，那么就表示差别很大，如果小于某个阈值，则认为是同一个人。</p><h1 id="Siamese网络"><a href="#Siamese网络" class="headerlink" title="Siamese网络"></a>Siamese网络</h1><p>那么如何计算这个$d(img1,img2)$呢？</p><p>可以利用Siamese网络来实现。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2v3cqwj20ou0dx0um.jpg" alt=""></p><p>如图，输入两张图片$x^{(1)},x^{(2)}$，经过一个卷积神经网络，去掉最后的softmax层，可以得到N维的向量，$f(x^{(1)}),f(x^{(2)})$，假设是128维，而N维的向量就相当于是对输入图片的的<strong>编码(encoding)。</strong></p><p>然后比较这两个向量之间的差值：</p><p>$$d(x1,x2) = ||f(x1) - f(x2)||^{2}_{2}$$</p><p>如果距离$d$很小，那表示这两张图片很相近，认为是同一个人。</p><p>如果距离$d$很大，那么表示这两张图片差别很大，不是同一个人。</p><h2 id="Triplet-loss"><a href="#Triplet-loss" class="headerlink" title="Triplet loss"></a>Triplet loss</h2><p>那么，我们之前说到，要得到输入图片的向量编码$f(x)$，是需要经过卷积神经网络的，那么卷积神经网络的参数如何确定呢？使用的方法就是Triplet loss损失函数，而后用梯度下降法进行迭代。</p><p>我们需要比较两组成对的图像 <strong>(Anchor, Positive, Negative)，简写(A,P,N)</strong></p><p>Anchor：表示要检测的目标图片</p><p>Positive：表示与anchor同个人的图片</p><p>Negative：表示与anchor不同个人的图片</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u6lkzj20j8072mz6.jpg" alt=""></p><p>所以我们希望A和P的距离小，A和N的距离大，因此有了如下不等式：</p><p>$$||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha \leq 0$$</p><p>这里这个公式与SVM的损失函数很类似，$\alpha$是表示margin边界，也就是增加$d(A,P)$和$d(A,N)$之间的差距。</p><p>而如果上面的不等式小于0，那说明是符合我们的要求的，如果是大于0，则要计入损失函数中，所以得到了Triplet loss的公式是：</p><p>$$L(A,P,N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha,0)$$</p><p>整个网络的代价函数就是把所有的图片损失加起来：</p><p>$$J = \sum L(A,P,N)$$</p><h2 id="三元组的选择"><a href="#三元组的选择" class="headerlink" title="三元组的选择"></a>三元组的选择</h2><p>每个三元组的选择是有讲究的，如果你要识别的是一个女人，然后对比的Negative是个老大爷，那么条件就很容易满足，学不到什么东西。所以应该尽量选择那些相似的图片进行每一组的训练，也就是：</p><p>$$d(A,P) \approx d(A,N) $$</p><p>选择的例子如下图，可以看到，每一个三元组对比的都是一些比较相似的图片：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u59x9j20kg0bv41m.jpg" alt=""></p><h2 id="脸部验证和二分类"><a href="#脸部验证和二分类" class="headerlink" title="脸部验证和二分类"></a>脸部验证和二分类</h2><p>除了之前说的用Triplet loss进行训练以外，还有别的方法来进行训练，也就是可以把Siamese网络当做一个二分类的问题。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u74asj20m50brgnh.jpg" alt=""></p><p>如图，输入两张图片，当计算得到了两个图片的向量编码后，求两张图片的距离，然后通过一个sigmoid函数，把他变成一个二分类问题，如果同个人，输出1，不同个人则输出0。其中，权重$W,b$都可以通过训练来得到。</p><p>这个时候，人脸识别问题就变成了一个<strong>监督学习</strong>的问题，在创建每一对训练集的时候，应该有对应的输出标签y。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2vnh63j20kx0bv41k.jpg" alt=""></p><h1 id="神经风格迁移"><a href="#神经风格迁移" class="headerlink" title="神经风格迁移"></a>神经风格迁移</h1><p>神经风格的迁移，就是输入两张图片，一张当做内容图片content，另一张当做风格图片style，输出的图片g兼具有一张的内容，和另一张的风格。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2wcuc4j20ow0ctqg6.jpg" alt=""></p><h2 id="卷积神经网络学什么？"><a href="#卷积神经网络学什么？" class="headerlink" title="卷积神经网络学什么？"></a>卷积神经网络学什么？</h2><p>在进行风格迁移前，我们需要了解我们的神经网络到底在学些什么东西，把中间的隐藏单元拎出来看看。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2wgh0ej21360l214v.jpg" alt=""></p><p>如上图，假设我们有一个卷积神经网络，要看到不同层的隐藏单元计算结果，怎么办？依次对各个层进行如下操作：</p><ul><li>在当前层挑选一个隐藏单元；</li><li>遍历训练集，找到最大化地激活了该运算单元的图片或者图片块；</li><li>对该层的其他运算单元执行操作。</li></ul><p>对于在第一层的隐藏单元中，其只能看到卷积网络的小部分内容，也就是最后我们找到的那些最大化激活第一层隐层单元的是一些小的图片块。我们可以理解为第一层的神经单元通常会寻找一些简单的特征，如边缘或者颜色阴影等。</p><p>而后随着层数的增加，隐藏层单元看到的东西就越来越复杂了：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2v7oo0j20ok06sagx.jpg" alt=""></p><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>对于神经风格迁移，我们的目标是由内容图片C和风格图片S，生成最终的风格迁移图片G。所以定义代价函数为：</p><p>$$J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G)$$</p><ul><li>$J_{content}(C, G)$: 代表生成图片G的内容和内容图片C的内容的相似度</li><li>$J_{style}(S, G)$: 代表生成图片G的内容和风格图片S的内容的相似度</li><li>$\alpha, \beta$: 两个超参数用来表示以上两者之间的权重</li></ul><p>首先随机初始化G的像素，然后进行梯度下降：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2v5on0j20or0dcte1.jpg" alt=""></p><h2 id="内容代价函数"><a href="#内容代价函数" class="headerlink" title="内容代价函数"></a>内容代价函数</h2><ul><li>首先假设我们使用第$l$层隐藏层来计算$J_{content}(C, G)$注意这里的$l$一般取在中间层，而不是最前面的层，或者最后层。因为太浅了啥也看不到，太深了就太像原图了。</li><li>使用一个预训练的卷积网络。（如，VGG或其他）</li><li>$a^{[l] (C)}$和$a^{[l] (G)}$分别代表内容图片C和生成图片G的$l$层的激活值；</li><li>内容损失函数$J_{content} = \frac{1}{2}||a^{[l] (C)} - a^{[l] (G)}||^2$</li></ul><h2 id="风格代价函数"><a href="#风格代价函数" class="headerlink" title="风格代价函数"></a>风格代价函数</h2><p>对于一个卷积网络中，我们选择网络的中间层$l$， 定义“Style”表示$l$层的各个通道激活项之间的相关性。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u35a3j20oa08wdgh.jpg" alt=""></p><p>那如何计算这个相关性呢？</p><p>假设我们在第$l$层有5个通道：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2undn5j20nu0d5ada.jpg" alt=""></p><p><strong>不同的通道之间代表着不同的神经元学习到的特征，</strong>如第一个通道（红色）可以表示含有垂直纹理的特征，第二个通道（黄色）表示区域中出现橙色的特征。</p><p>那么两个通道的相关性就表示图片中出现垂直纹理又出现橙色的可能性大小。</p><p>所以可以得到相关系数的矩阵<strong>“Gram Matrix</strong>：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u2s3rj209f068q2x.jpg" alt=""></p><p>$i,j.k$表示神经元所在的高度，宽度和通道。也就是每个通道的神经元分别乘上另一个通道对应位置的神经元再求和即可得到这两个通道$k,k^{\prime}$的相关系数。这个矩阵的维度是$(n_{c}^{[l]},n_{c}^{[l]})$的，也就是第$l$层的通道数乘通道数的大小。</p><p>而代价函数即为两张图片中相关系数矩阵的差值求和，再取平均。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u1z3wj20im03mmx4.jpg" alt=""></p><h2 id="1D-to-3D-卷积"><a href="#1D-to-3D-卷积" class="headerlink" title="1D to 3D 卷积"></a>1D to 3D 卷积</h2><p>图片都是2D的卷积运算，其实还可以推广到1D和3D的情况。</p><p>典型的1D情况就是信号处理。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u42q0j20o90dqabb.jpg" alt=""></p><p>3D情况就像CT的切片，是一层一层叠加起来的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本周讲了CNN的两个特殊应用：人脸识别和神经风格转换。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(4-3)-- 目标检测（Object detection）</title>
    <link href="http://fangzh.top/2018/dl-ai-4-3h/"/>
    <id>http://fangzh.top/2018/dl-ai-4-3h/</id>
    <published>2018-10-11T12:15:58.000Z</published>
    <updated>2018-10-12T02:12:56.964Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>本周的作业实现了YOLO算法，并用于自动驾驶的目标检测中。</p><h1 id="Model-details"><a href="#Model-details" class="headerlink" title="Model details"></a>Model details</h1><p>输入： (m, 608, 608, 3)</p><p>输出： (m, 19, 19, 5, 85)</p><p>IMAGE (m, 608, 608, 3) -&gt; DEEP CNN -&gt; ENCODING (m, 19, 19, 5, 85)</p><p>也就是有5个Anchor boxes，一共有80个分类。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4kterjsbj219s0p44d0.jpg" alt=""></p><p>所以，每个box的scores也就是等于每个类预测的可能性：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktdx30qj219u0k4tcm.jpg" alt=""></p><h1 id="Filtering-with-a-threshold-on-class-scores"><a href="#Filtering-with-a-threshold-on-class-scores" class="headerlink" title="Filtering with a threshold on class scores"></a>Filtering with a threshold on class scores</h1><p>这个时候开始创建一个函数，得到每一个box中scores最大的那个类，分数，以及位置，去掉其他没用的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: yolo_filter_boxes</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_filter_boxes</span><span class="params">(box_confidence, boxes, box_class_probs, threshold = <span class="number">.6</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Filters YOLO boxes by thresholding on object and class confidence.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    box_confidence -- tensor of shape (19, 19, 5, 1)</span></span><br><span class="line"><span class="string">    boxes -- tensor of shape (19, 19, 5, 4)</span></span><br><span class="line"><span class="string">    box_class_probs -- tensor of shape (19, 19, 5, 80)</span></span><br><span class="line"><span class="string">    threshold -- real value, if [ highest class probability score &lt; threshold], then get rid of the corresponding box</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    scores -- tensor of shape (None,), containing the class probability score for selected boxes</span></span><br><span class="line"><span class="string">    boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes</span></span><br><span class="line"><span class="string">    classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: "None" is here because you don't know the exact number of selected boxes, as it depends on the threshold. </span></span><br><span class="line"><span class="string">    For example, the actual output size of scores would be (10,) if there are 10 boxes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Compute box scores</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line)</span></span><br><span class="line">    box_scores = box_confidence * box_class_probs</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Find the box_classes thanks to the max box_scores, keep track of the corresponding score</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines)</span></span><br><span class="line">    box_classes = K.argmax(box_scores, axis=<span class="number">-1</span>)    <span class="comment">#得到box的类别 (19,19,5)</span></span><br><span class="line">    box_class_scores = K.max(box_scores, axis=<span class="number">-1</span>, keepdims=<span class="keyword">False</span>)  <span class="comment">#得到box这个类别的分数(19,19,5)</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Create a filtering mask based on "box_class_scores" by using "threshold". The mask should have the</span></span><br><span class="line">    <span class="comment"># same dimension as box_class_scores, and be True for the boxes you want to keep (with probability &gt;= threshold)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line)</span></span><br><span class="line">    filtering_mask = box_class_scores &gt;= threshold</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 4: Apply the mask to scores, boxes and classes</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines)</span></span><br><span class="line">    scores = tf.boolean_mask(box_class_scores, filtering_mask)</span><br><span class="line">    boxes = tf.boolean_mask(boxes, filtering_mask)</span><br><span class="line">    classes = tf.boolean_mask(box_classes, filtering_mask)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> scores, boxes, classes</span><br></pre></td></tr></table></figure><h1 id="Non-max-suppression"><a href="#Non-max-suppression" class="headerlink" title="Non-max suppression"></a>Non-max suppression</h1><p>找到了这些boxes后，还需要进行筛选过滤掉。先完成一个IOU算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: iou</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou</span><span class="params">(box1, box2)</span>:</span></span><br><span class="line">    <span class="string">"""Implement the intersection over union (IoU) between box1 and box2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    box1 -- first box, list object with coordinates (x1, y1, x2, y2)</span></span><br><span class="line"><span class="string">    box2 -- second box, list object with coordinates (x1, y1, x2, y2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the (y1, x1, y2, x2) coordinates of the intersection of box1 and box2. Calculate its Area.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 5 lines)</span></span><br><span class="line">    xi1 = np.maximum(box1[<span class="number">0</span>], box2[<span class="number">0</span>])</span><br><span class="line">    yi1 = np.maximum(box1[<span class="number">1</span>], box2[<span class="number">1</span>])</span><br><span class="line">    xi2 = np.minimum(box1[<span class="number">2</span>], box2[<span class="number">2</span>])</span><br><span class="line">    yi2 = np.minimum(box1[<span class="number">3</span>], box2[<span class="number">3</span>])</span><br><span class="line">    inter_area = max(xi2 - xi1,<span class="number">0</span>) * max(yi2 - yi1,<span class="number">0</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###    </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines)</span></span><br><span class="line">    box1_area = (box1[<span class="number">2</span>] - box1[<span class="number">0</span>]) * (box1[<span class="number">3</span>] - box1[<span class="number">1</span>])</span><br><span class="line">    box2_area = (box2[<span class="number">2</span>] - box2[<span class="number">0</span>]) * (box2[<span class="number">3</span>] - box2[<span class="number">1</span>])</span><br><span class="line">    union_area = box1_area + box2_area - inter_area</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute the IoU</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line)</span></span><br><span class="line">    iou = inter_area / union_area</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure><p>tensorflow已经帮你实现了iou算法了，不用用自己刚才写的了：</p><ul><li><a href="https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression" target="_blank" rel="noopener">tf.image.non_max_suppression()</a></li><li><a href="https://www.tensorflow.org/api_docs/python/tf/gather" target="_blank" rel="noopener">K.gather()</a></li></ul><p>思想就是拿掉IOU比较大的那些box</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: yolo_non_max_suppression</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_non_max_suppression</span><span class="params">(scores, boxes, classes, max_boxes = <span class="number">10</span>, iou_threshold = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Applies Non-max suppression (NMS) to set of boxes</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    scores -- tensor of shape (None,), output of yolo_filter_boxes()</span></span><br><span class="line"><span class="string">    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later)</span></span><br><span class="line"><span class="string">    classes -- tensor of shape (None,), output of yolo_filter_boxes()</span></span><br><span class="line"><span class="string">    max_boxes -- integer, maximum number of predicted boxes you'd like</span></span><br><span class="line"><span class="string">    iou_threshold -- real value, "intersection over union" threshold used for NMS filtering</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    scores -- tensor of shape (, None), predicted score for each box</span></span><br><span class="line"><span class="string">    boxes -- tensor of shape (4, None), predicted box coordinates</span></span><br><span class="line"><span class="string">    classes -- tensor of shape (, None), predicted class for each box</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: The "None" dimension of the output tensors has obviously to be less than max_boxes. Note also that this</span></span><br><span class="line"><span class="string">    function will transpose the shapes of scores, boxes, classes. This is made for convenience.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    max_boxes_tensor = K.variable(max_boxes, dtype=<span class="string">'int32'</span>)     <span class="comment"># tensor to be used in tf.image.non_max_suppression()</span></span><br><span class="line">    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) <span class="comment"># initialize variable max_boxes_tensor</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line)</span></span><br><span class="line">    nms_indices = tf.image.non_max_suppression(boxes,scores,max_boxes,iou_threshold)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use K.gather() to select only nms_indices from scores, boxes and classes</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines)</span></span><br><span class="line">    scores = K.gather(scores,nms_indices)</span><br><span class="line">    boxes = K.gather(boxes,nms_indices)</span><br><span class="line">    classes = K.gather(classes,nms_indices)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> scores, boxes, classes</span><br></pre></td></tr></table></figure><p>而后结合刚才的函数，先去掉scores低的，然后运算NMS算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: yolo_eval</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_eval</span><span class="params">(yolo_outputs, image_shape = <span class="params">(<span class="number">720.</span>, <span class="number">1280.</span>)</span>, max_boxes=<span class="number">10</span>, score_threshold=<span class="number">.6</span>, iou_threshold=<span class="number">.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors:</span></span><br><span class="line"><span class="string">                    box_confidence: tensor of shape (None, 19, 19, 5, 1)</span></span><br><span class="line"><span class="string">                    box_xy: tensor of shape (None, 19, 19, 5, 2)</span></span><br><span class="line"><span class="string">                    box_wh: tensor of shape (None, 19, 19, 5, 2)</span></span><br><span class="line"><span class="string">                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)</span></span><br><span class="line"><span class="string">    image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype)</span></span><br><span class="line"><span class="string">    max_boxes -- integer, maximum number of predicted boxes you'd like</span></span><br><span class="line"><span class="string">    score_threshold -- real value, if [ highest class probability score &lt; threshold], then get rid of the corresponding box</span></span><br><span class="line"><span class="string">    iou_threshold -- real value, "intersection over union" threshold used for NMS filtering</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    scores -- tensor of shape (None, ), predicted score for each box</span></span><br><span class="line"><span class="string">    boxes -- tensor of shape (None, 4), predicted box coordinates</span></span><br><span class="line"><span class="string">    classes -- tensor of shape (None,), predicted class for each box</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve outputs of the YOLO model (≈1 line)</span></span><br><span class="line">    box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert boxes to be ready for filtering functions </span></span><br><span class="line">    boxes = yolo_boxes_to_corners(box_xy, box_wh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use one of the functions you've implemented to perform Score-filtering with a threshold of score_threshold (≈1 line)</span></span><br><span class="line">    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, score_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Scale boxes back to original image shape.</span></span><br><span class="line">    boxes = scale_boxes(boxes, image_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use one of the functions you've implemented to perform Non-max suppression with a threshold of iou_threshold (≈1 line)</span></span><br><span class="line">    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> scores, boxes, classes</span><br></pre></td></tr></table></figure><p>进行预测:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(sess, image_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Runs the graph stored in "sess" to predict boxes for "image_file". Prints and plots the preditions.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    sess -- your tensorflow/Keras session containing the YOLO graph</span></span><br><span class="line"><span class="string">    image_file -- name of an image stored in the "images" folder.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    out_scores -- tensor of shape (None, ), scores of the predicted boxes</span></span><br><span class="line"><span class="string">    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes</span></span><br><span class="line"><span class="string">    out_classes -- tensor of shape (None, ), class index of the predicted boxes</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: "None" actually represents the number of predicted boxes, it varies between 0 and max_boxes. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Preprocess your image</span></span><br><span class="line">    image, image_data = preprocess_image(<span class="string">"images/"</span> + image_file, model_image_size = (<span class="number">608</span>, <span class="number">608</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run the session with the correct tensors and choose the correct placeholders in the feed_dict.</span></span><br><span class="line">    <span class="comment"># You'll need to use feed_dict=&#123;yolo_model.input: ... , K.learning_phase(): 0&#125;)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line)</span></span><br><span class="line">    out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes], feed_dict = &#123;yolo_model.input:image_data, K.learning_phase(): <span class="number">0</span>&#125;)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print predictions info</span></span><br><span class="line">    print(<span class="string">'Found &#123;&#125; boxes for &#123;&#125;'</span>.format(len(out_boxes), image_file))</span><br><span class="line">    <span class="comment"># Generate colors for drawing bounding boxes.</span></span><br><span class="line">    colors = generate_colors(class_names)</span><br><span class="line">    <span class="comment"># Draw bounding boxes on the image file</span></span><br><span class="line">    draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)</span><br><span class="line">    <span class="comment"># Save the predicted bounding box on the image</span></span><br><span class="line">    image.save(os.path.join(<span class="string">"out"</span>, image_file), quality=<span class="number">90</span>)</span><br><span class="line">    <span class="comment"># Display the results in the notebook</span></span><br><span class="line">    output_image = scipy.misc.imread(os.path.join(<span class="string">"out"</span>, image_file))</span><br><span class="line">    imshow(output_image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> out_scores, out_boxes, out_classes</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktc3ohoj20ai06777k.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本周的作业实现了YOLO算法，并用于自动驾驶的目标检测中。&lt;/p&gt;
&lt;h1 id=&quot;Mode
      
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(4-3)-- 目标检测（Object detection）</title>
    <link href="http://fangzh.top/2018/dl-ai-4-3/"/>
    <id>http://fangzh.top/2018/dl-ai-4-3/</id>
    <published>2018-10-11T09:02:09.000Z</published>
    <updated>2018-10-11T13:00:12.827Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>这一周主要讲了卷积神经网络的进一步应用：目标检测。</p><p>主要内容有：目标定位、特征点检测、目标检测、滑动窗口、Bounding Box，IOU，NMS，Anchor Boxes，Yolo算法。</p><a id="more"></a><h1 id="目标定位（Object-localization）"><a href="#目标定位（Object-localization）" class="headerlink" title="目标定位（Object localization）"></a>目标定位（Object localization）</h1><p>在进行目标检测之前，需要先了解一下目标定位。</p><p>因为进行目标检测的时候需要预测出目标的具体位置，所以在训练的时候需要先标定一下这个目标的实际位置。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktch0hzj20ot0e1juw.jpg" alt=""></p><p>假设我们需要分类的类别有3个，行人，汽车，自行车，如果什么都没有，那么就是背景。可以看到，y一共有8个参数：</p><ul><li>$P_c$：是否有目标</li><li>$b_x,b_y,b_h,b_w$：目标的位置x,y，高宽h,w</li><li>$c_1,c_2,c_3$：行人、汽车、自行车</li></ul><p>如果$P_c = 0$那么表示没有目标，那么我们就不关心后面的其他参数了。</p><h1 id="特征点检测-Landmark-detection"><a href="#特征点检测-Landmark-detection" class="headerlink" title="特征点检测(Landmark detection)"></a>特征点检测(Landmark detection)</h1><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktegta7j20p50e0tga.jpg" alt=""></p><p>如果是要检测人脸，那么可以在人脸上标注若干个特征点，假设有64个特征点，那么这个时候就有128个参数了，再加上判断是否有人脸，就有129个参数。</p><p>假设要检测的是人体肢体的动作，那么同样也可以标注若干个肢体上的特征点。</p><p>注意，这些都是需要<strong>人工进行标注</strong>的。</p><h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><p><strong>滑动窗口</strong></p><p>目标检测通常采用的是滑动窗口的方法来检测的。也就是用一定窗口的大小，按照指定的步长，遍历整个图像；而后再选取更大的窗口，再次遍历，依次循环。这样子，每个窗口都相当于一张小图片，对这个小图片进行图像识别，从而得到预测结果。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktcgv82j20oo0dy0wz.jpg" alt=""></p><p>但是这个方法有个很明显的问题，就是每个窗口都要进行一次图像识别，速度太慢了。因此就有了滑动窗口的卷积实现。在此之前，我们需要知道如何把全连接层变为卷积层。</p><p><strong>全连接层转化为卷积层</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktb7cdvj20ov0dx0tx.jpg" alt=""></p><p>如图，在经过Max pool后，我们得到了$5 \times 5 \times 16$的图像，经过第一个FC层后变成了400个节点。</p><p>而此时我们可以使用400个$5\times5$的卷积核，进行卷积，得到了$1\times1\times400$</p><p>而后再使用400个$1\times1$的卷积核，再得到了$1\times1\times400$矩阵，所以我们就将全连接层转化成了卷积层。</p><p><strong>卷积滑动窗口的实现</strong></p><p>因为之前的滑动窗口每一次都要进行一次计算，太慢了。所以利用上面的全连接层转化为卷积层的做法，可以一次性把滑动窗口的结果都计算出来。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktb94djj20ns09k0vh.jpg" alt=""></p><p>为了方面观察，这里把三维图像画成了平面。</p><p>假设滑动的窗口是$14\times14\times3$，原图像大小是$16\times\times16\times3$。</p><p>蓝色表示滑动窗口，如果步数是2的话，很容易可以得到$2\times2$的图像，不难看出，在图中最后输出的左上角的蓝色部分就是原图中蓝色部分的计算结果，以此类推。</p><p>也就是说，只需要原图进行一次运算，就可以一次性得到多个滑动窗口的输出值。</p><p>具体例子如下图：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktb8z1tj20no0cdwhr.jpg" alt=""></p><p>可以看到，原图为$28\times28$，最后得到了$8\times8 = 64$个滑动窗口。</p><h1 id="Bounding-Box"><a href="#Bounding-Box" class="headerlink" title="Bounding Box"></a>Bounding Box</h1><p>上面介绍的滑动窗口的方法有一个问题，就是很多情况下并不能检测出窗口的精确位置。</p><p>那么如何找到这个准确的边界框呢？有一个很快的算法叫做YOLO(you only look once)，只需要计算一次便可以检测出物体的位置。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktcya16j20p10btq8q.jpg" alt=""></p><p>如图，首先，将图片分为$n \times n$个部分，如图是划分成了$3\times3=9$份，而每一份都由一个向量y来表示。</p><p>因此最终得到了$3\times3\times8$的矩阵。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktblfnuj20h70cg414.jpg" alt=""></p><p>要得到这个$3\times3\times8$的矩阵，只要选择适当的卷积神经网络，让输出矩阵为这个矩阵就行，而每一个小图像都有一个目标标签y，这个时候y中的$b_x,b_y$都是这个小图像的相对位置，在0-1之间，而$b_h,b_w$是可以大于1的，因为整个大目标有可能在框框外。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktb8qxxj20v606i0v4.jpg" alt=""></p><p>在实际过程中可以选用更精细的划分，如$19\times19$。</p><h1 id="交并比-Intersection-over-Union-IoU"><a href="#交并比-Intersection-over-Union-IoU" class="headerlink" title="交并比(Intersection over Union, IoU)"></a>交并比(Intersection over Union, IoU)</h1><p>如何判断框框是否正确呢？</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4kte96wxj20ay0at7af.jpg" alt=""></p><p>如图红色为车身的框，而紫色为检测到的框，那么紫色的框到底算不算有车呢？</p><p>这个时候可以用交并比来判断，也就是两个框框的交集和并集之比。</p><p>$$IoU = \frac{交集面积}{并集面积}$$</p><p>一般来说 $IoU  \geq 0.5$，那么说明检测正确，当然，这个阈值可以自己设定。</p><h1 id="非最大值抑制（NMS）"><a href="#非最大值抑制（NMS）" class="headerlink" title="非最大值抑制（NMS）"></a>非最大值抑制（NMS）</h1><p>在实际过程中，很可能很多个框框都检测出同一个物体，那么如何判断这些边界框检测的是同一个对象呢？</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktdewjvj20a6088tdv.jpg" alt=""></p><ul><li>首先，每一个框都会返回一个概率$P_c$，我们需要先去掉那些概率比较低的框，如去掉$P_c \leq 0.55$的框。</li><li>而后，在$P_c$中找到概率最大的框，然后用算法遍历其他的边框，找出并取消掉和这个边框IoU大于0.5的框（因为如果IoU大于0.5，我们可以认为是同一个物体）</li><li>循环第二步的操作</li></ul><p>如果有多个目标类别的检测，那么对每个类别分别进行上面的NMS算法。</p><h1 id="Anchor-Box"><a href="#Anchor-Box" class="headerlink" title="Anchor Box"></a>Anchor Box</h1><p>如果一张格子中有多个目标，那怎么办？这时候就需要Anchor Box了，可以同时检测出多个对象。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktbwaixj2064065mzy.jpg" alt=""></p><p>我们预先定义了两个不同形状的Anchor box，如比较高的来检测人，比较宽的来检测汽车，然后重新定义了目标向量y：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktb6okuj20e50bmq38.jpg" alt=""></p><p>这个时候最后输出的矩阵从原来的$3\times3\times8$变成了$3\times3\times16$，也可以是$3\times3\times2\times8$</p><p>在计算的时候就可以根据不同的box输出了，？号表示我们不关系这个值。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktb77nnj20dn0dtmxs.jpg" alt=""></p><p><strong>问题：</strong></p><ul><li>如果使用两个Box，那么如果出现3个目标怎么办，这时候需要别的手段了</li><li>如果同一个格子的两个对象的box相同怎么办，那也需要别的手段来处理了。</li></ul><p>因为这两种情况出现的几率都比较少，所以问题不大。</p><p><strong>注意：</strong></p><ul><li>Anchor box的形状都是人工指定的，一般可以选择5-10种不同的形状，来涵盖我们想要检测的不同对象</li><li>更高级一点的使用k-means聚类算法，将不同的对象形状进行聚类，然后得到一组比较具有代表性的boxes</li></ul><h1 id="YOLO算法"><a href="#YOLO算法" class="headerlink" title="YOLO算法"></a>YOLO算法</h1><p>假设我们需要检测三种目标：行人、汽车、摩托车，使用两种anchor box</p><p> <strong>在训练集中：</strong></p><ul><li>输入同样大小的图片X</li><li>每张图片的输出Y是$3\times3\times16$的矩阵</li><li>人工标定输出Y</li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktdbjbij20om0de42t.jpg" alt=""></p><p><strong>预测：</strong></p><p>输入图片和训练集大小相同，得到$3\times3\times16$的输出结果</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktc1dooj20p00cjadn.jpg" alt=""></p><p>这个时候得到了很多个框框，如果是两个Anchor box，那么就有$2\times9=18$个预测框框，那么就需要把没用的框框都去掉，也就用到了上面的NMS非最大值抑制算法。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktd13lnj208v08rn2c.jpg" alt=""></p><p><strong>进行NMS算法：</strong></p><ul><li>去掉$P_c$小于某个阈值的框框</li><li>对于每个对象分别使用NMS算法得到最终的边界框。</li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktdzoa1j209z09w461.jpg" alt=""></p><h1 id="候选区域"><a href="#候选区域" class="headerlink" title="候选区域"></a>候选区域</h1><p>这里还介绍了其他的目标检测算法，不过貌似都是比较慢的。</p><p><strong>R-CNN：</strong></p><p>原本的滑动窗口，只有在少部分的区域是可以检测到目标的，很多区域都是背景，所以运算很慢，用R-CNN后，只选择一些候选的窗口，不需要对整个图片进行滑动。</p><p>R-CNN使用的是图像分割算法，将图片分割成很多个色块，从而减少了窗口数量。</p><p>是对每个候选区域进行分类，输出的标签和bounding box</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw4ktfhue7j20xn08lwo3.jpg" alt=""></p><p><strong>Fast R-CNN：</strong></p><p>候选区域，使用滑动窗口在区分所有的候选区域。</p><p><strong>Faster R-CNN：</strong></p><p>使用卷积神经网络而不是图像分割来获得候选区域。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;这一周主要讲了卷积神经网络的进一步应用：目标检测。&lt;/p&gt;
&lt;p&gt;主要内容有：目标定位、特征点检测、目标检测、滑动窗口、Bounding Box，IOU，NMS，Anchor Boxes，Yolo算法。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(4-2)-- 深度卷积网络实例探究（Deep convolutional models:case studies）</title>
    <link href="http://fangzh.top/2018/dl-ai-4-2h/"/>
    <id>http://fangzh.top/2018/dl-ai-4-2h/</id>
    <published>2018-10-09T11:20:57.000Z</published>
    <updated>2018-10-09T11:49:10.706Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>本周作业分为两部分，一部分是keras的基本使用，另一部分是ResNet的构建。</p><a id="more"></a><h1 id="Part1-Keras-–-Tutorial"><a href="#Part1-Keras-–-Tutorial" class="headerlink" title="Part1: Keras – Tutorial"></a>Part1: Keras – Tutorial</h1><p>Keras是TensorFlow的高层封装，可以更高效的实现神经网络的搭建。</p><p>先导入库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> kt_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><p><strong>构建模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HappyModel</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the HappyModel.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Feel free to use the suggested outline in the text above to get started, and run through the whole</span></span><br><span class="line">    <span class="comment"># exercise (including the later portions of this notebook) once. The come back also try out other</span></span><br><span class="line">    <span class="comment"># network architectures as well. </span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line">    X = Conv2D(<span class="number">32</span>,(<span class="number">7</span>,<span class="number">7</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),name=<span class="string">"Conv0"</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">'bn0'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'max_pool'</span>)(X)</span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>, name=<span class="string">'fc'</span>)(X)</span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">'HappyModel'</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>然后实例化这个模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">happyModel = HappyModel(X_train.shape[<span class="number">1</span>:])</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p>进行优化器和loss的选择</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">happyModel.compile(optimizer=<span class="string">'Adam'</span>,loss=<span class="string">'binary_crossentropy'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">happyModel.fit(x=X_train,y = Y_train,epochs=<span class="number">10</span>,batch_size=<span class="number">32</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p>预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">preds = happyModel.evaluate(X_test,Y_test)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line">print()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Test Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>可以用summary()来看看详细信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.summary()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         (None, 64, 64, 3)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">zero_padding2d_1 (ZeroPaddin (None, 70, 70, 3)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">Conv0 (Conv2D)               (None, 64, 64, 32)        4736      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">bn0 (BatchNormalization)     (None, 64, 64, 32)        128       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">activation_1 (Activation)    (None, 64, 64, 32)        0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pool (MaxPooling2D)      (None, 32, 32, 32)        0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten)          (None, 32768)             0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">fc (Dense)                   (None, 1)                 32769     </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 37,633</span><br><span class="line">Trainable params: 37,569</span><br><span class="line">Non-trainable params: 64</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure><p>用plot_model()来得到详细的graph</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_model(happyModel, to_file=<span class="string">'HappyModel.png'</span>)</span><br><span class="line">SVG(model_to_dot(happyModel).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure><h1 id="Part2-Residual-Networks"><a href="#Part2-Residual-Networks" class="headerlink" title="Part2: Residual Networks"></a>Part2: Residual Networks</h1><p>主要有两个步骤：</p><ul><li>构建基本的ResNet的块</li><li>将块放到一起，变成一个网络，来做图像分类</li></ul><h2 id="1-The-problem-of-very-deep-neural-networks"><a href="#1-The-problem-of-very-deep-neural-networks" class="headerlink" title="1 - The problem of very deep neural networks"></a>1 - The problem of very deep neural networks</h2><p>这一部分非常深的神经网络的一些问题，主要是参数会变得很小或者爆炸，这样子训练的时候就会收敛的很慢，因此，用残差网络可以有效的改善这个问题。</p><h2 id="2-Building-a-Residual-Network"><a href="#2-Building-a-Residual-Network" class="headerlink" title="2 - Building a Residual Network"></a>2 - Building a Residual Network</h2><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw27o433mij21160du404.jpg" alt=""></p><p>根据输入输入的维度不同，分为两种块：</p><p><strong>1. identity block（一致块）</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw27o3rki0j20wu07a0td.jpg" alt=""></p><p>可以看到，identity block的前后两端维度是一致的，可以直接相加。</p><p>在这里我们实现了一个跳跃三层的块。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw27o3qwtuj214i08ot9m.jpg" alt=""></p><p>基本结构是:</p><p>First component of main path:</p><ul><li>The first CONV2D has F1F1 filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. Use 0 as the seed for the random initialization.</li><li>The first BatchNorm is normalizing the channels axis. Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters.</li></ul><p>Second component of main path:</p><ul><li>The second CONV2D has F2F2 filters of shape (f,f)(f,f) and a stride of (1,1). Its padding is “same” and its name should be <code>conv_name_base + &#39;2b&#39;</code>. Use 0 as the seed for the random initialization.</li><li>The second BatchNorm is normalizing the channels axis. Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters.</li></ul><p>Third component of main path:</p><ul><li>The third CONV2D has F3F3 filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2c&#39;</code>. Use 0 as the seed for the random initialization.</li><li>The third BatchNorm is normalizing the channels axis. Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component.</li></ul><p>Final step:</p><ul><li>The shortcut and the input are added together.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters.</li></ul><p>注意在跳跃相加部分要用函数keras的函授Add()，不能用加号，不然会出错。</p><p>这里f是卷积核的大小，filters是这三层卷积层的深度的list，stage指的是哪一大层的网络，用来取名字的，后面有用，block是在stage下的某一层的网络，用a,b,c,d等字母表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: identity_block</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(X, f, filters, stage, block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the identity block as defined in Figure 3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    X = Conv2D(filters = F1, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(filters = F2, kernel_size = (f, f), strides= (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'same'</span>, name = conv_name_base + <span class="string">'2b'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(filters = F3, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides= (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2c'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X,X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p><strong>2. The convolutional block(卷积块)</strong></p><p>当两端的维度不一致时，可以加一个卷积核来转化维度，这时候没有激活函数。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw27o3rbcpj20zy0amjsb.jpg" alt=""></p><p>First component of main path:</p><ul><li>The first CONV2D has F1F1 filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>.</li><li>The first BatchNorm is normalizing the channels axis. Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters.</li></ul><p>Second component of main path:</p><ul><li>The second CONV2D has F2F2 filters of (f,f) and a stride of (1,1). Its padding is “same” and it’s name should be <code>conv_name_base + &#39;2b&#39;</code>.</li><li>The second BatchNorm is normalizing the channels axis. Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters.</li></ul><p>Third component of main path:</p><ul><li>The third CONV2D has F3F3 filters of (1,1) and a stride of (1,1). Its padding is “valid” and it’s name should be <code>conv_name_base + &#39;2c&#39;</code>.</li><li>The third BatchNorm is normalizing the channels axis. Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component.</li></ul><p>Shortcut path:</p><ul><li>The CONV2D has F3F3 filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;1&#39;</code>.</li><li>The BatchNorm is normalizing the channels axis. Its name should be <code>bn_name_base + &#39;1&#39;</code>.</li></ul><p>Final step:</p><ul><li>The shortcut and the main path values are added together.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters.</li></ul><p>这里参数新增了s是stride每一步数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span><span class="params">(X, f, filters, stage, block, s = <span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the convolutional block as defined in Figure 4</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    s -- Integer, specifying the stride to be used</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value</span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">##### MAIN PATH #####</span></span><br><span class="line">    <span class="comment"># First component of main path </span></span><br><span class="line">    X = Conv2D(F1, (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(F2, (f, f), strides = (<span class="number">1</span>,<span class="number">1</span>), name = conv_name_base + <span class="string">'2b'</span>, padding = <span class="string">'same'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(F3, (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), name = conv_name_base + <span class="string">'2c'</span>, padding = <span class="string">'valid'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### SHORTCUT PATH #### (≈2 lines)</span></span><br><span class="line">    X_shortcut = Conv2D(F3, (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">'1'</span>, padding = <span class="string">'valid'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X_shortcut)</span><br><span class="line">    X_shortcut = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'1'</span>)(X_shortcut)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X,X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h2 id="3-Building-your-first-ResNet-model-50-layers"><a href="#3-Building-your-first-ResNet-model-50-layers" class="headerlink" title="3 - Building your first ResNet model (50 layers)"></a>3 - Building your first ResNet model (50 layers)</h2><p>构建一个50层的网络，分为5块，结构如下：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw27o3s4ecj212s07w0tt.jpg" alt=""></p><p>The details of this ResNet-50 model are:</p><ul><li>Zero-padding pads the input with a pad of (3,3)</li><li>Stage 1:<ul><li>The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is “conv1”.</li><li>BatchNorm is applied to the channels axis of the input.</li><li>MaxPooling uses a (3,3) window and a (2,2) stride.</li></ul></li><li>Stage 2:<ul><li>The convolutional block uses three set of filters of size [64,64,256], “f” is 3, “s” is 1 and the block is “a”.</li><li>The 2 identity blocks use three set of filters of size [64,64,256], “f” is 3 and the blocks are “b” and “c”.</li></ul></li><li>Stage 3:<ul><li>The convolutional block uses three set of filters of size [128,128,512], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 3 identity blocks use three set of filters of size [128,128,512], “f” is 3 and the blocks are “b”, “c” and “d”.</li></ul></li><li>Stage 4:<ul><li>The convolutional block uses three set of filters of size [256, 256, 1024], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 5 identity blocks use three set of filters of size [256, 256, 1024], “f” is 3 and the blocks are “b”, “c”, “d”, “e” and “f”.</li></ul></li><li>Stage 5:<ul><li>The convolutional block uses three set of filters of size [512, 512, 2048], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 2 identity blocks use three set of filters of size [512, 512, 2048], “f” is 3 and the blocks are “b” and “c”.</li></ul></li><li>The 2D Average Pooling uses a window of shape (2,2) and its name is “avg_pool”.</li><li>The flatten doesn’t have any hyperparameters or name.</li><li>The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be <code>&#39;fc&#39; + str(classes)</code>.</li></ul><p><strong>Exercise</strong>: Implement the ResNet with 50 layers described in the figure above. We have implemented Stages 1 and 2. Please implement the rest. (The syntax for implementing Stages 3-5 should be quite similar to that of Stage 2.) Make sure you follow the naming convention in the text above.</p><p>You’ll need to use this function:</p><ul><li>Average pooling <a href="https://keras.io/layers/pooling/#averagepooling2d" target="_blank" rel="noopener">see reference</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: ResNet50</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">(input_shape = <span class="params">(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span>, classes = <span class="number">6</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the popular ResNet50 the following architecture:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3</span></span><br><span class="line"><span class="string">    -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string">    classes -- integer, number of classes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input as a tensor with shape input_shape</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zero-Padding</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Stage 1</span></span><br><span class="line">    X = Conv2D(<span class="number">64</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">2</span>, <span class="number">2</span>), name = <span class="string">'conv1'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">'bn_conv1'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 2</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage = <span class="number">2</span>, block=<span class="string">'a'</span>, s = <span class="number">1</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 3 (≈4 lines)</span></span><br><span class="line"></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">'a'</span>, s = <span class="number">2</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">'c'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 4 (≈6 lines)</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'a'</span>, s = <span class="number">2</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">'c'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">'d'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">'e'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 5 (≈3 lines)</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">512</span>, <span class="number">512</span>, <span class="number">2048</span>], stage = <span class="number">5</span>, block=<span class="string">'a'</span>, s = <span class="number">2</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">512</span>, <span class="number">512</span>, <span class="number">2048</span>], stage=<span class="number">5</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">512</span>, <span class="number">512</span>, <span class="number">2048</span>], stage=<span class="number">5</span>, block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># AVGPOOL (≈1 line). Use "X = AveragePooling2D(...)(X)"</span></span><br><span class="line">    X = AveragePooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'valid'</span>)(X)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># output layer</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(classes, activation=<span class="string">'softmax'</span>, name=<span class="string">'fc'</span> + str(classes), kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create model</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">'ResNet50'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本周作业分为两部分，一部分是keras的基本使用，另一部分是ResNet的构建。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(4-2)-- 深度卷积网络实例探究（Deep convolutional models:case studies）</title>
    <link href="http://fangzh.top/2018/dl-ai-4-2/"/>
    <id>http://fangzh.top/2018/dl-ai-4-2/</id>
    <published>2018-10-09T09:17:04.000Z</published>
    <updated>2018-10-09T11:49:06.379Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>本周主要讲了深度卷积网络的一些模型：LeNet,AlexNet,VGGNet,ResNet,Inception,1×1卷积，迁移学习等。</p><a id="more"></a><h1 id="经典的卷积网络"><a href="#经典的卷积网络" class="headerlink" title="经典的卷积网络"></a>经典的卷积网络</h1><p>经典的卷及网络有三种：LeNet、AlexNet、VGGNet。</p><p><strong>LeNet-5</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgc3e4j20ox0c5t9k.jpg" alt=""></p><p>LeNet-5主要是单通道的手写字体的识别，这是80年代提出的算法，当时没有用padding，而且pooling用的是average pooling，但是现在大家都用max pooling了。</p><p>论文中的最后预测用的是sigmoid和tanh，而现在都用了softmax。</p><p><strong>AlexNet</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgltf9j20p20d9q3w.jpg" alt=""></p><p>AlexNet是2012年提出的算法。用来对彩色的图片进行处理，其实大致的结构和LeNet-5是很相似的，但是网络更大，参数更多了。</p><p>这个时候已经用Relu来作为激活函数了，而且用了多GPU进行计算。</p><p><strong>VGG-16</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgmomwj217i0n4di7.jpg" alt=""></p><p>VGG-16是2015的论文，比较简化的是，卷积层和池化层都是用相同的卷积核大小，卷积核都是3×3，stride=1，same padding，池化层用的maxpooling，为2×2，stride=2。只是在卷积的时候改变了每一层的通道数。</p><p>网络很大，参数有1.38亿个参数。</p><p><strong>建议阅读论文顺序：AlexNet-&gt;VGG-&gt;LeNet</strong></p><h1 id="Residual-Network-残差网络"><a href="#Residual-Network-残差网络" class="headerlink" title="Residual Network(残差网络)"></a>Residual Network(残差网络)</h1><p>残差网络是由若干个残差块组成的。</p><p>因为在非常深的网络中会存在梯度消失和梯度爆炸的问题，为此，引入了<strong>Skip Connection</strong>来解决，也就是残差网络的实现。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgavrvj20bc05sdfw.jpg" alt=""></p><p>上图即为一个残差块的基本原理，在原本的传播过程(称为主线)中，加上了$a^{[l]}$到$z^{[l+2]}$的连接，成为’short cut’或者’skip connetction’。</p><p>所以输出的表达式变成了:$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgcjiqj20yv0c7gml.jpg" alt=""></p><p>残差网络是由多个残差块组成的：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgdlpfj20xc08gq3p.jpg" alt=""></p><p>没有残差网络和加上残差网络的效果对比，可以看到，随着layers的增加，ResNet表现的更好：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgd9gaj20yo0anq3e.jpg" alt=""></p><h1 id="ResNet为何有用？"><a href="#ResNet为何有用？" class="headerlink" title="ResNet为何有用？"></a>ResNet为何有用？</h1><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgwuudj20io05vq50.jpg" alt=""></p><p>假设我们已经经过了一个很大的神经网络Big NN,得到了$a^{[l]}$</p><p>那么这个时候再经过两层的神经网络得到$a^{[l+2]}$,那么表达式为：</p><p>$$a^{[l+2]} = g(z^{[l+2]} + a^{[l]}) = g(W^{[l+2]} a^{[l+2]} + b^{[l+2]} + a^{[l]})$$</p><p>如果加上正则化，那么权值就会很小，假设$W^{[l+2]},b^{[l+2]} = 0$， 因为激活函数是Relu，所以</p><p>$$a^{[l+2]} = g(a^{[l]}) = a^{[l]}$$</p><p>所以可以看到，加上残差块以后，更深的网络最差也只是和前面的效果一样，何况还有可能更好。</p><p>如果只是普通的两层网络，那么结果可能更好，也可能更差。</p><p>注意的是$a^{[l+2]}$要和$a^{[l]}$的维度一样，可以使用same padding，来保持维度。</p><h1 id="1×1卷积"><a href="#1×1卷积" class="headerlink" title="1×1卷积"></a>1×1卷积</h1><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgeo2fj20hs09vq2z.jpg" alt=""></p><p>用1×1的卷积核可以来减少通道数，从而减少参数个数。</p><h1 id="Inception-Network"><a href="#Inception-Network" class="headerlink" title="Inception Network"></a>Inception Network</h1><p>Inception的主要好处就是不需要人工来选择filter的大小和是否要添加池化层的问题。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgr815j20nj0bo756.jpg" alt=""></p><p>如图可以一次性把各个卷积核的大小和max pool一起加进去，然后让机器自己学习里面的参数。</p><p>但是这样有一个问题，就是计算量太大了，假设是上面的$5 \times 5 \times 192$的卷积核，有32个，这样一共要进行$28\times\28\times32\times5\times5\times192=120M$的乘法次数，运算量是很大的。</p><p>如何解决这个问题呢？就需要用到前面的1×1的卷积核了。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgtwmtj20og0dqgmd.jpg" alt=""></p><p>可以看到经过维度压缩，计算次数少了十倍。</p><h1 id="Inception-网络"><a href="#Inception-网络" class="headerlink" title="Inception 网络"></a>Inception 网络</h1><p>单个的inception模块如下：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgsv81j215g0m70w1.jpg" alt=""></p><p>构成的google net如下：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kh9sd1j21eh0quk1o.jpg" alt=""></p><h1 id="使用开源的实现方案"><a href="#使用开源的实现方案" class="headerlink" title="使用开源的实现方案"></a>使用开源的实现方案</h1><p>别人已经实现的网络已经很厉害了，我觉得重复造轮子很没有必要，而且浪费时间，何况你水平也没有别人高。。还不如直接用别人的网络，然后稍加改造，这样可以很快的实现你的想法。</p><p>在GitHub上找到自己感兴趣的网络结构fork过来，好好研究！</p><h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p>之前已经讲过迁移学习了，也就是用别人训练好的网络，固定他们已经训练好的网络参数，然后套到自己的训练集上，完成训练。</p><p>如果你只有很少的数据集，那么，改变已有网络的最后一层softmax就可以了，比如原来别人的模型是有1000个分类，现在你只需要有3个分类。然后freeze冻结前面隐藏层的所有参数不变。这样就好像是你自己在训练一个很浅的神经网络，把隐藏层看做一个函数来映射，只需要训练最后的softmax层就可以了。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgqbpqj20no062aca.jpg" alt=""></p><p>如果你有一定量的数据，那么freeze的范围可以减少，你可以训练后面的几层隐藏层，或者自己设计后面的隐藏层。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fw26kgiq0xj20nm04wabg.jpg" alt=""></p><h1 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h1><p>数据不够的话，进行数据扩充是很有用的。</p><p>可以采用</p><ul><li>镜像</li><li>随机裁剪</li><li>色彩转换color shifting（如三通道：R+20,G-20,B+20）等等</li></ul><p><strong>tips:</strong></p><p>在数据比赛中</p><ul><li>ensembling：训练多个网络模型，然后平均结果，或者加权平均</li><li>测试时使用muti-crop，也就是在把单张测试图片用数据扩充的形式变成很多张，然后运行分类器，得到的结果进行平均。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本周主要讲了深度卷积网络的一些模型：LeNet,AlexNet,VGGNet,ResNet,Inception,1×1卷积，迁移学习等。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(4-1)-- 卷积神经网络（Foundations of CNN）</title>
    <link href="http://fangzh.top/2018/dl-ai-4-1h/"/>
    <id>http://fangzh.top/2018/dl-ai-4-1h/</id>
    <published>2018-09-30T08:07:23.000Z</published>
    <updated>2018-09-30T09:53:32.577Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>本周的作业分为了两部分：</p><ul><li>卷积神经网络的模型搭建</li><li>用TensorFlow来训练卷积神经网络</li></ul><a id="more"></a><h1 id="Part1：Convolutional-Neural-Networks-Step-by-Step"><a href="#Part1：Convolutional-Neural-Networks-Step-by-Step" class="headerlink" title="Part1：Convolutional Neural Networks: Step by Step"></a>Part1：Convolutional Neural Networks: Step by Step</h1><p>主要内容：</p><ul><li>convolution funtions:<ul><li>Zero Padding</li><li>Convolve window</li><li>Convolution forward</li><li>Convolution backward (optional)</li></ul></li><li>Pooling functions：<ul><li>Pooling forward</li><li>Create mask</li><li>Distribute value</li><li>Pooling backward (optional)</li></ul></li></ul><h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><p>创建CNN的主要函数</p><p><strong>1. Zero Padding</strong></p><p>先创建一个padding函数，用来输入图像X，输出padding后的图像，这里使用的是<code>np.pad()</code>函数，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.pad(a, ((<span class="number">0</span>,<span class="number">0</span>), (<span class="number">1</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">0</span>), (<span class="number">3</span>,<span class="number">3</span>), (<span class="number">0</span>,<span class="number">0</span>)), <span class="string">'constant'</span>, constant_values = (..,..))</span><br><span class="line">表示a有<span class="number">5</span>个维度，在第<span class="number">1</span>维的两边都填上<span class="number">1</span>个pad，和第<span class="number">3</span>维的两边都填上<span class="number">3</span>个pad，constant_values表示两边要填充的值</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_pad</span><span class="params">(X, pad)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, </span></span><br><span class="line"><span class="string">    as illustrated in Figure 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images</span></span><br><span class="line"><span class="string">    pad -- integer, amount of padding around each image on vertical and horizontal dimensions</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line)</span></span><br><span class="line">    X_pad = np.pad(X, ((<span class="number">0</span>,<span class="number">0</span>),(pad,pad),(pad,pad),(<span class="number">0</span>,<span class="number">0</span>)), <span class="string">'constant'</span>, constant_values=(<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_pad</span><br></pre></td></tr></table></figure><p><strong>2.Single step of convolution</strong></p><p>创建一个单步的卷积运算，也就是一次输入一个切片，大小和卷积核相同，对应元素相乘再求和，最后再加个bias项。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: conv_single_step</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_single_step</span><span class="params">(a_slice_prev, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation </span></span><br><span class="line"><span class="string">    of the previous layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    <span class="comment"># Element-wise product between a_slice and W. Do not add the bias yet.</span></span><br><span class="line">    s = a_slice_prev * W</span><br><span class="line">    <span class="comment"># Sum over all entries of the volume s.</span></span><br><span class="line">    Z = np.sum(s)</span><br><span class="line">    <span class="comment"># Add bias b to Z. Cast b to a float() so that Z results in a scalar value.</span></span><br><span class="line">    Z = Z + float(b)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><p><strong>3.Convolutional Neural Networks - Forward pass</strong></p><p>创建一次完整的卷积过程，也就是利用上面的一次卷积，进行for循环。进行切片的时候，注意边界<code>vert_start, vert_end, horiz_start and horiz_end</code></p><p>这一步应该先弄清楚A_prev，A，W，b的维度，超参数项包括了stride和pad</p><p>$$ n_H = \lfloor \frac{n_{H_{prev}} - f + 2 \times pad}{stride} \rfloor +1 $$<br>$$ n_W = \lfloor \frac{n_{W_{prev}} - f + 2 \times pad}{stride} \rfloor +1 $$<br>$$ n_C = \text{number of filters used in the convolution}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: conv_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(A_prev, W, b, hparameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    b -- Biases, numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "stride" and "pad"</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward() function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape (≈1 line)  </span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W's shape (≈1 line)</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from "hparameters" (≈2 lines)</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    pad = hparameters[<span class="string">'pad'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)</span></span><br><span class="line">    n_H = int((n_H_prev + <span class="number">2</span> * pad - f) / stride + <span class="number">1</span>)</span><br><span class="line">    n_W = int((n_W_prev + <span class="number">2</span> * pad - f) / stride + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the output volume Z with zeros. (≈1 line)</span></span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create A_prev_pad by padding A_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over the batch of training examples</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i]                               <span class="comment"># Select ith training example's padded activation</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                           <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                       <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):                   <span class="comment"># loop over channels (= #filters) of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice" (≈4 lines)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = h * stride + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = w * stride + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)</span></span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start : vert_end, horiz_start : horiz_end]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)</span></span><br><span class="line">                    Z[i, h, w, c] = conv_single_step(a_slice_prev,W[:,:,:,c],b[:,:,:,c])</span><br><span class="line">                                        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save information in "cache" for the backprop</span></span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h2 id="Pooling-layer"><a href="#Pooling-layer" class="headerlink" title="Pooling layer"></a>Pooling layer</h2><p>创建池化层，注意得到的维度需要向下取整，用int()对float()进行转换</p><p>$$ n_H = \lfloor \frac{n_{H_{prev}} - f}{stride} \rfloor +1 $$<br>$$ n_W = \lfloor \frac{n_{W_{prev}} - f}{stride} \rfloor +1 $$<br>$$ n_C = n_{C_{prev}}$$</p><p>同样需要先进行切边，而后分为max和average两种，分别用np.max和np.mean</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_forward</span><span class="params">(A_prev, hparameters, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward pass of the pooling layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "f" and "stride"</span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from the input shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve hyperparameters from "hparameters"</span></span><br><span class="line">    f = hparameters[<span class="string">"f"</span>]</span><br><span class="line">    stride = hparameters[<span class="string">"stride"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the dimensions of the output</span></span><br><span class="line">    n_H = int(<span class="number">1</span> + (n_H_prev - f) / stride)</span><br><span class="line">    n_W = int(<span class="number">1</span> + (n_W_prev - f) / stride)</span><br><span class="line">    n_C = n_C_prev</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize output matrix A</span></span><br><span class="line">    A = np.zeros((m, n_H, n_W, n_C))              </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                         <span class="comment"># loop over the training examples</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                     <span class="comment"># loop on the vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                 <span class="comment"># loop on the horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range (n_C):            <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice" (≈4 lines)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)</span></span><br><span class="line">                    a_prev_slice = A_prev[i, vert_start : vert_end, horiz_start : horiz_end, c]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.max(a_prev_slice)</span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.mean(a_prev_slice)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Store the input and hparameters in "cache" for pool_backward()</span></span><br><span class="line">    cache = (A_prev, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><h2 id="Backpropagation-in-convolutional-neural-networks"><a href="#Backpropagation-in-convolutional-neural-networks" class="headerlink" title="Backpropagation in convolutional neural networks"></a>Backpropagation in convolutional neural networks</h2><p>卷积神经网络的求导是比较难以理解的，这里有卷积层的求导和池化层的求导。</p><h3 id="1-Convolutional-layer-backward-pass"><a href="#1-Convolutional-layer-backward-pass" class="headerlink" title="1.Convolutional layer backward pass"></a>1.Convolutional layer backward pass</h3><p>假设经过卷积层后我们的输出$Z = W \times A +b$</p><p>那么反向传播过程中需要求的就是$dA,dW,db$，其中$dA$是原输入的数据，包含了原图像中的每一个像素，</p><p>而这个时候假设从后面传过来的$dZ$是已经知道的。</p><p><strong>1.计算dA</strong></p><p>从公式可以看出，$dA = W \times dZ$，具体一点，$dA$的每一个切片就是$W_c$乘上$dZ$在输出图片的<strong>每一个像素</strong>的求和结果，从矩阵的角度，每一次$W_c\times dZ_{hw}$得到的就是从<strong>单个输出的图片像素到输入图片切片（大小为W）</strong>的映射。因此公式为：</p><p>$$ dA += \sum _{h=0} ^{n_H} \sum_{w=0} ^{n_W} W_c \times dZ_{hw} $$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><p><strong>2.计算dW</strong></p><p>$dW = A \times dZ$，而更具体一点，因为<strong>W对Z的每一个像素都是有作用的</strong>，所以就等于每一个输入图片的切片乘以对应输出图片像素的导数，然后再求和！</p><p>$$ dW_c  += \sum _{h=0} ^{n_H} \sum_{w=0} ^ {n_W} a_{slice} \times dZ_{hw}  $$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><p><strong>3.计算db</strong></p><p>$$ db = \sum_h \sum_w dZ_{hw} $$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db[:,:,:,c] += dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><p>所以得到以下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward(), output of conv_forward()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),</span></span><br><span class="line"><span class="string">               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    dW -- gradient of the cost with respect to the weights of the conv layer (W)</span></span><br><span class="line"><span class="string">          numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    db -- gradient of the cost with respect to the biases of the conv layer (b)</span></span><br><span class="line"><span class="string">          numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (A_prev, W, b, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W's shape</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from "hparameters"</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    pad = hparameters[<span class="string">'pad'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from dZ's shape</span></span><br><span class="line">    (m, n_H, n_W, n_C) = dZ.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev, dW, db with the correct shapes</span></span><br><span class="line">    dA_prev = np.zeros(A_prev.shape)                           </span><br><span class="line">    dW = np.zeros(W.shape)</span><br><span class="line">    db = np.zeros(b.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pad A_prev and dA_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    dA_prev_pad = zero_pad(dA_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select ith training example from A_prev_pad and dA_prev_pad</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i]</span><br><span class="line">        da_prev_pad = dA_prev_pad[i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice"</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = h * stride + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = w * stride + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the slice from a_prev_pad</span></span><br><span class="line">                    a_slice = a_prev_pad[vert_start : vert_end, horiz_start : horiz_end, : ]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Update gradients for the window and the filter's parameters using the code formulas given above</span></span><br><span class="line">                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[ i, h, w ,c]</span><br><span class="line"></span><br><span class="line">                    dW[:,:,:,c] += a_slice * dZ[ i, h, w ,c]</span><br><span class="line">                    db[:,:,:,c] += dZ[ i, h, w ,c]</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])</span></span><br><span class="line">        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h3 id="Pooling-layer-backward-pass"><a href="#Pooling-layer-backward-pass" class="headerlink" title="Pooling layer - backward pass"></a>Pooling layer - backward pass</h3><p>这里max pooling和average poolling要分开处理。</p><p><strong>1. Max pooling - backward pass</strong></p><p>假设pool size是$2 \times 2$的，那么，4个像素中只有1个留下来了，其余的都没有效果了，所以在max pooling中，从后面传递过来的导数值，<strong>只作用在max的那个元素，而且继续往前传递，不做任何改动，在其余3个元素的导数都是0</strong>。</p><p>创建一个mask矩阵，让最大值为1，其余的都为0，这样子就可以作为一个映射矩阵向前映射了。</p><p>$$ X = \begin{bmatrix}1 &amp;&amp; 3  \\ 4 &amp;&amp; 2 \end{bmatrix} \quad \rightarrow  \quad M =\begin{bmatrix}<br>0 &amp;&amp; 0 \\<br>1 &amp;&amp; 0<br>\end{bmatrix}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_mask_from_window</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a mask from an input matrix x, to identify the max entry of x.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Array of shape (f, f)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈1 line)</span></span><br><span class="line">    mask = (x == np.max(x))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><p><strong>2.  Average pooling - backward pass</strong></p><p>和max不同，average pooling相当于把backward传过来的值分成了$n_H \times n_W$等分。所以要计算的参数就比max pooling多很多了，这也就是为什么一般都用max pooling，不用average pooling</p><p>$$ dZ = 1 \quad \rightarrow  \quad dZ =\begin{bmatrix}<br>1/4 &amp;&amp; 1/4 \\<br>1/4 &amp;&amp; 1/4<br>\end{bmatrix}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distribute_value</span><span class="params">(dz, shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Distributes the input value in the matrix of dimension shape</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dz -- input scalar</span></span><br><span class="line"><span class="string">    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Array of size (n_H, n_W) for which we distributed the value of dz</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shape (≈1 line)</span></span><br><span class="line">    (n_H, n_W) = shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the value to distribute on the matrix (≈1 line)</span></span><br><span class="line">    average = n_H * n_W</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a matrix where every entry is the "average" value (≈1 line)</span></span><br><span class="line">    a = dz / average * np.ones((n_H, n_W))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><p>结合两种方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_backward</span><span class="params">(dA, cache, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward pass of the pooling layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A</span></span><br><span class="line"><span class="string">    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters </span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from cache (≈1 line)</span></span><br><span class="line">    (A_prev, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve hyperparameters from "hparameters" (≈2 lines)</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    f = hparameters[<span class="string">'f'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)</span></span><br><span class="line">    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape</span><br><span class="line">    m, n_H, n_W, n_C = dA.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev with zeros (≈1 line)</span></span><br><span class="line">    dA_prev = np.zeros(A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select training example from A_prev (≈1 line)</span></span><br><span class="line">        a_prev = A_prev[i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop on the vertical axis</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop on the horizontal axis</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels (depth)</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice" (≈4 lines)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Compute the backward propagation in both modes.</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Use the corners and "c" to define the current slice from a_prev (≈1 line)</span></span><br><span class="line">                        a_prev_slice = a_prev[vert_start : vert_end, horiz_start : horiz_end, c]</span><br><span class="line">                        <span class="comment"># Create the mask from a_prev_slice (≈1 line)</span></span><br><span class="line">                        mask = create_mask_from_window(a_prev_slice)</span><br><span class="line">                        <span class="comment"># Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i, h, w, c]</span><br><span class="line">                        </span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Get the value a from dA (≈1 line)</span></span><br><span class="line">                        da = dA[i, h, w, c]</span><br><span class="line">                        <span class="comment"># Define the shape of the filter as fxf (≈1 line)</span></span><br><span class="line">                        shape = (f, f)</span><br><span class="line">                        <span class="comment"># Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)</span><br><span class="line">                        </span><br><span class="line">    <span class="comment">### END CODE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev</span><br></pre></td></tr></table></figure><h1 id="Part2：Convolutional-Neural-Networks-Application"><a href="#Part2：Convolutional-Neural-Networks-Application" class="headerlink" title="Part2：Convolutional Neural Networks: Application"></a>Part2：Convolutional Neural Networks: Application</h1><p>用TensorFlow来搭建卷积神经网络。</p><h2 id="1-Create-placeholders"><a href="#1-Create-placeholders" class="headerlink" title="1.Create placeholders"></a>1.Create placeholders</h2><p>先创建placeholders，用来训练中传递X,Y</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_H0, n_W0, n_C0, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates the placeholders for the tensorflow session.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    n_H0 -- scalar, height of an input image</span></span><br><span class="line"><span class="string">    n_W0 -- scalar, width of an input image</span></span><br><span class="line"><span class="string">    n_C0 -- scalar, number of channels of the input</span></span><br><span class="line"><span class="string">    n_y -- scalar, number of classes</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype "float"</span></span><br><span class="line"><span class="string">    Y -- placeholder for the input labels, of shape [None, n_y] and dtype "float"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈2 lines)</span></span><br><span class="line">    X = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>,n_H0, n_W0, n_C0))</span><br><span class="line">    Y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>,n_y))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><h2 id="2-Initialize-parameters"><a href="#2-Initialize-parameters" class="headerlink" title="2.Initialize parameters"></a>2.Initialize parameters</h2><p>用来初始化参数，主要是W1,W2,在这里就没有用b了</p><p>用<code>W = tf.get_variable(&quot;W&quot;, [1,2,3,4], initializer = ...)</code></p><p>initializer 用<code>tf.contrib.layers.xavier_initializer</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes weight parameters to build a neural network with tensorflow. The shapes are:</span></span><br><span class="line"><span class="string">                        W1 : [4, 4, 3, 8]</span></span><br><span class="line"><span class="string">                        W2 : [2, 2, 8, 16]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary of tensors containing W1, W2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)                              <span class="comment"># so that your "random" numbers match ours</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines of code)</span></span><br><span class="line">    W1 = tf.get_variable(<span class="string">'W1'</span>, [<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>],initializer= tf.contrib.layers.xavier_initializer(seed = <span class="number">0</span> ))</span><br><span class="line">    W2 = tf.get_variable(<span class="string">'W2'</span>, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">16</span>],initializer= tf.contrib.layers.xavier_initializer(seed = <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>记得这只是创建了图而已，并没有真正的初始化参数，在执行中还需要</p><p><code>init = tf.global_variables_initializer()</code>    </p><p><code>sess_test.run(init)</code></p><h2 id="3-Forward-propagation"><a href="#3-Forward-propagation" class="headerlink" title="3. Forward propagation"></a>3. Forward propagation</h2><p>模型为：CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">- Conv2D: stride 1, padding is &quot;SAME&quot;</span><br><span class="line">- ReLU</span><br><span class="line">- Max pool: Use an 8 by 8 filter size and an 8 by 8 stride, padding is &quot;SAME&quot;</span><br><span class="line">- Conv2D: stride 1, padding is &quot;SAME&quot;</span><br><span class="line">- ReLU</span><br><span class="line">- Max pool: Use a 4 by 4 filter size and a 4 by 4 stride, padding is &quot;SAME&quot;</span><br><span class="line">- Flatten the previous output.</span><br><span class="line">- FULLYCONNECTED (FC) layer：这里全连接层不需要有激活函数，因为后面计算cost的时候会加上softmax，因此这里不需要加</span><br></pre></td></tr></table></figure><p>用到的函数：</p><ul><li><p><strong>tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = ‘SAME’):</strong> given an input $X$ and a group of filters $W1$, this function convolves $W1$’s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener">here</a></p></li><li><p><strong>tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = ‘SAME’):</strong> given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool" target="_blank" rel="noopener">here</a></p></li><li><p><strong>tf.nn.relu(Z1):</strong> computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/nn/relu" target="_blank" rel="noopener">here.</a></p></li><li><p><strong>tf.contrib.layers.flatten(P)</strong>: given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k]. You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten" target="_blank" rel="noopener">here.</a></p></li><li><p><strong>tf.contrib.layers.fully_connected(F, num_outputs):</strong> given a the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected" target="_blank" rel="noopener">here.</a></p></li></ul><p>In the last function above (<code>tf.contrib.layers.fully_connected</code>), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for the model:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset placeholder, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "W2"</span></span><br><span class="line"><span class="string">                  the shapes are given in initialize_parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z3 -- the output of the last LINEAR unit</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve the parameters from the dictionary "parameters" </span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># CONV2D: stride of 1, padding 'SAME'</span></span><br><span class="line">    Z1 = tf.nn.conv2d(X, filter=W1, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A1 = tf.nn.relu(Z1)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 8x8, sride 8, padding 'SAME'</span></span><br><span class="line">    P1 = tf.nn.max_pool(A1,ksize=[<span class="number">1</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># CONV2D: filters W2, stride 1, padding 'SAME'</span></span><br><span class="line">    Z2 = tf.nn.conv2d(P1, filter=W2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A2 = tf.nn.relu(Z2)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 4x4, stride 4, padding 'SAME'</span></span><br><span class="line">    P2 = tf.nn.max_pool(A2,ksize=[<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># FLATTEN</span></span><br><span class="line">    P2 = tf.contrib.layers.flatten(P2)</span><br><span class="line">    <span class="comment"># FULLY-CONNECTED without non-linear activation function (not not call softmax).</span></span><br><span class="line">    <span class="comment"># 6 neurons in output layer. Hint: one of the arguments should be "activation_fn=None" </span></span><br><span class="line">    Z3 = tf.contrib.layers.fully_connected(P2, <span class="number">6</span>,activation_fn=<span class="keyword">None</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><h2 id="4-Compute-cost"><a href="#4-Compute-cost" class="headerlink" title="4. Compute cost"></a>4. Compute cost</h2><ul><li><strong>tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):</strong> computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  <a href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits" target="_blank" rel="noopener">here.</a>这个函数已经包含了计算softmax，还有求cross-entropy两件事了。</li><li><strong>tf.reduce_mean:</strong> computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean" target="_blank" rel="noopener">here.</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line of code)</span></span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3,labels=Y))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="5-Model"><a href="#5-Model" class="headerlink" title="5. Model"></a>5. Model</h2><p>把前面的函数都结合起来，创建一个完整的模型。</p><p>其中<code>random_mini_batches()</code>已经给我们了，优化器使用了</p><p><code>optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, learning_rate = <span class="number">0.009</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          num_epochs = <span class="number">100</span>, minibatch_size = <span class="number">64</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer ConvNet in Tensorflow:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_train -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    X_test -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_test -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs of the optimization loop</span></span><br><span class="line"><span class="string">    minibatch_size -- size of a minibatch</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 100 epochs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    train_accuracy -- real number, accuracy on the train set (X_train)</span></span><br><span class="line"><span class="string">    test_accuracy -- real number, testing accuracy on the test set (X_test)</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    ops.reset_default_graph()                         <span class="comment"># to be able to rerun the model without overwriting tf variables</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)                             <span class="comment"># to keep results consistent (tensorflow seed)</span></span><br><span class="line">    seed = <span class="number">3</span>                                          <span class="comment"># to keep results consistent (numpy seed)</span></span><br><span class="line">    (m, n_H0, n_W0, n_C0) = X_train.shape             </span><br><span class="line">    n_y = Y_train.shape[<span class="number">1</span>]                            </span><br><span class="line">    costs = []                                        <span class="comment"># To keep track of the cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create Placeholders of the correct shape</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    X, Y = create_placeholders(n_H0, n_W0,n_C0,n_y)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Forward propagation: Build the forward propagation in the tensorflow graph</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    Z3 = forward_propagation(X,parameters)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Cost function: Add cost function to tensorflow graph</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize all the variables globally</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">     </span><br><span class="line">    <span class="comment"># Start the session to compute the tensorflow graph</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Run the initialization</span></span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Do the training loop</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line"></span><br><span class="line">            minibatch_cost = <span class="number">0.</span></span><br><span class="line">            num_minibatches = int(m / minibatch_size) <span class="comment"># number of minibatches of size minibatch_size in the train set</span></span><br><span class="line">            seed = seed + <span class="number">1</span></span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Select a minibatch</span></span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                <span class="comment"># IMPORTANT: The line that runs the graph on a minibatch.</span></span><br><span class="line">                <span class="comment"># Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).</span></span><br><span class="line">                <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">                _ , temp_cost = sess.run([optimizer,cost],feed_dict=&#123;X:minibatch_X,Y:minibatch_Y&#125;)</span><br><span class="line">                <span class="comment">### END CODE HERE ###</span></span><br><span class="line">                </span><br><span class="line">                minibatch_cost += temp_cost / num_minibatches</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">            <span class="comment"># Print the cost every epoch</span></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, minibatch_cost))</span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                costs.append(minibatch_cost)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># plot the cost</span></span><br><span class="line">        plt.plot(np.squeeze(costs))</span><br><span class="line">        plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">        plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the correct predictions</span></span><br><span class="line">        predict_op = tf.argmax(Z3, <span class="number">1</span>)</span><br><span class="line">        correct_prediction = tf.equal(predict_op, tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate accuracy on the test set</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">        print(accuracy)</span><br><span class="line">        train_accuracy = accuracy.eval(&#123;X: X_train, Y: Y_train&#125;)</span><br><span class="line">        test_accuracy = accuracy.eval(&#123;X: X_test, Y: Y_test&#125;)</span><br><span class="line">        print(<span class="string">"Train Accuracy:"</span>, train_accuracy)</span><br><span class="line">        print(<span class="string">"Test Accuracy:"</span>, test_accuracy)</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> train_accuracy, test_accuracy, parameters</span><br></pre></td></tr></table></figure><p>得到效果如图：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrot3vuucj20b007qdfx.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本周的作业分为了两部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;卷积神经网络的模型搭建&lt;/li&gt;
&lt;li&gt;用TensorFlow来训练卷积神经网络&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(4-1)-- 卷积神经网络（Foundations of CNN）</title>
    <link href="http://fangzh.top/2018/dl-ai-4-1/"/>
    <id>http://fangzh.top/2018/dl-ai-4-1/</id>
    <published>2018-09-30T02:20:54.000Z</published>
    <updated>2018-09-30T07:49:24.135Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt=""></p><p>第四门课开始就学习深度学习关于计算机视觉的重要应用—卷积神经网络。</p><p>第一周主要是对卷积神经网络的基本构造和原理做了介绍。</p><a id="more"></a><h1 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h1><p>计算机视觉是深度学习的一个非常重要的应用。比如图像分类，目标检测，图片风格迁移等。</p><p>用传统的深度学习算法，假设你有一张$64×64$的猫片，又有RGB三通道，那么这个时候是$64×64×3=12288$，input layer的维度就是12288，这样其实也还可以，因为图片很小。那么如果你有$1000×1000$的照片呢，你的向量就会有300万！假设有1000个隐藏神经元，那么就是第一层的参数矩阵$W$有30亿个参数！算到地老天荒。所以用传统的深度学习算法是不现实的。</p><h1 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h1><p>如图，这些边缘检测中，用水平检测和垂直检测会得到不同的结果。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1v3yw2j20hg0813z5.jpg" alt=""></p><p>垂直检测如下图，用一个$3×3$的过滤器（filter），也叫卷积核，在原图片$6×6$的对应地方按元素相乘，得到$4×4$的图片。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1whcopj20x40iln2i.jpg" alt=""></p><p>可以看到，用垂直边缘的filter可以将原图片中间的边缘区分出来，也就是得到了最右图中最亮的部分即为检测到的边缘。</p><p>当然，如果左图的亮暗分界线反过来，则输出图片中最暗的部分表示边缘。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1v5z60j216s0o2q4z.jpg" alt=""></p><p>也自然有水平的边缘分类器。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1v2vcqj20t509ajrh.jpg" alt=""></p><p>还有更复杂的，但是我们不需要进行人工的决定这些filter是什么，因为我们可以通过训练，让机器自己学到这些参数。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1v72odj21450mh0v0.jpg" alt=""></p><h1 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h1><p>padding是填充的意思。</p><ul><li><p>我们可以从之前的例子看到，每经过一次卷积运算，图片的像素都会变小，从$6×6 —&gt; 4×4$，这样子图片就会越来越小，后面就毛都不剩了。</p></li><li><p>还有一点就是，从卷积的运算方法来看，边缘和角落的位置卷积的次数少，会丢失有用信息。</p></li></ul><p>所以就有padding的想法了，也就是在图片四周填补上像素。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1v7wyoj20c70avju6.jpg" alt="填充后从$6\times6 -&gt;8\times8$，经过$3\times3$卷积后，还是$6\times6$"></p><p>计算方法如下，</p><p>原数据是$n \times n$，filter为$f \times f$,padding为$p \times p$，</p><p>那么得到的矩阵大小是$(n + 2p -f +1)\times(n + 2p -f +1)$</p><p>padding有两种：</p><ul><li>valid：也就是不填充</li><li>same：输入与输出大小相同的图片, $p=(f - 1) / 2$，一般padding为奇数，因为filter是奇数</li></ul><h1 id="stride（步长）"><a href="#stride（步长）" class="headerlink" title="stride（步长）"></a>stride（步长）</h1><p>卷积的步长也就是每一次运算后平移的距离，之前使用都是stride=1。</p><p>假设stride=2，就会得到：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1wj691j20x60if78d.jpg" alt=""></p><p>得到的矩阵大小是</p><p>$$\lfloor \frac{n+2p-f}{s}+1\rfloor \times \lfloor \frac{n+2p-f}{s}+1\rfloor$$</p><p>向下取整: 59/60 = 0</p><h1 id="立体卷积"><a href="#立体卷积" class="headerlink" title="立体卷积"></a>立体卷积</h1><p>之前都是单通道的图片进行卷积，如果有RGB三种颜色的话，就要使用立体卷积了。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1v6zxyj211k0lt413.jpg" alt=""></p><p>这个时候的卷积核就变成了$3 \times 3 \times 3$的三维卷积核，一共27个参数，每次对应着原图片上的RGB一共27个像素运算，然后求和得到输出图片的一个像素。因为只有一个卷积核，这个时候输出的还是$4 \times 4 \times 1$的图片。</p><p><strong>多个卷积核</strong></p><p>因为不同的卷积核可以提取不同的图片特征，所以可以有很多个卷积核，同时提取图片的特征，如分别提取图片的水平和垂直边缘特征。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1vbgemj213t0m6gnu.jpg" alt=""></p><p>因为有了两个卷积核，这时候输出的图片就是有两通道的图片$4\times 4 \times 2$。</p><p>这里要搞清两个概念，卷积核的通道数和个数：</p><ul><li>通道数channel：即卷积核要作用在原图片上，原图片的通道处$n_c$，卷积核的通道数必须和原图片通道数相同</li><li>个数：即要使用多少个这样的卷积核，使用$n_{c}^{\prime}$表示，卷积核的个数也就是输出图片的通道数，如有两个卷积核，那么生成了$4\times 4 \times 2$的图片，2  就是卷积核的个数</li><li>即 $n \times n \times n_c$ ，乘上的$n_{c}^{\prime}$个卷积核 $ f \times f \times n_c$，得到$(n -f +1)\times (n - f +1 ) \times n_{c}^{\prime}$的新图片</li></ul><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p><strong>单层卷积网络</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1x8wxmj21ba0qd4nz.jpg" alt=""></p><p>如图是单层卷积的基本过程，先经过两个卷积核，然后再加上bias进行relu激活函数。</p><p>那么假设某层卷积层有10个$3 \times 3 \times 3$的卷积核，那么一共有$(3\times3\times3+1) \times10=280$个参数，加1是加上了bias</p><p>在这里总结了各个参数的表示方法：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1whr44j20m50bx43e.jpg" alt=""></p><p><strong>简单神经网络</strong></p><p>一般卷积神经网络层的类型有：</p><ul><li>convolution卷积层</li><li>pool池化层</li><li>fully connected全连接层</li></ul><h1 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h1><p>pooling 的作用就是用来压缩数据，加速运算，提高提取特征的鲁棒性</p><p><strong>Max pooling</strong></p><p>在范围内取最大值</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1whjhbj20ra0g3wkp.jpg" alt=""></p><p><strong>Average Pooling</strong></p><p>取平均值</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1vp1w0j20pf0cbtaq.jpg" alt=""></p><h1 id="卷积神经网络示例"><a href="#卷积神经网络示例" class="headerlink" title="卷积神经网络示例"></a>卷积神经网络示例</h1><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1w3xmij20z00jl76m.jpg" alt=""></p><p>一般conv后都会进行pooling，所以可以把conv和pooling当做一层。</p><p>如上图就是$conv-pool-conv-pool-fc-fc-fc-softmax$的卷积神经网络结构。</p><p>各个层的参数是这样的：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrm1vv63zj20lv0bcdjg.jpg" alt=""></p><p>可以看到，在卷积层的参数非常少，池化层没有参数，大量的参数在全连接层。</p><h1 id="为何用卷积神经网络？"><a href="#为何用卷积神经网络？" class="headerlink" title="为何用卷积神经网络？"></a>为何用卷积神经网络？</h1><p>这里给出了两点主要原因：</p><ul><li>参数共享：卷积核的参数是原图片中各个像素之间共享的，所以大大减少了参数</li><li>连接的稀疏性：每个输出值，实际上只取决于很少量的输入而已。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;第四门课开始就学习深度学习关于计算机视觉的重要应用—卷积神经网络。&lt;/p&gt;
&lt;p&gt;第一周主要是对卷积神经网络的基本构造和原理做了介绍。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>cs231n作业：assignment1 - features</title>
    <link href="http://fangzh.top/2018/cs231n-1h-5/"/>
    <id>http://fangzh.top/2018/cs231n-1h-5/</id>
    <published>2018-09-27T09:25:31.000Z</published>
    <updated>2018-09-30T10:21:09.494Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvz19onj20yh0k1dms.jpg" alt=""></p><p>GitHub地址：<a href="https://github.com/ZJUFangzh/cs231n" target="_blank" rel="noopener">https://github.com/ZJUFangzh/cs231n</a></p><p>抽取图像的HOG和HSV特征。</p><a id="more"></a><p>对于每张图，我们会计算<strong>梯度方向直方图(HOG)</strong>特征和用<strong>HSV（Hue色调，Saturation饱和度,Value明度）</strong>颜色空间的<strong>色调</strong>特征。把每张图的梯度方向直方图和颜色直方图特征合并形成我们最后的特征向量。</p><p>粗略的讲呢，HOG应该可以捕捉到图像的纹理特征而忽略了颜色信息，颜色直方图会表示图像的颜色特征而忽略了纹理特征(详细见<a href="http://www.jianshu.com/p/395f0582c5f7" target="_blank" rel="noopener">这篇</a>)。所以我们预估把两者结合起来得到的效果应该是比用其中一种得到的效果好。对于后面的bonus，验证一下这个设想是不错的选择。</p><p><code>hog_feature</code>和<code>color_histogram_hsv</code>两个函数都只对一张图做操作并返回这张图片的特征向量。<code>extract_features</code>函数接收一堆图片和一个list的特征函数，然后用每个特征函数在每张图片上过一遍，把结果存到一个矩阵里面，矩阵的每一行都是一张图片的所有特征的合并。</p><p>在features.py中写了两个特征的计算方法，HOG是改写了scikit-image的fog接口，并且首先要转换成灰度图。颜色直方图是实现用matplotlib.colors.rgb_to_hsv的接口把图片从RGB变成HSV，再提取明度(value)，把value投射到不同的bin当中去。关于HOG的原理请谷歌百度。</p><p><strong>如果出错</strong>：</p><p><code>orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[cx/2::cx, cy/2::cy].T</code>这行报错,<em>“TypeError: slice indices must be integers or None or have an <strong>index</strong> method”</em>,可以把代码改成: <code>orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[int(cx/2)::cx, int(cy/2)::cy].T</code></p><p>通过这一步，把原来的数据集都提取出了特征，换成了X_train_feats,X_val_feats,X_test_feats</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cs231n.features <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">num_color_bins = <span class="number">10</span> <span class="comment"># Number of bins in the color histogram</span></span><br><span class="line">feature_fns = [hog_feature, <span class="keyword">lambda</span> img: color_histogram_hsv(img, nbin=num_color_bins)]</span><br><span class="line">X_train_feats = extract_features(X_train, feature_fns, verbose=<span class="keyword">True</span>)</span><br><span class="line">X_val_feats = extract_features(X_val, feature_fns)</span><br><span class="line">X_test_feats = extract_features(X_test, feature_fns)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing: Subtract the mean feature</span></span><br><span class="line">mean_feat = np.mean(X_train_feats, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">X_train_feats -= mean_feat</span><br><span class="line">X_val_feats -= mean_feat</span><br><span class="line">X_test_feats -= mean_feat</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing: Divide by standard deviation. This ensures that each feature</span></span><br><span class="line"><span class="comment"># has roughly the same scale.</span></span><br><span class="line">std_feat = np.std(X_train_feats, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">X_train_feats /= std_feat</span><br><span class="line">X_val_feats /= std_feat</span><br><span class="line">X_test_feats /= std_feat</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing: Add a bias dimension</span></span><br><span class="line">X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[<span class="number">0</span>], <span class="number">1</span>))])</span><br><span class="line">X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[<span class="number">0</span>], <span class="number">1</span>))])</span><br><span class="line">X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[<span class="number">0</span>], <span class="number">1</span>))])</span><br></pre></td></tr></table></figure><h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><p>跟之前都一样的，把训练集换成 ***_feats就行了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use the validation set to tune the learning rate and regularization strength</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cs231n.classifiers.linear_classifier <span class="keyword">import</span> LinearSVM</span><br><span class="line"></span><br><span class="line">learning_rates = [<span class="number">1e-9</span>, <span class="number">1e-8</span>, <span class="number">1e-7</span>]</span><br><span class="line">regularization_strengths = [<span class="number">5e4</span>, <span class="number">5e5</span>, <span class="number">5e6</span>]</span><br><span class="line"></span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = <span class="number">-1</span></span><br><span class="line">best_svm = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">learning_rates =[<span class="number">5e-9</span>, <span class="number">7.5e-9</span>, <span class="number">1e-8</span>]</span><br><span class="line">regularization_strengths = [(<span class="number">5</span>+i)*<span class="number">1e6</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">-3</span>,<span class="number">4</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Use the validation set to set the learning rate and regularization strength. #</span></span><br><span class="line"><span class="comment"># This should be identical to the validation that you did for the SVM; save    #</span></span><br><span class="line"><span class="comment"># the best trained classifer in best_svm. You might also want to play          #</span></span><br><span class="line"><span class="comment"># with different numbers of bins in the color histogram. If you are careful    #</span></span><br><span class="line"><span class="comment"># you should be able to get accuracy of near 0.44 on the validation set.       #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="keyword">for</span> learning_rate <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> regularization_strength <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">        svm = LinearSVM()</span><br><span class="line">        loss_hist = svm.train(X_train_feats, y_train, learning_rate=learning_rate, reg=regularization_strength,</span><br><span class="line">                      num_iters=<span class="number">1500</span>, verbose=<span class="keyword">False</span>)</span><br><span class="line">        y_train_pred = svm.predict(X_train_feats)</span><br><span class="line">        y_val_pred = svm.predict(X_val_feats)</span><br><span class="line">        y_train_acc = np.mean(y_train_pred==y_train)</span><br><span class="line">        y_val_acc = np.mean(y_val_pred==y_val)</span><br><span class="line">        results[(learning_rate,regularization_strength)] = [y_train_acc, y_val_acc]</span><br><span class="line">        <span class="keyword">if</span> y_val_acc &gt; best_val:</span><br><span class="line">            best_val = y_val_acc</span><br><span class="line">            best_svm = svm</span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                              END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out results.</span></span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> sorted(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    print(<span class="string">'lr %e reg %e train accuracy: %f val accuracy: %f'</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'best validation accuracy achieved during cross-validation: %f'</span> % best_val)</span><br></pre></td></tr></table></figure><h1 id="Neural-Network-on-image-features"><a href="#Neural-Network-on-image-features" class="headerlink" title="Neural Network on image features"></a>Neural Network on image features</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cs231n.classifiers.neural_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line"></span><br><span class="line">input_dim = X_train_feats.shape[<span class="number">1</span>]</span><br><span class="line">hidden_dim = <span class="number">500</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">net = TwoLayerNet(input_dim, hidden_dim, num_classes)</span><br><span class="line">best_net = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Train a two-layer neural network on image features. You may want to    #</span></span><br><span class="line"><span class="comment"># cross-validate various parameters as in previous sections. Store your best   #</span></span><br><span class="line"><span class="comment"># model in the best_net variable.                                              #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line">best_val = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">learning_rates = [<span class="number">1.2e-3</span>, <span class="number">1.5e-3</span>, <span class="number">1.75e-3</span>]</span><br><span class="line">regularization_strengths = [<span class="number">1</span>, <span class="number">1.25</span>, <span class="number">1.5</span> , <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> reg <span class="keyword">in</span> regularization_strengths:<span class="comment">#</span></span><br><span class="line"><span class="comment">#        net = TwoLayerNet(input_dim, hidden_dim, num_classes)</span></span><br><span class="line">        loss_hist = net.train(X_train_feats, y_train, X_val_feats, y_val,</span><br><span class="line">                    num_iters=<span class="number">1000</span>, batch_size=<span class="number">200</span>,</span><br><span class="line">                    learning_rate=lr, learning_rate_decay=<span class="number">0.95</span>,</span><br><span class="line">                    reg=reg, verbose=<span class="keyword">False</span>)</span><br><span class="line">        y_train_pred = net.predict(X_train_feats)</span><br><span class="line">        y_val_pred = net.predict(X_val_feats)</span><br><span class="line">        y_train_acc = np.mean(y_train_pred==y_train)</span><br><span class="line">        y_val_acc = np.mean(y_val_pred==y_val)</span><br><span class="line">        results[(lr,reg)] = [y_train_acc, y_val_acc]</span><br><span class="line">        <span class="keyword">if</span> y_val_acc &gt; best_val:</span><br><span class="line">            best_val = y_val_acc</span><br><span class="line">            best_net = net</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> sorted(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    print(<span class="string">'lr %e reg %e train accuracy: %f val accuracy: %f'</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'best validation accuracy achieved during cross-validation: %f'</span> % best_val)</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                              END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvz19onj20yh0k1dms.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;GitHub地址：&lt;a href=&quot;https://github.com/ZJUFangzh/cs231n&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/ZJUFangzh/cs231n&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;抽取图像的HOG和HSV特征。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="cs231n" scheme="http://fangzh.top/tags/cs231n/"/>
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>cs231n作业：assignment1 - two_layer_net</title>
    <link href="http://fangzh.top/2018/cs231n-1h-4/"/>
    <id>http://fangzh.top/2018/cs231n-1h-4/</id>
    <published>2018-09-27T09:05:59.000Z</published>
    <updated>2018-09-30T10:19:48.026Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvz19onj20yh0k1dms.jpg" alt=""></p><p>github地址：<a href="https://github.com/ZJUFangzh/cs231n" target="_blank" rel="noopener">https://github.com/ZJUFangzh/cs231n</a></p><p>搭建一个两层的神经网络。</p><a id="more"></a><h1 id="Forward-pass"><a href="#Forward-pass" class="headerlink" title="Forward pass"></a>Forward pass</h1><p>先计算前向传播过程，编辑<code>cs231n/classifiers/neural_net.py</code>的<code>TwoLayerNet.loss</code>函数</p><p>这个就和之前的svm和softmax一样了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None, reg=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Compute the loss and gradients for a two layer fully connected neural</span></span><br><span class="line"><span class="string">  network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs:</span></span><br><span class="line"><span class="string">  - X: Input data of shape (N, D). Each X[i] is a training sample.</span></span><br><span class="line"><span class="string">  - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></span><br><span class="line"><span class="string">    an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></span><br><span class="line"><span class="string">    is not passed then we only return scores, and if it is passed then we</span></span><br><span class="line"><span class="string">    instead return the loss and gradients.</span></span><br><span class="line"><span class="string">  - reg: Regularization strength.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">  If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></span><br><span class="line"><span class="string">  the score for class c on input X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  If y is not None, instead return a tuple of:</span></span><br><span class="line"><span class="string">  - loss: Loss (data loss and regularization loss) for this batch of training</span></span><br><span class="line"><span class="string">    samples.</span></span><br><span class="line"><span class="string">  - grads: Dictionary mapping parameter names to gradients of those parameters</span></span><br><span class="line"><span class="string">    with respect to the loss function; has the same keys as self.params.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Unpack variables from the params dictionary</span></span><br><span class="line">  W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</span><br><span class="line">  W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">  N, D = X.shape</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Compute the forward pass</span></span><br><span class="line">  scores = <span class="keyword">None</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># <span class="doctag">TODO:</span> Perform the forward pass, computing the class scores for the input. #</span></span><br><span class="line">  <span class="comment"># Store the result in the scores variable, which should be an array of      #</span></span><br><span class="line">  <span class="comment"># shape (N, C).                                                             #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  Z1 = X.dot(W1) + b1</span><br><span class="line">  A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">  scores = A1.dot(W2) + b2</span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                              END OF YOUR CODE                             #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># If the targets are not given then jump out, we're done</span></span><br><span class="line">  <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Compute the loss</span></span><br><span class="line">  loss = <span class="keyword">None</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># <span class="doctag">TODO:</span> Finish the forward pass, and compute the loss. This should include  #</span></span><br><span class="line">  <span class="comment"># both the data loss and L2 regularization for W1 and W2. Store the result  #</span></span><br><span class="line">  <span class="comment"># in the variable loss, which should be a scalar. Use the Softmax           #</span></span><br><span class="line">  <span class="comment"># classifier loss.                                                          #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  scores -= np.max(scores, axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">  exp_scores = np.exp(scores)</span><br><span class="line">  probs = exp_scores / np.sum(exp_scores,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">  y_label = np.zeros((N,probs.shape[<span class="number">1</span>]))</span><br><span class="line">  y_label[np.arange(N),y] = <span class="number">1</span></span><br><span class="line">  loss = (<span class="number">-1</span>) * np.sum(np.multiply(np.log(probs),y_label)) / N</span><br><span class="line">  loss +=  reg * (np.sum(W1 * W1) + np.sum(W2 * W2))</span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                              END OF YOUR CODE                             #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br></pre></td></tr></table></figure><p>检验一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss, _ = net.loss(X, y, reg=0.05)</span><br><span class="line">correct_loss = 1.30378789133</span><br><span class="line"></span><br><span class="line"># should be very small, we get &lt; 1e-12</span><br><span class="line">print(&apos;Difference between your loss and correct loss:&apos;)</span><br><span class="line">print(np.sum(np.abs(loss - correct_loss)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Difference between your loss and correct loss:</span><br><span class="line">1.7985612998927536e-13</span><br></pre></td></tr></table></figure><h1 id="Backword-pass"><a href="#Backword-pass" class="headerlink" title="Backword pass"></a>Backword pass</h1><p>依旧是这个loss函数里面，根据W1,b1,W2,b2，求出grads，求导的公式课程里没给，不过NG老师给了，<a href="http://fangzh.top/2018/2018091216/">shallow neural networks</a>，但是表示的维度不太一样，需要做稍微的修改:</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8euz8cj21to0z4wwp.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Backward pass: compute gradients</span></span><br><span class="line">grads = &#123;&#125;</span><br><span class="line"><span class="comment">#############################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Compute the backward pass, computing the derivatives of the weights #</span></span><br><span class="line"><span class="comment"># and biases. Store the results in the grads dictionary. For example,       #</span></span><br><span class="line"><span class="comment"># grads['W1'] should store the gradient on W1, and be a matrix of same size #</span></span><br><span class="line"><span class="comment">#############################################################################</span></span><br><span class="line">dZ2 = probs-y_label</span><br><span class="line">dW2 = A1.T.dot(dZ2)</span><br><span class="line">dW2 /= N</span><br><span class="line">dW2 += <span class="number">2</span> * reg*W2</span><br><span class="line">db2 = np.sum(dZ2,axis=<span class="number">0</span>) / N</span><br><span class="line">dZ1 = (dZ2).dot(W2.T) * (A1 &gt; <span class="number">0</span>)</span><br><span class="line">dW1 = X.T.dot(dZ1) / N + <span class="number">2</span> * reg * W1</span><br><span class="line">db1 = np.sum(dZ1,axis=<span class="number">0</span>) / N</span><br><span class="line">grads[<span class="string">'W2'</span>] = dW2</span><br><span class="line">grads[<span class="string">'b2'</span>] = db2</span><br><span class="line">grads[<span class="string">'W1'</span>] = dW1</span><br><span class="line">grads[<span class="string">'b1'</span>] = db1</span><br><span class="line"><span class="comment">#############################################################################</span></span><br><span class="line"><span class="comment">#                              END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">#############################################################################</span></span><br></pre></td></tr></table></figure><p>检验一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W2 max relative error: 3.440708e-09</span><br><span class="line">b2 max relative error: 3.865091e-11</span><br><span class="line">W1 max relative error: 3.561318e-09</span><br><span class="line">b1 max relative error: 1.555471e-09</span><br></pre></td></tr></table></figure><h1 id="train-network"><a href="#train-network" class="headerlink" title="train network"></a>train network</h1><p>补全<code>train()</code>函数，其实是一样的，先创建一个minibatch，然后计算得到loss和grads，更新params：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#########################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create a random minibatch of training data and labels, storing  #</span></span><br><span class="line"><span class="comment"># them in X_batch and y_batch respectively.                             #</span></span><br><span class="line"><span class="comment">#########################################################################</span></span><br><span class="line">batch_inx = np.random.choice(num_train,batch_size)</span><br><span class="line">X_batch = X[batch_inx,:]</span><br><span class="line">y_batch = y[batch_inx]</span><br><span class="line"><span class="comment">#########################################################################</span></span><br><span class="line"><span class="comment">#                             END OF YOUR CODE                          #</span></span><br><span class="line"><span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss and gradients using the current minibatch</span></span><br><span class="line">loss, grads = self.loss(X_batch, y=y_batch, reg=reg)</span><br><span class="line">loss_history.append(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#########################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Use the gradients in the grads dictionary to update the         #</span></span><br><span class="line"><span class="comment"># parameters of the network (stored in the dictionary self.params)      #</span></span><br><span class="line"><span class="comment"># using stochastic gradient descent. You'll need to use the gradients   #</span></span><br><span class="line"><span class="comment"># stored in the grads dictionary defined above.                         #</span></span><br><span class="line"><span class="comment">#########################################################################</span></span><br><span class="line">self.params[<span class="string">'W1'</span>] -=  learning_rate * grads[<span class="string">'W1'</span>]</span><br><span class="line">self.params[<span class="string">'b1'</span>] -=  learning_rate * grads[<span class="string">'b1'</span>]</span><br><span class="line">self.params[<span class="string">'W2'</span>] -=  learning_rate * grads[<span class="string">'W2'</span>]</span><br><span class="line">self.params[<span class="string">'b2'</span>] -=  learning_rate * grads[<span class="string">'b2'</span>]</span><br><span class="line"><span class="comment">#########################################################################</span></span><br><span class="line"><span class="comment">#                             END OF YOUR CODE                          #</span></span><br><span class="line"><span class="comment">#########################################################################</span></span><br></pre></td></tr></table></figure><p>再补全<code>predict()</code>函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###########################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Implement this function; it should be VERY simple!                #</span></span><br><span class="line"><span class="comment">###########################################################################</span></span><br><span class="line">score = self.loss(X)</span><br><span class="line">y_pred = np.argmax(score,axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">###########################################################################</span></span><br><span class="line"><span class="comment">#                              END OF YOUR CODE                           #</span></span><br><span class="line"><span class="comment">###########################################################################</span></span><br></pre></td></tr></table></figure><p>然后可以计算画图了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">net = init_toy_model()</span><br><span class="line">stats = net.train(X, y, X, y,</span><br><span class="line">            learning_rate=<span class="number">1e-1</span>, reg=<span class="number">5e-6</span>,</span><br><span class="line">            num_iters=<span class="number">100</span>, verbose=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Final training loss: '</span>, stats[<span class="string">'loss_history'</span>][<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the loss history</span></span><br><span class="line">plt.plot(stats[<span class="string">'loss_history'</span>])</span><br><span class="line">plt.xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'training loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training Loss history'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvns9izj20at07q3yi.jpg" alt=""></p><h1 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h1><p>接下来就可以载入大的数据集，进行训练了，代码都写好了，</p><p>得到的准确度是:0.287</p><p>画个图：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvnshkcj20at07qdg5.jpg" alt=""></p><h1 id="调超参数"><a href="#调超参数" class="headerlink" title="调超参数"></a>调超参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">best_net = <span class="keyword">None</span> <span class="comment"># store the best model into this </span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = <span class="number">-1</span></span><br><span class="line">learning_rates = [<span class="number">1.2e-3</span>, <span class="number">1.5e-3</span>, <span class="number">1.75e-3</span>]</span><br><span class="line">regularization_strengths = [<span class="number">1</span>, <span class="number">1.25</span>, <span class="number">1.5</span> , <span class="number">2</span>]</span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Tune hyperparameters using the validation set. Store your best trained  #</span></span><br><span class="line"><span class="comment"># model in best_net.                                                            #</span></span><br><span class="line"><span class="comment">#                                                                               #</span></span><br><span class="line"><span class="comment"># To help debug your network, it may help to use visualizations similar to the  #</span></span><br><span class="line"><span class="comment"># ones we used above; these visualizations will have significant qualitative    #</span></span><br><span class="line"><span class="comment"># differences from the ones we saw above for the poorly tuned network.          #</span></span><br><span class="line"><span class="comment">#                                                                               #</span></span><br><span class="line"><span class="comment"># Tweaking hyperparameters by hand can be fun, but you might find it useful to  #</span></span><br><span class="line"><span class="comment"># write code to sweep through possible combinations of hyperparameters          #</span></span><br><span class="line"><span class="comment"># automatically like we did on the previous exercises.                          #</span></span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> reg <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">        net = TwoLayerNet(input_size, hidden_size, num_classes)</span><br><span class="line">        loss_hist = net.train(X_train, y_train, X_val, y_val,</span><br><span class="line">                    num_iters=<span class="number">1000</span>, batch_size=<span class="number">200</span>,</span><br><span class="line">                    learning_rate=lr, learning_rate_decay=<span class="number">0.95</span>,</span><br><span class="line">                    reg=reg, verbose=<span class="keyword">False</span>)</span><br><span class="line">        y_train_pred = net.predict(X_train)</span><br><span class="line">        y_val_pred = net.predict(X_val)</span><br><span class="line">        y_train_acc = np.mean(y_train_pred==y_train)</span><br><span class="line">        y_val_acc = np.mean(y_val_pred==y_val)</span><br><span class="line">        results[(lr,reg)] = [y_train_acc, y_val_acc]</span><br><span class="line">        <span class="keyword">if</span> y_val_acc &gt; best_val:</span><br><span class="line">            best_val = y_val_acc</span><br><span class="line">            best_net = net</span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> sorted(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    print(<span class="string">'lr %e reg %e train accuracy: %f val accuracy: %f'</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'best validation accuracy achieved during cross-validation: %f'</span> % best_val)</span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line"><span class="comment">#                               END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">#################################################################################</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvz19onj20yh0k1dms.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;github地址：&lt;a href=&quot;https://github.com/ZJUFangzh/cs231n&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/ZJUFangzh/cs231n&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;搭建一个两层的神经网络。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="cs231n" scheme="http://fangzh.top/tags/cs231n/"/>
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>cs231n作业：assignment1 - softmax</title>
    <link href="http://fangzh.top/2018/cs231n-1h-3/"/>
    <id>http://fangzh.top/2018/cs231n-1h-3/</id>
    <published>2018-09-27T08:02:57.000Z</published>
    <updated>2018-09-30T10:21:01.555Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvz19onj20yh0k1dms.jpg" alt=""></p><p>GitHub地址：<a href="https://github.com/ZJUFangzh/cs231n" target="_blank" rel="noopener">https://github.com/ZJUFangzh/cs231n</a></p><p>softmax是最常用的分类器之一。</p><a id="more"></a><p>softmax和svm都是常用的分类器，而softmax更为常用。</p><p>具体可以参考我这篇的最后，ng老师有讲，<a href="http://fangzh.top/2018/2018091720/">softmax</a></p><p>前面数据集的都跟SVM的一样。</p><p>直接进入loss和grads推导环节。</p><p>$$L_i = -log(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}})$$</p><p>可以看到，计算的公式也就是cross-entropy，即</p><p>$$H(p,q) = - \sum_i y_i log(y_{i}^{hat})$$</p><p>但是，这样有一个缺点，就是指数$e^{f_{y_i}}$可能会特别大，这样可能导致内存不足，计算不稳定等问题。那么可以在分子分母同乘一个常数C，一般C取为$logC = -max f_j$</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvnrx2tj20at02s3yf.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure><p>精确地说，SVM分类器使用的是<em>折叶损失（hinge loss）</em>，有时候又被称为<em>最大边界损失（max-margin loss）</em>。Softmax分类器使用的是<em>交叉熵损失（corss-entropy loss）</em>。Softmax分类器的命名是从<em>softmax函数</em>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p><p>求导过程参考：<a href="https://zhuanlan.zhihu.com/p/37416115" target="_blank" rel="noopener">cs231n softmax求导</a></p><p>最终得到的公式是：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvnsbaoj20al07kaae.jpg" alt=""></p><p><strong>softmax代码实现</strong></p><p>编辑<code>cs231n/classifiers/softmax.py</code>,先写一下<code>softmax_loss_naive</code>函数，依旧是循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Softmax loss function, naive implementation (with loops)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">  of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs:</span></span><br><span class="line"><span class="string">  - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">  - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">    that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">  - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns a tuple of:</span></span><br><span class="line"><span class="string">  - loss as single float</span></span><br><span class="line"><span class="string">  - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">  loss = <span class="number">0.0</span></span><br><span class="line">  dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># <span class="doctag">TODO:</span> Compute the softmax loss and its gradient using explicit loops.     #</span></span><br><span class="line">  <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">  <span class="comment"># here, it is easy to run into numeric instability. Don't forget the        #</span></span><br><span class="line">  <span class="comment"># regularization!                                                           #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  (N, D) = X.shape</span><br><span class="line">  C = W.shape[<span class="number">1</span>]</span><br><span class="line">  <span class="comment">#遍历每个样本</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">    f_i = X[i].dot(W)</span><br><span class="line">    <span class="comment">#进行公式的指数修正</span></span><br><span class="line">    f_i -= np.max(f_i)</span><br><span class="line">    sum_j = np.sum(np.exp(f_i))</span><br><span class="line">    <span class="comment">#得到样本中每个类别的概率</span></span><br><span class="line">    p = <span class="keyword">lambda</span> k : np.exp(f_i[k]) / sum_j</span><br><span class="line">    loss += - np.log(p(y[i]))</span><br><span class="line">    <span class="comment">#根据softmax求导公式</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(C):</span><br><span class="line">      p_k = p(k)</span><br><span class="line">      dW[:, k] += (p_k - (k == y[i])) * X[i]</span><br><span class="line">  </span><br><span class="line">  loss /= N</span><br><span class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line">  dW /= N</span><br><span class="line">  dW += reg*W</span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                          END OF YOUR CODE                                 #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><p>验证一下loss和grad得到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">numerical: -0.621593 analytic: -0.621593, relative error: 7.693773e-09</span><br><span class="line">numerical: -2.576505 analytic: -2.576505, relative error: 4.492083e-09</span><br><span class="line">numerical: -1.527801 analytic: -1.527801, relative error: 4.264914e-08</span><br><span class="line">numerical: 1.101379 analytic: 1.101379, relative error: 9.735173e-09</span><br><span class="line">numerical: 2.375620 analytic: 2.375620, relative error: 3.791861e-08</span><br><span class="line">numerical: 3.166961 analytic: 3.166960, relative error: 8.526285e-09</span><br><span class="line">numerical: -1.440997 analytic: -1.440998, relative error: 4.728898e-08</span><br><span class="line">numerical: 0.563304 analytic: 0.563304, relative error: 2.409996e-08</span><br><span class="line">numerical: -2.057292 analytic: -2.057292, relative error: 1.820335e-08</span><br><span class="line">numerical: -0.450338 analytic: -0.450338, relative error: 8.075985e-08</span><br><span class="line">numerical: -0.233090 analytic: -0.233090, relative error: 4.136546e-08</span><br><span class="line">numerical: 0.251391 analytic: 0.251391, relative error: 4.552523e-08</span><br><span class="line">numerical: 0.787031 analytic: 0.787031, relative error: 5.036469e-08</span><br><span class="line">numerical: -1.801593 analytic: -1.801594, relative error: 3.159903e-08</span><br><span class="line">numerical: -0.294108 analytic: -0.294109, relative error: 1.792497e-07</span><br><span class="line">numerical: -1.974307 analytic: -1.974307, relative error: 1.160708e-08</span><br><span class="line">numerical: 2.986921 analytic: 2.986921, relative error: 2.788065e-08</span><br><span class="line">numerical: -0.247281 analytic: -0.247281, relative error: 8.957573e-08</span><br><span class="line">numerical: 0.569337 analytic: 0.569337, relative error: 2.384912e-08</span><br><span class="line">numerical: -1.579298 analytic: -1.579298, relative error: 1.728733e-08</span><br></pre></td></tr></table></figure><p><strong>向量化softmax</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">  loss = <span class="number">0.0</span></span><br><span class="line">  dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># <span class="doctag">TODO:</span> Compute the softmax loss and its gradient using no explicit loops.  #</span></span><br><span class="line">  <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">  <span class="comment"># here, it is easy to run into numeric instability. Don't forget the        #</span></span><br><span class="line">  <span class="comment"># regularization!                                                           #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  (N, D) = X.shape</span><br><span class="line">  C = W.shape[<span class="number">1</span>]</span><br><span class="line">  f = X.dot(W)</span><br><span class="line">  <span class="comment">#在列方向进行指数修正</span></span><br><span class="line">  f -= np.max(f,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">  <span class="comment">#求得softmax各个类的概率</span></span><br><span class="line">  p = np.exp(f) / np.sum(np.exp(f),axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">  y_lable = np.zeros((N,C))</span><br><span class="line">  <span class="comment">#y_lable就是(N,C)维的矩阵，每一行中只有对应的那个正确类别 = 1，其他都是0</span></span><br><span class="line">  y_lable[np.arange(N),y] = <span class="number">1</span></span><br><span class="line">  <span class="comment">#cross entropy</span></span><br><span class="line">  loss = <span class="number">-1</span> * np.sum(np.multiply(np.log(p),y_lable)) / N</span><br><span class="line">  loss += <span class="number">0.5</span> * reg * np.sum( W * W)</span><br><span class="line">  <span class="comment">#求导公式，很清晰</span></span><br><span class="line">  dW = X.T.dot(p-y_lable)</span><br><span class="line">  dW /= N</span><br><span class="line">  dW += reg*W</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                          END OF YOUR CODE                                 #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><p>检验一下向量化和非向量化的时间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">naive loss: 2.357905e+00 computed in 0.091724s</span><br><span class="line">vectorized loss: 2.357905e+00 computed in 0.002995s</span><br><span class="line">Loss difference: 0.000000</span><br><span class="line">Gradient difference: 0.000000</span><br></pre></td></tr></table></figure><p>softmax的函数已经编写完成了，接下来调一下学习率和正则化两个超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rates and regularization strengths; if you are careful you should be able to</span></span><br><span class="line"><span class="comment"># get a classification accuracy of over 0.35 on the validation set.</span></span><br><span class="line"><span class="keyword">from</span> cs231n.classifiers <span class="keyword">import</span> Softmax</span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = <span class="number">-1</span></span><br><span class="line">best_softmax = <span class="keyword">None</span></span><br><span class="line">learning_rates = [<span class="number">1e-7</span>, <span class="number">5e-7</span>]</span><br><span class="line">regularization_strengths = [<span class="number">2.5e4</span>, <span class="number">5e4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Use the validation set to set the learning rate and regularization strength. #</span></span><br><span class="line"><span class="comment"># This should be identical to the validation that you did for the SVM; save    #</span></span><br><span class="line"><span class="comment"># the best trained softmax classifer in best_softmax.                          #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> reg <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">        softmax = Softmax()</span><br><span class="line">        loss_hist = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,</span><br><span class="line">                      num_iters=<span class="number">1500</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line">        y_train_pred = softmax.predict(X_train)</span><br><span class="line">        y_val_pred = softmax.predict(X_val)</span><br><span class="line">        y_train_acc = np.mean(y_train_pred==y_train)</span><br><span class="line">        y_val_acc = np.mean(y_val_pred==y_val)</span><br><span class="line">        results[(lr,reg)] = [y_train_acc, y_val_acc]</span><br><span class="line">        <span class="keyword">if</span> y_val_acc &gt; best_val:</span><br><span class="line">            best_val = y_val_acc</span><br><span class="line">            best_softmax = softmax</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                              END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Print out results.</span></span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> sorted(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    print(<span class="string">'lr %e reg %e train accuracy: %f val accuracy: %f'</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'best validation accuracy achieved during cross-validation: %f'</span> % best_val)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.350592 val accuracy: 0.354000</span><br><span class="line">lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.329551 val accuracy: 0.342000</span><br><span class="line">lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.347286 val accuracy: 0.359000</span><br><span class="line">lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.328551 val accuracy: 0.337000</span><br><span class="line">best validation accuracy achieved during cross-validation: 0.359000</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvz19onj20yh0k1dms.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;GitHub地址：&lt;a href=&quot;https://github.com/ZJUFangzh/cs231n&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/ZJUFangzh/cs231n&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;softmax是最常用的分类器之一。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="cs231n" scheme="http://fangzh.top/tags/cs231n/"/>
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
  </entry>
  
</feed>
