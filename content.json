{"meta":{"title":"Fangzh的个人博客 | 人工智能拯救世界","subtitle":"人工智能、人生感悟","description":"人工智能、计算机、机器学习、linux、程序员","author":"Fangzh","url":"http://fangzh.top"},"pages":[{"title":"","date":"2018-09-10T07:05:02.669Z","updated":"2018-09-10T07:05:02.669Z","comments":true,"path":"todo.html","permalink":"http://fangzh.top/todo.html","excerpt":"","text":"将要写的博客 hexo(3) done layout的几种形式 阅读量统计 评论系统 访问量统计 seo设置 图床 done git总结 sql总结 shell命令总结 李宏毅DL NG，DL.AI"},{"title":"小黑板","date":"2018-09-10T07:05:02.669Z","updated":"2018-09-10T07:05:02.669Z","comments":true,"path":"board/index.html","permalink":"http://fangzh.top/board/index.html","excerpt":"","text":"小伙伴们敬请留言吧！"}],"posts":[{"title":"DeepLearning.ai作业:(2-2)-- 优化算法（Optimization algorithms）","slug":"dl-ai-2-2h","date":"2018-09-17T03:06:06.000Z","updated":"2018-09-17T03:45:44.300Z","comments":true,"path":"2018/2018091711/","link":"","permalink":"http://fangzh.top/2018/2018091711/","excerpt":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 本周作业实践了课上的各种优化算法： mini-batch momentum Adam","text":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 本周作业实践了课上的各种优化算法： mini-batch momentum Adam 首先是标准的gradient descent： 123456789101112131415161718192021222324252627def update_parameters_with_gd(parameters, grads, learning_rate): \"\"\" Update parameters using one step of gradient descent Arguments: parameters -- python dictionary containing your parameters to be updated: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients to update each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl learning_rate -- the learning rate, scalar. Returns: parameters -- python dictionary containing your updated parameters \"\"\" L = len(parameters) // 2 # number of layers in the neural networks # Update rule for each parameter for l in range(L): ### START CODE HERE ### (approx. 2 lines) parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads['dW' + str(l+1)] parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads['db' + str(l+1)] ### END CODE HERE ### return parameters mini-batch步骤是： shuffle：将数据随机打乱，使用np.random.permutation(m)函数可以把m个样本的顺序重新映射，变成一个len为m的列表，里面的值就是映射原本的顺序。 再根据size大小进行分区，需要注意的是最后的数据有可能小于size大小的，因为可能无法整除，要单独考虑 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: random_mini_batchesdef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0): \"\"\" Creates a list of random minibatches from (X, Y) Arguments: X -- input data, of shape (input size, number of examples) Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples) mini_batch_size -- size of the mini-batches, integer Returns: mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y) \"\"\" np.random.seed(seed) # To make your \"random\" minibatches the same as ours m = X.shape[1] # number of training examples mini_batches = [] # Step 1: Shuffle (X, Y) permutation = list(np.random.permutation(m)) print(permutation) shuffled_X = X[:, permutation] shuffled_Y = Y[:, permutation].reshape((1,m)) # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case. num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning for k in range(0, num_complete_minibatches): ### START CODE HERE ### (approx. 2 lines) mini_batch_X = shuffled_X[:,k * mini_batch_size:(k+1)* mini_batch_size] mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k+1)* mini_batch_size] ### END CODE HERE ### mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) # Handling the end case (last mini-batch &lt; mini_batch_size) if m % mini_batch_size != 0: ### START CODE HERE ### (approx. 2 lines) mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:] mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:] ### END CODE HERE ### mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) return mini_batches Momentum先初始化为0， 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: initialize_velocitydef initialize_velocity(parameters): \"\"\" Initializes the velocity as a python dictionary with: - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters. Arguments: parameters -- python dictionary containing your parameters. parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl Returns: v -- python dictionary containing the current velocity. v['dW' + str(l)] = velocity of dWl v['db' + str(l)] = velocity of dbl \"\"\" L = len(parameters) // 2 # number of layers in the neural networks v = &#123;&#125; # Initialize velocity for l in range(L): ### START CODE HERE ### (approx. 2 lines) v[\"dW\" + str(l+1)] = np.zeros((parameters['W' + str(l+1) ].shape[0],parameters['W' + str(l+1) ].shape[1])) v[\"db\" + str(l+1)] = np.zeros((parameters['b' + str(l+1) ].shape[0],parameters['b' + str(l+1) ].shape[1])) ### END CODE HERE ### return v 再按公式进行迭代，因为指数加权平均不需要知道前面n个数据，只要一步一步进行迭代，知道当前的数据就行，节省空间。 123456789101112131415161718192021222324252627282930313233343536373839# GRADED FUNCTION: update_parameters_with_momentumdef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate): \"\"\" Update parameters using Momentum Arguments: parameters -- python dictionary containing your parameters: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients for each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl v -- python dictionary containing the current velocity: v['dW' + str(l)] = ... v['db' + str(l)] = ... beta -- the momentum hyperparameter, scalar learning_rate -- the learning rate, scalar Returns: parameters -- python dictionary containing your updated parameters v -- python dictionary containing your updated velocities \"\"\" L = len(parameters) // 2 # number of layers in the neural networks # Momentum update for each parameter for l in range(L): ### START CODE HERE ### (approx. 4 lines) # compute velocities v[\"dW\" + str(l+1)] = beta * v[\"dW\" + str(l+1)] + (1 - beta) * grads[\"dW\" + str(l+1)] v[\"db\" + str(l+1)] = beta * v[\"db\" + str(l+1)] + (1 - beta) * grads[\"db\" + str(l+1)] # update parameters parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)] parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)] ### END CODE HERE ### return parameters, v Adam没什么好说的，先初始化，根据公式来就行了。 1234567891011121314151617181920212223242526272829303132333435def initialize_adam(parameters) : \"\"\" Initializes v and s as two python dictionaries with: - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters. Arguments: parameters -- python dictionary containing your parameters. parameters[\"W\" + str(l)] = Wl parameters[\"b\" + str(l)] = bl Returns: v -- python dictionary that will contain the exponentially weighted average of the gradient. v[\"dW\" + str(l)] = ... v[\"db\" + str(l)] = ... s -- python dictionary that will contain the exponentially weighted average of the squared gradient. s[\"dW\" + str(l)] = ... s[\"db\" + str(l)] = ... \"\"\" L = len(parameters) // 2 # number of layers in the neural networks v = &#123;&#125; s = &#123;&#125; # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\". for l in range(L): ### START CODE HERE ### (approx. 4 lines) v[\"dW\" + str(l+1)] = np.zeros((parameters['W'+str(l+1)].shape[0],parameters['W'+str(l+1)].shape[1])) v[\"db\" + str(l+1)] = np.zeros((parameters['b'+str(l+1)].shape[0],parameters['b'+str(l+1)].shape[1])) s[\"dW\" + str(l+1)] = np.zeros((parameters['W'+str(l+1)].shape[0],parameters['W'+str(l+1)].shape[1])) s[\"db\" + str(l+1)] = np.zeros((parameters['b'+str(l+1)].shape[0],parameters['b'+str(l+1)].shape[1])) ### END CODE HERE ### return v, s 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8): \"\"\" Update parameters using Adam Arguments: parameters -- python dictionary containing your parameters: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients for each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl v -- Adam variable, moving average of the first gradient, python dictionary s -- Adam variable, moving average of the squared gradient, python dictionary learning_rate -- the learning rate, scalar. beta1 -- Exponential decay hyperparameter for the first moment estimates beta2 -- Exponential decay hyperparameter for the second moment estimates epsilon -- hyperparameter preventing division by zero in Adam updates Returns: parameters -- python dictionary containing your updated parameters v -- Adam variable, moving average of the first gradient, python dictionary s -- Adam variable, moving average of the squared gradient, python dictionary \"\"\" L = len(parameters) // 2 # number of layers in the neural networks v_corrected = &#123;&#125; # Initializing first moment estimate, python dictionary s_corrected = &#123;&#125; # Initializing second moment estimate, python dictionary # Perform Adam update on all parameters for l in range(L): # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\". ### START CODE HERE ### (approx. 2 lines) v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1-beta1) * grads['dW' + str(l+1)] v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1-beta1) * grads['db' + str(l+1)] ### END CODE HERE ### # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\". ### START CODE HERE ### (approx. 2 lines) v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1 ** t) v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1 ** t) ### END CODE HERE ### # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\". ### START CODE HERE ### (approx. 2 lines) s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1-beta2) * (grads['dW' + str(l+1)]**2) s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1-beta2) * (grads['db' + str(l+1)]**2) ### END CODE HERE ### # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\". ### START CODE HERE ### (approx. 2 lines) s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t) s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t) ### END CODE HERE ### # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\". ### START CODE HERE ### (approx. 2 lines) parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / (s_corrected[\"dW\" + str(l+1)]**0.5 + epsilon) parameters[\"b\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (s_corrected[\"db\" + str(l+1)]**0.5 + epsilon) ### END CODE HERE ### return parameters, v, s 最后代入模型函数，根据关键字选择需要的优化算法就行了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, num_epochs = 10000, print_cost = True): \"\"\" 3-layer neural network model which can be run in different optimizer modes. Arguments: X -- input data, of shape (2, number of examples) Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples) layers_dims -- python list, containing the size of each layer learning_rate -- the learning rate, scalar. mini_batch_size -- the size of a mini batch beta -- Momentum hyperparameter beta1 -- Exponential decay hyperparameter for the past gradients estimates beta2 -- Exponential decay hyperparameter for the past squared gradients estimates epsilon -- hyperparameter preventing division by zero in Adam updates num_epochs -- number of epochs print_cost -- True to print the cost every 1000 epochs Returns: parameters -- python dictionary containing your updated parameters \"\"\" L = len(layers_dims) # number of layers in the neural networks costs = [] # to keep track of the cost t = 0 # initializing the counter required for Adam update seed = 10 # For grading purposes, so that your \"random\" minibatches are the same as ours # Initialize parameters parameters = initialize_parameters(layers_dims) # Initialize the optimizer if optimizer == \"gd\": pass # no initialization required for gradient descent elif optimizer == \"momentum\": v = initialize_velocity(parameters) elif optimizer == \"adam\": v, s = initialize_adam(parameters) # Optimization loop for i in range(num_epochs): # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch seed = seed + 1 minibatches = random_mini_batches(X, Y, mini_batch_size, seed) for minibatch in minibatches: # Select a minibatch (minibatch_X, minibatch_Y) = minibatch # Forward propagation a3, caches = forward_propagation(minibatch_X, parameters) # Compute cost cost = compute_cost(a3, minibatch_Y) # Backward propagation grads = backward_propagation(minibatch_X, minibatch_Y, caches) # Update parameters if optimizer == \"gd\": parameters = update_parameters_with_gd(parameters, grads, learning_rate) elif optimizer == \"momentum\": parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate) elif optimizer == \"adam\": t = t + 1 # Adam counter parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon) # Print the cost every 1000 epoch if print_cost and i % 1000 == 0: print (\"Cost after epoch %i: %f\" %(i, cost)) if print_cost and i % 100 == 0: costs.append(cost) # plot the cost plt.plot(costs) plt.ylabel('cost') plt.xlabel('epochs (per 100)') plt.title(\"Learning rate = \" + str(learning_rate)) plt.show() return parameters 效果gradient descent gradient descent with momentum Adam mode 效果还是很明显的：","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"},{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"}]},{"title":"DeepLearning.ai笔记:(2-2)-- 优化算法（Optimization algorithms）","slug":"dl-ai-2-2","date":"2018-09-16T13:42:33.000Z","updated":"2018-09-17T03:18:34.061Z","comments":true,"path":"2018/2018091621/","link":"","permalink":"http://fangzh.top/2018/2018091621/","excerpt":"这周学习了优化算法，可以让神经网络运行的更快。","text":"这周学习了优化算法，可以让神经网络运行的更快。 主要有: mini-batch 动量梯度下降(momentum) RMSprop Adam优化算法 学习率衰减 mini-batch(小批量)原本的梯度下降算法，在每一次的迭代中，要把所有的数据都进行计算再取平均，那如果你的数据量特别大的话，每进行一次迭代就会耗费大量的时间。 所以就有了mini-batch，做小批量的计算迭代。也就是把训练集划分成n等分，比如数据量有500万个的时候，以1000为单位，将数据集划分为5000份，$$x = {x^{\\lbrace 1 \\rbrace},x^{\\lbrace 2 \\rbrace},x^{\\lbrace 3 \\rbrace},…..,x^{\\lbrace 5000 \\rbrace}}$$ 用大括弧表示每一份的mini-batch，其中每一份$x^{\\lbrace t \\rbrace}$都是1000个样本。 这个时候引入epoch的概念，1个epoch相当于是遍历了一次数据集，比如用mini-batch，1个epoch就可以进行5000次迭代，而传统的batch把数据集都一起计算，相当于1个epoch只进行了1次迭代。 具体计算步骤是： 先划分好每一个mini-batch for t in range(5000)，循环每次迭代 循环里面和之前的计算过程一样，前向传播，但每次计算量是1000个样本 计算损失函数 反向传播 更新参数 batch和mini-batch的对比如图： 如果mini-batch的样本为m的话，其实就是batch gradient descent，缺点是如果样本量太大的话，每一次迭代的时间会比较长，但是优点是每一次迭代的损失函数都是下降的，比较平稳。 mini-batch样本为1的话，那就是随机梯度下降（Stochastic gradient descent）,也就是每次迭代只选择其中一个样本进行迭代，但是这样会失去了样本向量化带来的计算加速效果，损失函数总体是下降的，但是局部会很抖动，很可能无法达到全局最小点。 所以选择一个合适的size很重要，$1 &lt; size &lt; m$，可以实现快速的计算效果，也能够享受向量化带来的加速。 mini-batch size的选择 因为电脑的内存和使用方式都是二进制的，而且是2的n次方，所以之前选1000也不太合理，可以选1024，但是1024也比较少见，一般是从64到512。也就是$64、128、256、512$ 指数加权平均(Exponentially weighted averages ) 蓝色的点是每一天的气温，可以看到是非常抖动的，那如果可以把它平均一下，比如把10天内的气温平均一下，就可以得到如红色的曲线。 但是如果是单纯的把前面的10天气温一起平均的话，那么这样你就需要把前10天的气温全部储存记录下来，这样子虽然会更准一点，但是很浪费储存空间，所以就有了指数加权平均这样的概念。方法如下： $$V_0 = 0$$ $$V_1 = \\beta * V_0 + (1 - \\beta) \\theta_1$$ $……$ $$V_t = \\beta * V_{t-1} + (1 - \\beta) \\theta_t$$ 其中，$\\theta_t$表示第t天的温度，而$V_t$表示指数加权平均后的第t天温度，$\\beta$这个参数表示$\\frac{1}{1-\\beta}$天的平均，也就是，$\\beta = 0.9$，表示10天内的平均，$\\beta = 0.98$，表示50天内的平均。 理解指数加权平均我们再来看一下公式： $$v_t = \\beta v_{t-1} + (1 - \\beta) \\theta_t$$ 假设$\\beta = 0.9$，那么 $$v_{100} = 0.9v_{99} + 0.1\\theta_{100}$$ $$v_{99} = 0.9v_{98} + 0.1\\theta_{99}$$ $$v_{98} = 0.9v_{97} + 0.1\\theta_{98}$$ 展开一下，得到： $$ v_{100} = 0.1 \\theta_{100} + 0.1 \\times 0.9 \\times \\theta_{99} + 0.1 \\times 0.9^2 \\times \\theta_{98} + ……$$ 看到没有，每一项都会乘以0.9，这样就是指数加权的意思了，那么为什么表示的是10天内的平均值呢？明明是10天以前的数据都有加进去的才对，其实是因为$0.9^{10} \\approx 0.35 \\approx \\frac{1}{e}$，也就是10天以前的权重只占了三分之一左右，已经很小了，所以我们就可以认为这个权重就是10天内的温度平均，其实有详细的数学证明的，这里就不要证明了，反正理解了$(1-\\epsilon)^{\\frac{1}{\\epsilon}} \\approx \\frac{1}{e}$，$\\epsilon$为0.02的时候，就代表了50天内的数据。 因为指数加权平均不需要知道前面n个数据，只要一步一步进行迭代，知道当前的数据就行，所以非常节省空间。 指数加权平均的偏差修正如果你细心一点，你就会发现其实这个公式有问题， $$V_0 = 0$$ $$V_1 = \\beta * V_0 + (1 - \\beta) \\theta_1$$ $……$ $$V_t = \\beta * V_{t-1} + (1 - \\beta) \\theta_t$$ 如果第一天的温度是40摄氏度，那么$V_1 = 0.1 * 40 = 4$，显然是不合理的。因为初始值$V_0 = 0$，也就是前面几天的数据都会普遍偏低。所以特别是在估测初期，需要进行一些修正，这个时候就不要用$v_t$了，而是用$\\frac{v_t}{1-\\beta^t}$来代表第t天的温度平均，你会发现随着t的增加，$\\beta^t$接近于0，所以偏差修正几乎就没有用了，而t比较小的时候，就非常有效果。 不过在大部分机器学习中，一般也不需要修正，因为只是前面的初始时期比较有偏差而已，到后面就基本不会有偏差了，所以也不太用。 动量梯度下降法 (Gradient descent with Momentum )用动量梯度下降法运行速度总是比标准的梯度下降法要来的快。它的基本思想是计算梯度的指数加权平均数，然后用该梯度来更新权重。 效果如图： 使用动量梯度下降法后，在竖直方向上的抖动减少了，而在水平方向上的运动反而加速了。 算法公式： 可以发现，就是根据指数平均计算出了$v_{dW}$，然后更新参数时把$dW$换成了$v_{dw}$，$\\beta$一般的取值是0.9。可以发现，在纵向的波动经过平均以后，变得非常小了，而因为在横向上，每一次的微分分量都是指向低点，所以平均后的值一直朝着低点前进。 物理意义： 个人的理解是大概这个公式也很像动量的公式$m v = m_1 v_1 + m_2 v_2$，也就是把两个物体合并了得到新物体的质量和速度的意思 理解成速度和加速度，把$v_{dW}$看成速度，$dW$看成加速度，这样每次因为有速度的存在，加速度只能影响到速度的大小而不能够立刻改变速度的方向。 RMSprop（root mean square prop）均方根传播。这是另一种梯度下降的优化算法。 顾名思义，先平方再开根号。 其实和动量梯度下降法公式差不多： 在更新参数的分母项加了一项$\\epsilon = 10^{-8}$,来确保算法不会除以0 Adam算法Adam算法其实就是结合了Momentum和RMSprop ，注意这个时候要加上偏差修正： 初始化参数：$v_{dW} = 0$，$S_{dW} =0$，$v_{db} = 0$，$S_{db} =0$ 在第$t$次迭代中， 计算mini-batch的dW,db Momentum: $v_{dW}= \\beta_{1}v_{dW} + ( 1 - \\beta_{1})dW$，$v_{db}= \\beta_{1}v_{db} + ( 1 -\\beta_{1} ){db}$ RMSprop:$S_{dW}=\\beta_{2}S_{dW} + ( 1 - \\beta_{2}){(dW)}^{2}$，$S_{db} =\\beta_{2}S_{db} + \\left( 1 - \\beta_{2} \\right){(db)}^{2}$ $v_{dW}^{\\text{corrected}}= \\frac{v_{dW}}{1 - \\beta_{1}^{t}}$，$v_{db}^{\\text{corrected}} =\\frac{v_{db}}{1 -\\beta_{1}^{t}}$ $S_{dW}^{\\text{corrected}} =\\frac{S_{dW}}{1 - \\beta_{2}^{t}}$，$S_{db}^{\\text{corrected}} =\\frac{S_{db}}{1 - \\beta_{2}^{t}}$ $W:= W - \\frac{a v_{dW}^{\\text{corrected}}}{\\sqrt{S_{dW}^{\\text{corrected}}} +\\varepsilon}$ 超参数有$\\alpha,\\beta_1,\\beta_2,\\epsilon$，一般$\\beta_1 = 0.9,\\beta_2 = 0.999,\\epsilon = 10^{-8}$ 学习率衰减在梯度下降时，如果是固定的学习率$\\alpha$，在到达最小值附近的时候，可能不会精确收敛，会很抖动，因此很难达到最小值，所以可以考虑学习率衰减，在迭代过程中，逐渐减小$\\alpha$，这样一开始比较快，后来慢慢的变慢。 常用的是： $$a= \\frac{1}{1 + decayrate * \\text{epoch_num}} a_{0}$$ $$a =\\frac{k}{\\sqrt{\\text{epoch_num}}}a_{0}$$ $$a =\\frac{k}{\\sqrt{t}}a_{0}$$","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(2-1)-- 深度学习的实践层面（Practical aspects of Deep Learning）","slug":"dl-ai-2-1h","date":"2018-09-15T07:58:33.000Z","updated":"2018-09-17T03:04:28.359Z","comments":true,"path":"2018/2018091515/","link":"","permalink":"http://fangzh.top/2018/2018091515/","excerpt":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！","text":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 本周的作业分了3部分： 初始化参数 正则化（L2、dropout） 梯度检验 part1：Initialization主要说明的不同的初始化对迭代的影响。 首先，模型函数是这样的： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"): \"\"\" Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- input data, of shape (2, number of examples) Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples) learning_rate -- learning rate for gradient descent num_iterations -- number of iterations to run gradient descent print_cost -- if True, print the cost every 1000 iterations initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\") Returns: parameters -- parameters learnt by the model \"\"\" grads = &#123;&#125; costs = [] # to keep track of the loss m = X.shape[1] # number of examples layers_dims = [X.shape[0], 10, 5, 1] # Initialize parameters dictionary. if initialization == \"zeros\": parameters = initialize_parameters_zeros(layers_dims) elif initialization == \"random\": parameters = initialize_parameters_random(layers_dims) elif initialization == \"he\": parameters = initialize_parameters_he(layers_dims) # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. a3, cache = forward_propagation(X, parameters) # Loss cost = compute_loss(a3, Y) # Backward propagation. grads = backward_propagation(X, Y, cache) # Update parameters. parameters = update_parameters(parameters, grads, learning_rate) # Print the loss every 1000 iterations if print_cost and i % 1000 == 0: print(\"Cost after iteration &#123;&#125;: &#123;&#125;\".format(i, cost)) costs.append(cost) # plot the loss plt.plot(costs) plt.ylabel('cost') plt.xlabel('iterations (per hundreds)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() return parameters 1. Zero Initialization 把参数全都置位0，结果是显而易见的，就是没有任何变化。 2. Random initialization 把W参数随机化了，但是乘以10倍系数，所以导致初始化的参数太大，收敛速度很慢 12345678910111213141516171819202122232425def initialize_parameters_random(layers_dims): \"\"\" Arguments: layer_dims -- python array (list) containing the size of each layer. Returns: parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\": W1 -- weight matrix of shape (layers_dims[1], layers_dims[0]) b1 -- bias vector of shape (layers_dims[1], 1) ... WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1]) bL -- bias vector of shape (layers_dims[L], 1) \"\"\" np.random.seed(3) # This seed makes sure your \"random\" numbers will be the as ours parameters = &#123;&#125; L = len(layers_dims) # integer representing the number of layers for l in range(1, L): ### START CODE HERE ### (≈ 2 lines of code) parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 10 parameters['b' + str(l)] = np.zeros((layers_dims[l], 1)) ### END CODE HERE ### return parameters 结果一般般 3. He initialization 把W参数随机化，但是乘上系数 sqrt(2./layers_dims[l-1]) 12345678910111213141516171819202122232425def initialize_parameters_he(layers_dims): \"\"\" Arguments: layer_dims -- python array (list) containing the size of each layer. Returns: parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\": W1 -- weight matrix of shape (layers_dims[1], layers_dims[0]) b1 -- bias vector of shape (layers_dims[1], 1) ... WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1]) bL -- bias vector of shape (layers_dims[L], 1) \"\"\" np.random.seed(3) parameters = &#123;&#125; L = len(layers_dims) - 1 # integer representing the number of layers for l in range(1, L + 1): ### START CODE HERE ### (≈ 2 lines of code) parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1]) * np.sqrt(2./layers_dims[l-1]) parameters['b' + str(l)] = np.zeros((layers_dims[l], 1)) ### END CODE HERE ### return parameters 结果非常理想。 Part 2：Regularization数据集： 模型函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1): \"\"\" Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- input data, of shape (input size, number of examples) Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples) learning_rate -- learning rate of the optimization num_iterations -- number of iterations of the optimization loop print_cost -- If True, print the cost every 10000 iterations lambd -- regularization hyperparameter, scalar keep_prob - probability of keeping a neuron active during drop-out, scalar. Returns: parameters -- parameters learned by the model. They can then be used to predict. \"\"\" grads = &#123;&#125; costs = [] # to keep track of the cost m = X.shape[1] # number of examples layers_dims = [X.shape[0], 20, 3, 1] # Initialize parameters dictionary. parameters = initialize_parameters(layers_dims) # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. if keep_prob == 1: a3, cache = forward_propagation(X, parameters) elif keep_prob &lt; 1: a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob) # Cost function if lambd == 0: cost = compute_cost(a3, Y) else: cost = compute_cost_with_regularization(a3, Y, parameters, lambd) # Backward propagation. assert(lambd==0 or keep_prob==1) # it is possible to use both L2 regularization and dropout, # but this assignment will only explore one at a time if lambd == 0 and keep_prob == 1: grads = backward_propagation(X, Y, cache) elif lambd != 0: grads = backward_propagation_with_regularization(X, Y, cache, lambd) elif keep_prob &lt; 1: grads = backward_propagation_with_dropout(X, Y, cache, keep_prob) # Update parameters. parameters = update_parameters(parameters, grads, learning_rate) # Print the loss every 10000 iterations if print_cost and i % 10000 == 0: print(\"Cost after iteration &#123;&#125;: &#123;&#125;\".format(i, cost)) if print_cost and i % 1000 == 0: costs.append(cost) # plot the cost plt.plot(costs) plt.ylabel('cost') plt.xlabel('iterations (x1,000)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() return parameters 没有使用正则化时，效果： L2 正则计算代价函数 $$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L] (i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} $$ 公式已经给了，只要加上后面那一项就可以了 使用np.sum(np.square(Wl))来计算$\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$ 12345678910111213141516171819202122232425262728# GRADED FUNCTION: compute_cost_with_regularizationdef compute_cost_with_regularization(A3, Y, parameters, lambd): \"\"\" Implement the cost function with L2 regularization. See formula (2) above. Arguments: A3 -- post-activation, output of forward propagation, of shape (output size, number of examples) Y -- \"true\" labels vector, of shape (output size, number of examples) parameters -- python dictionary containing parameters of the model Returns: cost - value of the regularized loss function (formula (2)) \"\"\" m = Y.shape[1] W1 = parameters[\"W1\"] W2 = parameters[\"W2\"] W3 = parameters[\"W3\"] cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost ### START CODE HERE ### (approx. 1 line) L2_regularization_cost = lambd / (m * 2) * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) ### END CODER HERE ### cost = cross_entropy_cost + L2_regularization_cost return cost 计算反向传播函数 在$dW$上加上了正则项$\\frac{\\lambda}{m} W$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: backward_propagation_with_regularizationdef backward_propagation_with_regularization(X, Y, cache, lambd): \"\"\" Implements the backward propagation of our baseline model to which we added an L2 regularization. Arguments: X -- input dataset, of shape (input size, number of examples) Y -- \"true\" labels vector, of shape (output size, number of examples) cache -- cache output from forward_propagation() lambd -- regularization hyperparameter, scalar Returns: gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables \"\"\" m = X.shape[1] (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache dZ3 = A3 - Y ### START CODE HERE ### (approx. 1 line) dW3 = 1./m * np.dot(dZ3, A2.T) + lambd / m * W3 ### END CODE HERE ### db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True) dA2 = np.dot(W3.T, dZ3) dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0)) ### START CODE HERE ### (approx. 1 line) dW2 = 1./m * np.dot(dZ2, A1.T) + lambd / m * W2 ### END CODE HERE ### db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True) dA1 = np.dot(W2.T, dZ2) dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0)) ### START CODE HERE ### (approx. 1 line) dW1 = 1./m * np.dot(dZ1, X.T) + lambd / m * W1 ### END CODE HERE ### db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True) gradients = &#123;\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1&#125; return gradients 加上L2正则项后，效果很明显： dropout在每一次迭代中，都随机删除一定概率的neurons。 1. Forward propagation with dropout 分4步: 每一层的$d^{[l]}$对应每一层的$a^{[l]}$,因为有m个样本，所以就有$D^{[1]} = [d^{1} d^{1} … d^{1}] $of the same dimension as $A^{[1]}$.使用np.random.rand(n,m) 将$D^{[l]}$布尔化， $ &lt; keepprob$ 分为 1和0 Set $A^{[1]}$ to $A^{[1]} * D^{[1]}$. Divide $A^{[1]}$ by keep_prob. 记得用cache把每一层的D都记录下来 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# GRADED FUNCTION: forward_propagation_with_dropoutdef forward_propagation_with_dropout(X, parameters, keep_prob = 0.5): \"\"\" Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID. Arguments: X -- input dataset, of shape (2, number of examples) parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\": W1 -- weight matrix of shape (20, 2) b1 -- bias vector of shape (20, 1) W2 -- weight matrix of shape (3, 20) b2 -- bias vector of shape (3, 1) W3 -- weight matrix of shape (1, 3) b3 -- bias vector of shape (1, 1) keep_prob - probability of keeping a neuron active during drop-out, scalar Returns: A3 -- last activation value, output of the forward propagation, of shape (1,1) cache -- tuple, information stored for computing the backward propagation \"\"\" np.random.seed(1) # retrieve parameters W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] W3 = parameters[\"W3\"] b3 = parameters[\"b3\"] # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID Z1 = np.dot(W1, X) + b1 A1 = relu(Z1) ### START CODE HERE ### (approx. 4 lines) # Steps 1-4 below correspond to the Steps 1-4 described above. D1 = np.random.rand(A1.shape[0], A1.shape[1]) # Step 1: initialize matrix D1 = np.random.rand(..., ...) D1 = D1 &lt; keep_prob # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold) A1 = A1 * D1 # Step 3: shut down some neurons of A1 A1 = A1 / keep_prob # Step 4: scale the value of neurons that haven't been shut down ### END CODE HERE ### Z2 = np.dot(W2, A1) + b2 A2 = relu(Z2) ### START CODE HERE ### (approx. 4 lines) D2 = np.random.rand(A2.shape[0], A2.shape[1]) # Step 1: initialize matrix D2 = np.random.rand(..., ...) D2 = D2 &lt; keep_prob # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold) A2 = A2 * D2 # Step 3: shut down some neurons of A2 A2 = A2 / keep_prob # Step 4: scale the value of neurons that haven't been shut down ### END CODE HERE ### Z3 = np.dot(W3, A2) + b3 A3 = sigmoid(Z3) cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) return A3, cache 2. Backward propagation with dropout reapplying the same mask $D^{[1]}$ to dA1. divide dA1 by keep_prob 反向传播的时候，让之前的删除的neurons依旧归0，然后也要除以keepprob，因为dA = np.dot(W.T, dZ)，并没有重复除以过系数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: backward_propagation_with_dropoutdef backward_propagation_with_dropout(X, Y, cache, keep_prob): \"\"\" Implements the backward propagation of our baseline model to which we added dropout. Arguments: X -- input dataset, of shape (2, number of examples) Y -- \"true\" labels vector, of shape (output size, number of examples) cache -- cache output from forward_propagation_with_dropout() keep_prob - probability of keeping a neuron active during drop-out, scalar Returns: gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables \"\"\" m = X.shape[1] (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache dZ3 = A3 - Y dW3 = 1./m * np.dot(dZ3, A2.T) db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True) dA2 = np.dot(W3.T, dZ3) ### START CODE HERE ### (≈ 2 lines of code) dA2 = dA2 * D2 # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation dA2 = dA2 / keep_prob # Step 2: Scale the value of neurons that haven't been shut down ### END CODE HERE ### dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0)) dW2 = 1./m * np.dot(dZ2, A1.T) db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True) dA1 = np.dot(W2.T, dZ2) ### START CODE HERE ### (≈ 2 lines of code) dA1 = dA1 * D1 # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation dA1 = dA1 / keep_prob # Step 2: Scale the value of neurons that haven't been shut down ### END CODE HERE ### dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0)) dW1 = 1./m * np.dot(dZ1, X.T) db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True) gradients = &#123;\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1&#125; return gradients 最终结果,也还不错： 注意： dropout也是正则化的一种 训练的时候用，测试的时候不要用 在正向传播和反向传播中都要用 Part3:Gradient Checking首先写了一维的checking 12345678910111213141516171819# GRADED FUNCTION: forward_propagationdef forward_propagation(x, theta): \"\"\" Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x) Arguments: x -- a real-valued input theta -- our parameter, a real number as well Returns: J -- the value of function J, computed using the formula J(theta) = theta * x \"\"\" ### START CODE HERE ### (approx. 1 line) J = np.dot(theta,x) ### END CODE HERE ### return J 12345678910111213141516171819# GRADED FUNCTION: backward_propagationdef backward_propagation(x, theta): \"\"\" Computes the derivative of J with respect to theta (see Figure 1). Arguments: x -- a real-valued input theta -- our parameter, a real number as well Returns: dtheta -- the gradient of the cost with respect to theta \"\"\" ### START CODE HERE ### (approx. 1 line) dtheta = x ### END CODE HERE ### return dtheta 根据公式： $$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} $$ 步骤是： $\\theta^{+} = \\theta + \\varepsilon$ $\\theta^{-} = \\theta - \\varepsilon$ $J^{+} = J(\\theta^{+})$ $J^{-} = J(\\theta^{-})$ $gradapprox = \\frac{J^{+} - J^{-}}{2 \\varepsilon}$ 123456789101112131415161718192021222324252627282930313233343536373839def gradient_check(x, theta, epsilon = 1e-7): \"\"\" Implement the backward propagation presented in Figure 1. Arguments: x -- a real-valued input theta -- our parameter, a real number as well epsilon -- tiny shift to the input to compute approximated gradient with formula(1) Returns: difference -- difference (2) between the approximated gradient and the backward propagation gradient \"\"\" # Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit. ### START CODE HERE ### (approx. 5 lines) thetaplus = theta + epsilon # Step 1 thetaminus = theta - epsilon # Step 2 J_plus = forward_propagation(x, thetaplus) # Step 3 J_minus = forward_propagation(x, thetaminus) # Step 4 gradapprox = (J_plus - J_minus) / (2 * epsilon) # Step 5 ### END CODE HERE ### # Check if gradapprox is close enough to the output of backward_propagation() ### START CODE HERE ### (approx. 1 line) grad = backward_propagation(x, theta) ### END CODE HERE ### ### START CODE HERE ### (approx. 1 line) numerator = np.linalg.norm(grad - gradapprox) # Step 1' denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox) # Step 2' difference = numerator / denominator # Step 3' ### END CODE HERE ### if difference &lt; 1e-7: print (\"The gradient is correct!\") else: print (\"The gradient is wrong!\") return difference 在N维的空间中， 12345678910111213141516171819202122232425262728293031323334353637383940414243def forward_propagation_n(X, Y, parameters): \"\"\" Implements the forward propagation (and computes the cost) presented in Figure 3. Arguments: X -- training set for m examples Y -- labels for m examples parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\": W1 -- weight matrix of shape (5, 4) b1 -- bias vector of shape (5, 1) W2 -- weight matrix of shape (3, 5) b2 -- bias vector of shape (3, 1) W3 -- weight matrix of shape (1, 3) b3 -- bias vector of shape (1, 1) Returns: cost -- the cost function (logistic cost for one example) \"\"\" # retrieve parameters m = X.shape[1] W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] W3 = parameters[\"W3\"] b3 = parameters[\"b3\"] # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID Z1 = np.dot(W1, X) + b1 A1 = relu(Z1) Z2 = np.dot(W2, A1) + b2 A2 = relu(Z2) Z3 = np.dot(W3, A2) + b3 A3 = sigmoid(Z3) # Cost logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y) cost = 1./m * np.sum(logprobs) cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) return cost, cache 1234567891011121314151617181920212223242526272829303132333435def backward_propagation_n(X, Y, cache): \"\"\" Implement the backward propagation presented in figure 2. Arguments: X -- input datapoint, of shape (input size, 1) Y -- true \"label\" cache -- cache output from forward_propagation_n() Returns: gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables. \"\"\" m = X.shape[1] (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache dZ3 = A3 - Y dW3 = 1./m * np.dot(dZ3, A2.T) db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True) dA2 = np.dot(W3.T, dZ3) dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0)) dW2 = 1./m * np.dot(dZ2, A1.T) * 2 db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True) dA1 = np.dot(W2.T, dZ2) dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0)) dW1 = 1./m * np.dot(dZ1, X.T) db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True) gradients = &#123;\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1&#125; return gradients 这个时候，给了两个函数，可以在字典和向量结构相互转换，也就是要计算$\\theta^{+}$时，把字典转为向量会比较好计算。 12dictionary_to_vector()vector_to_dictionary() J_plus[i]就是向量中的每一个元素，也就是W,b展开之后的每一项元素 To compute J_plus[i]: Set $\\theta^{+}$ to np.copy(parameters_values) Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$ Calculate $J^{+}_i$ using to forward_propagation_n(x, y, vector_to_dictionary($\\theta^{+}$ )). To compute J_minus[i]: do the same thing with $\\theta^{-}$ Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$ 代码如下，记住 thetaplus是一个(n,1)的向量，循环计算每一个参数的gradapprox，再和原本的grad比较： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# GRADED FUNCTION: gradient_check_ndef gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7): \"\"\" Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n Arguments: parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\": grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. x -- input datapoint, of shape (input size, 1) y -- true \"label\" epsilon -- tiny shift to the input to compute approximated gradient with formula(1) Returns: difference -- difference (2) between the approximated gradient and the backward propagation gradient \"\"\" # Set-up variables parameters_values, _ = dictionary_to_vector(parameters) grad = gradients_to_vector(gradients) num_parameters = parameters_values.shape[0] J_plus = np.zeros((num_parameters, 1)) J_minus = np.zeros((num_parameters, 1)) gradapprox = np.zeros((num_parameters, 1)) # Compute gradapprox for i in range(num_parameters): # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\". # \"_\" is used because the function you have to outputs two parameters but we only care about the first one ### START CODE HERE ### (approx. 3 lines) thetaplus = np.copy(parameters_values) # Step 1 thetaplus[i][0] = thetaplus[i][0] + epsilon # Step 2 J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus)) # Step 3 ### END CODE HERE ### # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\". ### START CODE HERE ### (approx. 3 lines) thetaminus = np.copy(parameters_values) # Step 1 thetaminus[i][0] = thetaminus[i][0] - epsilon # Step 2 J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus)) # Step 3 ### END CODE HERE ### # Compute gradapprox[i] ### START CODE HERE ### (approx. 1 line) gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon) ### END CODE HERE ### # Compare gradapprox to backward propagation gradients by computing difference. ### START CODE HERE ### (approx. 1 line) numerator = np.linalg.norm(grad - gradapprox) # Step 1' denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox) # Step 2' difference = numerator / denominator # Step 3' ### END CODE HERE ### if difference &gt; 2e-7: print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\") else: print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\") return difference 注意： 梯度检验太慢，不要在训练的时候运行，你运行只是为了保证你的算法是正确的。","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"},{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"}]},{"title":"DeepLearning.ai笔记:(2-1)-- 深度学习的实践层面（Practical aspects of Deep Learning）","slug":"dl-ai-2-1","date":"2018-09-15T05:37:15.000Z","updated":"2018-09-17T03:04:28.359Z","comments":true,"path":"2018/20180901513/","link":"","permalink":"http://fangzh.top/2018/20180901513/","excerpt":"第二门课主要讲的是如何改善神经网络，通过超参数的调试、正则化以及优化。 第一周主要是说了一些之前机器学习里面涉及到的数据集的划分，以及初始化，正则化的方法，还有梯度的验证。","text":"第二门课主要讲的是如何改善神经网络，通过超参数的调试、正则化以及优化。 第一周主要是说了一些之前机器学习里面涉及到的数据集的划分，以及初始化，正则化的方法，还有梯度的验证。 训练、验证、测试集的划分这些在之前的机器学习课程中都讲过了，这里简单说一下。 训练集也就是你训练的样本；验证集是你训练之后的参数放到这些数据中做验证；而最后做的测试集则是相当于用来最终的测试。 一般来说，划分比例为60%/20%/20%就可以了，但是当数据越来越大，变成上百万，上千万的时候，那么验证集和测试集就没必要占那么大比重了，因为太过浪费，一般在0.5%-3%左右就可以。 需要注意的是，验证集和测试集的数据要来源相同，同分布，也就是同一类的数据，不能验证集是网上的，测试集是你自己拍的照片，这样误差会很大。 bias and variance（偏差和方差） high bias 表示的是高偏差，一般出现在欠拟合(under fitting)的情况下， high variance表示高方差，一般出现在overfitting情况下。 如何解决呢： high bias 更多的隐藏层 每一层更多的神经元 high variance 增加数据 正则化 从左到右4种情况即是： high variance ; high bias ; high bias and high variance ; low bias and low variance regularization（正则化）high variance可以使用正则化来解决。 我们知道，在logistic regression中的正则化项，是在损失函数后面加上： L2 正则：$\\frac{\\lambda}{2m}||w||^{2}{2} = \\frac{\\lambda}{2m}\\sum{j=1}^{n_{x}}{|w|} = \\frac{\\lambda}{2m} w^T w$ L1正则：$\\frac{\\lambda}{2m}||w||{1} = \\frac{\\lambda}{2m}\\sum{j=1}^{n_{x}}{|w|}$ 一般用L2正则来做。 在neural network中， 可以看到后面的正则式是从第1层累加到了第L层的所有神经网络的权重$||W^{[l]}||_{F}$的平方。 而我们知道这个W是一个$n^{[l]} * n^{[l-1]}$的矩阵，那么 它表示矩阵中所有元素的平方和。也就这一项嵌套了3层的$\\sum$。 那么，如何实现这个范数的梯度下降呢？ 在原本的backprop中,加上的正则项的导数，$dJ / dW$ $$dW^{[l]} = (form backprop) + \\frac{\\lambda}{m}W^{[l]}$$ 代入 $$W^{[l]} = W^{[l]} - \\alpha dW^{[l]}$$ 得到： 可以看到，$(1 - \\frac{\\alpha \\lambda}{m}) &lt; 1$，所以每一次都会让W变小，因此L2范数正则化也成为“权重衰减” 正则化如何防止过拟合？直观理解是在代价函数加入正则项后，如果$\\lambda$非常大，为了满足代价函数最小化，那么$w^{[l]}$这一项必须非常接近于0，所以就等价于很多神经元都没有作用了，从原本的非线性结构变成了近似的线性结构，自然就不会过拟合了。 我们再来直观感受一下， 假设是一个tanh()函数，那么$z = wx + b$，当w非常接近于0时，z也接近于0，也就是在坐标轴上0附近范围内，这个时候斜率接近于线性，那么整个神经网络也非常接近于线性的网络，那么就不会发生过拟合了。 dropout 正则化dropout(随机失活)，也是正则化的一种，顾名思义，是让神经网络中的神经元按照一定的概率随机失活。 实现dropout：inverted dropout（反向随机失活） 实现dropout有好几种，但是最常用的还是这个inverted dropout 假设是一个3层的神经网络，keepprob表示保留节点的概率 12345keepprob = 0.8#d3是矩阵，每个元素有true,false,在python中代表1和0d3 = np.random.rand(a3.shape[0],a3.shape[1]) &lt; keepproba3 = np.multiply(a3,d3)a3 /= keepprob 其中第4式 $a3 /= keepprob$ 假设第三层有50个神经元 a3.shape[0] = 50，一共有 $50 * m$维，m是样本数，这样子就会有平均10个神经元被删除，因为$z^{[4]} = w^{[4]} a^{[3]} + b^{[4]}$，那么这个时候$z^{[4]}$的期望值就少了20%,所以在每个神经元上都除以keepprob的值，刚好弥补的之前的损失。 注意 在test阶段，就不需要再使用dropout了，而是像之前一样，直接乘以各个层的权重，得出预测值就可以。 理解dropout直观上，因为神经元有可能会被随机清除，这样子在训练中，就不会过分依赖某一个神经元或者特征的权重。 当然可以设置不同层有不同的dropout概率。 计算机视觉领域非常喜欢用这个dropout。 但是这个东西的一大缺点就是代价函数J不能再被明确定义，每次都会随机移除一些节点，所以很难进行复查。如果需要调试的话，通常会关闭dropout，设置为1，这样再来debug。 归一化归一化数据可以加速神经网络的训练速度。 一般有两个步骤： 零均值 归一化方差 这样子在gradient的时候就会走的顺畅一点： 参数初始化合理的参数初始化可以有效的加快神经网络的训练速度。 一般呢$z = w_1 x_1 + w_2 x_2 + … + w_n x_n$，一般希望z不要太大也不要太小。所以呢，希望n越大，w越小才好。最合理的就是方差 $w = \\frac{1}{n}$，所以： 1WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(1/n) 这个$n$即$n^{[l-1]}$ 如果是relu函数， 那么 $w = \\frac{2}{n}$比较好，也就是np.sqrt(2/n) 梯度的数值逼近$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} $$ 微积分的常识，用$\\varepsilon$来逼近梯度。 梯度检验用梯度检验可以来检查在反向传播中的算法有没有错误。 这个时候，可以把$W^{[1]},b^{[1]},……W^{[l]},b^{[l]}$变成一个向量，这样可以得到一个代价函数$J(\\theta)$，然后$dW,db$也可以转换成一个向量，用$d\\theta$表示，和$\\theta$有相同的维度。 再对每一个$d\\theta_{approx}[i]$求上面的双边梯度逼近，然后也用导数求得每一个$d\\theta[i]$，然后根据图上的cheak公式。求梯度逼近的时候，设置两边的$\\varepsilon = 10^{-7}$，最终求得的值如果是$10^{-7}$，那么很正常，$10^{-3}$就是错了的，如果是$10^{-5}$，那么就需要斟酌一下了。 注意 不要在训练中用梯度检验，因为很慢 如果发现有问题，那么定位到误差比较大的那一层查看 如果有正则化，记得加入正则项 不要和dropout一起使用，因为dropout本来就不容易计算梯度。","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(1-4)-- 深层神经网络（Deep neural networks）","slug":"dl-ai-1-4h","date":"2018-09-13T09:59:43.000Z","updated":"2018-09-13T11:22:59.836Z","comments":true,"path":"2018/2018091318/","link":"","permalink":"http://fangzh.top/2018/2018091318/","excerpt":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！","text":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 本周的作业分了两个部分，第一部分先构建神经网络的基本函数，第二部分才是构建出模型并预测。 Part1构建的函数有： Initialize the parameters two-layer L-layer forworad propagation Linear part 先构建一个线性的计算函数 linear-&gt;activation 在构建某一个神经元的线性和激活函数 L_model_forward funciton 再融合 L-1次的Relu 和 一次 的 sigmoid最后一层 Compute loss backward propagation Linear part linear-&gt;activation L_model_backward funciton Initialization初始化使用： w : np.random.randn(shape)*0.01 b : np.zeros(shape) 1. two-layer 先写了个两层的初始化函数，上周已经写过了。 1234567891011121314151617181920212223242526272829303132333435def initialize_parameters(n_x, n_h, n_y): \"\"\" Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: parameters -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) \"\"\" np.random.seed(1) ### START CODE HERE ### (≈ 4 lines of code) W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h,1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y,1)) ### END CODE HERE ### assert(W1.shape == (n_h, n_x)) assert(b1.shape == (n_h, 1)) assert(W2.shape == (n_y, n_h)) assert(b2.shape == (n_y, 1)) parameters = &#123;\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2&#125; return parameters 2. L-layer 然后写了个L层的初始化函数，其中，输入的参数是一个列表，如[12,4,3,1]，表示一共4层： 1234567891011121314151617181920212223242526def initialize_parameters_deep(layer_dims): \"\"\" Arguments: layer_dims -- python array (list) containing the dimensions of each layer in our network Returns: parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\": Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1]) bl -- bias vector of shape (layer_dims[l], 1) \"\"\" np.random.seed(3) parameters = &#123;&#125; L = len(layer_dims) # number of layers in the network for l in range(1, L): ### START CODE HERE ### (≈ 2 lines of code) parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01 parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) ### END CODE HERE ### assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1])) assert(parameters['b' + str(l)].shape == (layer_dims[l], 1)) return parameters Forward propagation module1. Linear Forward 利用公式： $$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$ where $A^{[0]} = X$. 这个时候，输入的参数是 A,W,b,输出是计算得到的Z，以及cache=（A， W， b）保存起来 12345678910111213141516171819202122def linear_forward(A, W, b): \"\"\" Implement the linear part of a layer's forward propagation. Arguments: A -- activations from previous layer (or input data): (size of previous layer, number of examples) W -- weights matrix: numpy array of shape (size of current layer, size of previous layer) b -- bias vector, numpy array of shape (size of the current layer, 1) Returns: Z -- the input of the activation function, also called pre-activation parameter cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently \"\"\" ### START CODE HERE ### (≈ 1 line of code) Z = np.dot(W, A) + b ### END CODE HERE ### assert(Z.shape == (W.shape[0], A.shape[1])) cache = (A, W, b) return Z, cache 2. Linear-Activation Forward 在这里就是把刚才得到的Z，通过$A = g(Z)$激活函数，合并成一个 这个时候，notebook已经给了我们现成的sigmoid和relu函数了，只要调用就行，不过在里面好像没有说明源代码，输出都是A和cache=Z，这里贴出来： 12345678910111213141516def sigmoid(Z): \"\"\" Implements the sigmoid activation in numpy Arguments: Z -- numpy array of any shape Returns: A -- output of sigmoid(z), same shape as Z cache -- returns Z as well, useful during backpropagation \"\"\" A = 1/(1+np.exp(-Z)) cache = Z return A, cache 123456789101112131415161718def relu(Z): \"\"\" Implement the RELU function. Arguments: Z -- Output of the linear layer, of any shape Returns: A -- Post-activation parameter, of the same shape as Z cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently \"\"\" A = np.maximum(0,Z) assert(A.shape == Z.shape) cache = Z return A, cache 而后利用之前的linear_forward，可以写出某层神经元的前向函数了，输入是$A^{[l-1]},W,b$，还有一个是说明sigmoid还是relu的字符串activation。 输出是$A^{[l]}$和cache，这里的cache已经包含的4个参数了，分别是$A^{[l-1]},W^{[l]},b^{[l]},Z^{[l]}$ 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: linear_activation_forwarddef linear_activation_forward(A_prev, W, b, activation): \"\"\" Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer Arguments: A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples) W -- weights matrix: numpy array of shape (size of current layer, size of previous layer) b -- bias vector, numpy array of shape (size of the current layer, 1) activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" Returns: A -- the output of the activation function, also called the post-activation value cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\"; stored for computing the backward pass efficiently \"\"\" if activation == \"sigmoid\": # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\". ### START CODE HERE ### (≈ 2 lines of code) Z, linear_cache = linear_forward(A_prev, W, b) A, activation_cache = sigmoid(Z) ### END CODE HERE ### elif activation == \"relu\": # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\". ### START CODE HERE ### (≈ 2 lines of code) Z, linear_cache = linear_forward(A_prev, W, b) A, activation_cache = relu(Z) ### END CODE HERE ### assert (A.shape == (W.shape[0], A_prev.shape[1])) cache = (linear_cache, activation_cache) # print(cache) return A, cache 3. L-Layer Model 这一步就把多层的神经网络从头到尾串起来了。前面有L-1层的Relu，第L层是sigmoid。 输入是X，也就是$A^{[0]}$，和 parameters包含了各个层的W,b 输出是最后一层的$A^{[L]}$，也就是预测结果$Y_hat$，以及每一层的caches : $A^{[l-1]},W^{[l]},b^{[l]},Z^{[l]}$ 1234567891011121314151617181920212223242526272829303132333435def L_model_forward(X, parameters): \"\"\" Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation Arguments: X -- data, numpy array of shape (input size, number of examples) parameters -- output of initialize_parameters_deep() Returns: AL -- last post-activation value caches -- list of caches containing: every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1) \"\"\" caches = [] A = X L = len(parameters) // 2 # number of layers in the neural network # Implement [LINEAR -&gt; RELU]*(L-1). Add \"cache\" to the \"caches\" list. for l in range(1, L): A_prev = A ### START CODE HERE ### (≈ 2 lines of code) A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], 'relu') caches.append(cache) ### END CODE HERE ### # Implement LINEAR -&gt; SIGMOID. Add \"cache\" to the \"caches\" list. ### START CODE HERE ### (≈ 2 lines of code) AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)],'sigmoid') caches.append(cache) ### END CODE HERE ### # print(AL.shape) assert(AL.shape == (1,X.shape[1])) return AL, caches Cost function$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{L}\\right)) $$ 利用np.multiply and np.sum求得交叉熵 123456789101112131415161718192021222324def compute_cost(AL, Y): \"\"\" Implement the cost function defined by equation (7). Arguments: AL -- probability vector corresponding to your label predictions, shape (1, number of examples) Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples) Returns: cost -- cross-entropy cost \"\"\" m = Y.shape[1] # Compute loss from aL and y. ### START CODE HERE ### (≈ 1 lines of code) cost = - np.sum(np.multiply(Y,np.log(AL)) + np.multiply(1-Y,np.log(1-AL))) / m print(cost) ### END CODE HERE ### cost = np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17). assert(cost.shape == ()) return cost Backward propagation module1. Linear backward 首先假设知道 $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$，然后想要求得的是$(dW^{[l]}, db^{[l]} dA^{[l-1]})$. 公式已经给你了：$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$ $$db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l] (i)}$$ $$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$ cache是linear cache: A_prev,W,b 1234567891011121314151617181920212223242526272829def linear_backward(dZ, cache): \"\"\" Implement the linear portion of backward propagation for a single layer (layer l) Arguments: dZ -- Gradient of the cost with respect to the linear output (of current layer l) cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer Returns: dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev dW -- Gradient of the cost with respect to W (current layer l), same shape as W db -- Gradient of the cost with respect to b (current layer l), same shape as b \"\"\" A_prev, W, b = cache m = A_prev.shape[1] ### START CODE HERE ### (≈ 3 lines of code) dW = 1 / m * np.dot(dZ, A_prev.T) db = 1 / m * np.sum(dZ, axis=1,keepdims=True) #print(db.shape) #print(b.shape) dA_prev = np.dot(W.T, dZ) ### END CODE HERE ### assert (dA_prev.shape == A_prev.shape) assert (dW.shape == W.shape) assert (db.shape == b.shape) return dA_prev, dW, db 2. Linear-Activation backward dA通过激活函数的导数可以求得dZ，再由上面的函数，最终： 输入$dA^{[l]} , cache$ 输出$dA^{[l-1]} ,dW,db$ 这个时候它有给了两个现成的函数dZ = sigmoid_backward(dA, activation_cache)、dZ = relu_backward(dA, activation_cache) 源代码如下,输入的都是dA，和 cache=Z，输出是dZ： $$dZ^{[l]} = dA^{[l]} * g’(Z^{[l]})$$ 1234567891011121314151617181920def sigmoid_backward(dA, cache): \"\"\" Implement the backward propagation for a single SIGMOID unit. Arguments: dA -- post-activation gradient, of any shape cache -- 'Z' where we store for computing backward propagation efficiently Returns: dZ -- Gradient of the cost with respect to Z \"\"\" Z = cache s = 1/(1+np.exp(-Z)) dZ = dA * s * (1-s) assert (dZ.shape == Z.shape) return dZ 123456789101112131415161718192021def relu_backward(dA, cache): \"\"\" Implement the backward propagation for a single RELU unit. Arguments: dA -- post-activation gradient, of any shape cache -- 'Z' where we store for computing backward propagation efficiently Returns: dZ -- Gradient of the cost with respect to Z \"\"\" Z = cache dZ = np.array(dA, copy=True) # just converting dz to a correct object. # When z &lt;= 0, you should set dz to 0 as well. dZ[Z &lt;= 0] = 0 assert (dZ.shape == Z.shape) return dZ 然后得到了函数如下,注意这里面的cache已经是4个元素了linear_cache=A_prev,W,b、activation_cache=Z： 12345678910111213141516171819202122232425262728293031# GRADED FUNCTION: linear_activation_backwarddef linear_activation_backward(dA, cache, activation): \"\"\" Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer. Arguments: dA -- post-activation gradient for current layer l cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" Returns: dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev dW -- Gradient of the cost with respect to W (current layer l), same shape as W db -- Gradient of the cost with respect to b (current layer l), same shape as b \"\"\" linear_cache, activation_cache = cache if activation == \"relu\": ### START CODE HERE ### (≈ 2 lines of code) dZ = relu_backward(dA, activation_cache) dA_prev, dW, db = linear_backward(dZ, linear_cache) ### END CODE HERE ### elif activation == \"sigmoid\": ### START CODE HERE ### (≈ 2 lines of code) dZ = sigmoid_backward(dA, activation_cache) dA_prev, dW, db = linear_backward(dZ, linear_cache) ### END CODE HERE ### return dA_prev, dW, db 3. L-Model Backward 可以把前面的函数穿起来，从后面往前面传播了，先算最后一层的sigmoid，然后往前算L-1的循环relu。其中，dAL是损失函数的导数，这个是预先求得知道的，也就是 $$-\\frac{y}{a}-\\frac{1-y}{1-a}$$ numpy表示为： 1dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) 整个backward中，我们的输入只有AL,Y和caches， 输出则是每一层的grads，包括了$dA,dW,db$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# GRADED FUNCTION: L_model_backwarddef L_model_backward(AL, Y, caches): \"\"\" Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group Arguments: AL -- probability vector, output of the forward propagation (L_model_forward()) Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) caches -- list of caches containing: every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2) the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1]) Returns: grads -- A dictionary with the gradients grads[\"dA\" + str(l)] = ... grads[\"dW\" + str(l)] = ... grads[\"db\" + str(l)] = ... \"\"\" grads = &#123;&#125; L = len(caches) # the number of layers m = AL.shape[1] Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL # Initializing the backpropagation ### START CODE HERE ### (1 line of code) dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) ### END CODE HERE ### # Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"] ### START CODE HERE ### (approx. 2 lines) current_cache = caches[L-1] grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid') ### END CODE HERE ### # Loop from l=L-2 to l=0 for l in reversed(range(L-1)): # lth layer: (RELU -&gt; LINEAR) gradients. # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] ### START CODE HERE ### (approx. 5 lines) current_cache = caches[l] dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA'+str(l+1)], current_cache, 'relu') grads[\"dA\" + str(l)] = dA_prev_temp grads[\"dW\" + str(l + 1)] = dW_temp grads[\"db\" + str(l + 1)] = db_temp ### END CODE HERE ### return grads Update Parameters12345678910111213141516171819202122232425# GRADED FUNCTION: update_parametersdef update_parameters(parameters, grads, learning_rate): \"\"\" Update parameters using gradient descent Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients, output of L_model_backward Returns: parameters -- python dictionary containing your updated parameters parameters[\"W\" + str(l)] = ... parameters[\"b\" + str(l)] = ... \"\"\" L = len(parameters) // 2 # number of layers in the neural network # Update rule for each parameter. Use a for loop. ### START CODE HERE ### (≈ 3 lines of code) for l in range(L): parameters[\"W\" + str(l+1)] -= learning_rate * grads['dW'+str(l+1)] parameters[\"b\" + str(l+1)] -= learning_rate * grads['db'+str(l+1)] ### END CODE HERE ### return parameters Part2有了part1中的函数，就很容易在part2中搭建模型和训练了。 依旧是识别猫猫的图片。 开始先用两层的layer做训练，得到了精确度是72%，这里贴代码就好了，L层再详细说说 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899### CONSTANTS DEFINING THE MODEL ####n_x = 12288 # num_px * num_px * 3n_h = 7n_y = 1layers_dims = (n_x, n_h, n_y)# GRADED FUNCTION: two_layer_modeldef two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False): \"\"\" Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- input data, of shape (n_x, number of examples) Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) layers_dims -- dimensions of the layers (n_x, n_h, n_y) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- If set to True, this will print the cost every 100 iterations Returns: parameters -- a dictionary containing W1, W2, b1, and b2 \"\"\" np.random.seed(1) grads = &#123;&#125; costs = [] # to keep track of the cost m = X.shape[1] # number of examples (n_x, n_h, n_y) = layers_dims # Initialize parameters dictionary, by calling one of the functions you'd previously implemented ### START CODE HERE ### (≈ 1 line of code) parameters = initialize_parameters(n_x, n_h, n_y) ### END CODE HERE ### # Get W1, b1, W2 and b2 from the dictionary parameters. W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\". ### START CODE HERE ### (≈ 2 lines of code) A1, cache1 = linear_activation_forward(X, W1, b1, 'relu') A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid') ### END CODE HERE ### # Compute cost ### START CODE HERE ### (≈ 1 line of code) cost = compute_cost(A2, Y) ### END CODE HERE ### # Initializing backward propagation dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2)) # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\". ### START CODE HERE ### (≈ 2 lines of code) dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid') dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu') ### END CODE HERE ### # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2 grads['dW1'] = dW1 grads['db1'] = db1 grads['dW2'] = dW2 grads['db2'] = db2 # Update parameters. ### START CODE HERE ### (approx. 1 line of code) parameters = update_parameters(parameters, grads, learning_rate) ### END CODE HERE ### # Retrieve W1, b1, W2, b2 from parameters W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] # Print the cost every 100 training example if print_cost and i % 100 == 0: print(\"Cost after iteration &#123;&#125;: &#123;&#125;\".format(i, np.squeeze(cost))) if print_cost and i % 100 == 0: costs.append(cost) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() return parameters L-layer Neural Network使用之前的函数： 123456789101112131415def initialize_parameters_deep(layers_dims): ... return parameters def L_model_forward(X, parameters): ... return AL, cachesdef compute_cost(AL, Y): ... return costdef L_model_backward(AL, Y, caches): ... return gradsdef update_parameters(parameters, grads, learning_rate): ... return parameters 这里一共4层： 1layers_dims = [12288, 20, 7, 5, 1] # 4-layer model 思路是： 初始化参数 进入for的n次迭代循环： L_model_forward(X, parameters) 得到 AL,caches 计算cost L_model_backward(AL, Y, caches)计算grads update_parameters(parameters, grads, learning_rate)更新参数 每100层记录一下cost的值 画出cost梯度下降图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# GRADED FUNCTION: L_layer_modeldef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009 \"\"\" Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- data, numpy array of shape (number of examples, num_px * num_px * 3) Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) layers_dims -- list containing the input size and each layer size, of length (number of layers + 1). learning_rate -- learning rate of the gradient descent update rule num_iterations -- number of iterations of the optimization loop print_cost -- if True, it prints the cost every 100 steps Returns: parameters -- parameters learnt by the model. They can then be used to predict. \"\"\" np.random.seed(1) costs = [] # keep track of cost # Parameters initialization. (≈ 1 line of code) ### START CODE HERE ### parameters = initialize_parameters_deep(layers_dims) ### END CODE HERE ### # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID. ### START CODE HERE ### (≈ 1 line of code) AL, caches = L_model_forward(X, parameters) ### END CODE HERE ### # Compute cost. ### START CODE HERE ### (≈ 1 line of code) cost = compute_cost(AL, Y) ### END CODE HERE ### # Backward propagation. ### START CODE HERE ### (≈ 1 line of code) grads = L_model_backward(AL, Y, caches) ### END CODE HERE ### # Update parameters. ### START CODE HERE ### (≈ 1 line of code) parameters = update_parameters(parameters, grads, learning_rate) ### END CODE HERE ### # Print the cost every 100 training example if print_cost and i % 100 == 0: print (\"Cost after iteration %i: %f\" %(i, cost)) if print_cost and i % 100 == 0: costs.append(cost) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() return parameters 2500的迭代次数，精度达到了80%！ 小结 过程其实是很清晰的，就是先初始化参数；再开始循环，循环中先计算前向传播，得到最后一层的AL，以及每一层的cache，其中cache包括了 A_prev，W，b，Z；然后计算一下每一次迭代的cost；再进行反向传播，得到每一层的梯度dA,dW,db;记得每100次迭代记录一下cost值，这样就可以画出cost是如何下降的了。 part1构建的那些函数，一步步来是比较简单的，但是如果自己要一下子想出来的话，也很难想得到。所以思路要清晰，一步一步来，才能构建好函数！","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"},{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"}]},{"title":"DeepLearning.ai笔记:(1-4)-- 深层神经网络（Deep neural networks）","slug":"dl-ai-1-4","date":"2018-09-13T08:54:18.000Z","updated":"2018-09-17T03:04:28.353Z","comments":true,"path":"2018/2018091316/","link":"","permalink":"http://fangzh.top/2018/2018091316/","excerpt":"这一周主要讲了深层的神经网络搭建。","text":"这一周主要讲了深层的神经网络搭建。 深层神经网络的符号表示 在深层的神经网络中， $L$表示神经网络的层数 $L = 4$ $n^{[l]}$表示第$l$层的神经网络个数 $W^{[l]}: (n^{[l]},n^{l-1})$ $dW^{[l]}: (n^{[l]},n^{l-1})$ $b^{[l]}: (n^{[l]},1)$ $db^{[l]}: (n^{[l]},1)$ $z^{[l]}:(n^{[l]},1)$ $a^{[l]}:(n^{[l]},1)$ 前向传播和反向传播前向传播 input $a^{[l-1]}$ output $a^{[l]},cache (z^{[l]})$ ，其中cache也顺便把 $W^{[l]}, b^{[l]}$也保存下来了 所以，前向传播的公式可以写作： $$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$$ $$A^{[l]} = g^{[l]}(Z^{[l]})$$ 维度 假设有m个样本，那么$Z^{[l]}$ 维度就是 $(n^{[l]}, m)$ ，$A^{[l]}$的维度和$Z^{[l]}$一样。 那么 $ W^{[l]} A^{[l-1]}$维度就是 $(n^{[l]},n^{l-1}) * (n^{[l-1]},m)$ 也就是 $(n^{[l]}, m)$，这个时候，还需要加上$b^{[l]}$，而$b^{[l]}$本身的维度是$(n^{[l]},1)$，借助python的广播，扩充到了m个维度。 反向传播 input $da^{[l]}$ output $da^{[l-1]} , dW^{[l]} , db^{[l]}$ 公式： 向量化： 正向传播和反向传播如图： 具体过程为，第一层和第二层用Relu函数，第三层输出用sigmoid，这个时候的输出值是$a^{[3]}$ 而首先进行反向传播的时候先求得$da^{[3]} = - \\frac{y}{a} - \\frac{1-y}{1-a}$，然后再包括之前存在cache里面的$z^{[3]}$,反向传播可以得到$dw^{[3]}, db^{[3]},da^{[2]}$，然后继续反向，直到得到了$dw^{[1]},db^{[1]}$后，更新一下w，b的参数，然后继续做前向传播、反向传播，不断循环。 Why Deep？ 如图直观上感觉，比如第一层，它会先识别出一些边缘信息；第二层则将这些边缘进行整合，得到一些五官信息，如眼睛、嘴巴等；到了第三层，就可以将这些信息整合起来，输出一张人脸了。 如果网络层数不够深的话，可以组合的情况就很少，或者需要类似门电路那样，用单层很多个特征才能得到和深层神经网络类似的效果。 搭建深层神经网络块 和之前说的一样，一个网络块中包含了前向传播和反向传播。 前向输入$a^{[l-1]}$，经过神经网络的计算，$g^{[l]}(w^{[l]}a^{[l-1]} + b^{[l]})$得到$a^{[l]}$ 反向传播，输入$da^{[l]}$，再有之前在cache的$z^{[l]}$,即可得到$dw^{[l]},db^{[l]}$还有上一层的$da^{[l-1]}$ 参数与超参数超参数就是你自己调的，玄学参数： learning_rate iterations L = len(hidden layer) $n^{[l]}$ activation function mini batch size（最小的计算批） regularization（正则） momentum（动量）","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(1-3)-- 浅层神经网络（Shallow neural networks）","slug":"dl-ai-1-3h","date":"2018-09-12T07:49:22.000Z","updated":"2018-09-12T08:41:19.470Z","comments":true,"path":"2018/2018091216/","link":"","permalink":"http://fangzh.top/2018/2018091216/","excerpt":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！","text":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 数据集数据集是一个类似花的数据集。 而如果用传统的logistic regression，做出来的就是一个二分类问题，简单粗暴的划出了一条线， 可以看见，准确率只有47%。 所以就需要构建神经网络模型了。 神经网络模型Reminder: The general methodology to build a Neural Network is to: 12345671. Define the neural network structure ( # of input units, # of hidden units, etc). 2. Initialize the model&apos;s parameters3. Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent) 已经给出思路了： 定义神经网络的结构 初始化模型参数 循环： 计算正向传播 计算损失函数 计算反向传播来得到grad 更新参数 1. 定义神经网络结构12345678910111213141516171819# GRADED FUNCTION: layer_sizesdef layer_sizes(X, Y): \"\"\" Arguments: X -- input dataset of shape (input size, number of examples) Y -- labels of shape (output size, number of examples) Returns: n_x -- the size of the input layer n_h -- the size of the hidden layer n_y -- the size of the output layer \"\"\" ### START CODE HERE ### (≈ 3 lines of code) n_x = X.shape[0] # size of input layer n_h = 4 n_y = Y.shape[0] # size of output layer ### END CODE HERE ### return (n_x, n_h, n_y) 2. 初始化参数来初始化w和b的参数 w: np.random.rand(a,b) * 0.01 b: np.zeros((a,b)) 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: initialize_parametersdef initialize_parameters(n_x, n_h, n_y): \"\"\" Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) \"\"\" np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random. ### START CODE HERE ### (≈ 4 lines of code) W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y, 1)) ### END CODE HERE ### assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = &#123;\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2&#125; return parameters 3. loop在这里可以使用sigmoid()来做输出层的函数，np.tanh()来做hidden layer的激活函数。 3.1 forward propagation在这个函数中，输入的是X，和parameters，然后就可以根据 $$z^{[1] (i)} = W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$ 得到每一层的Z和A了。 123456789101112131415161718192021222324252627282930313233343536# GRADED FUNCTION: forward_propagationdef forward_propagation(X, parameters): \"\"\" Argument: X -- input data of size (n_x, m) parameters -- python dictionary containing your parameters (output of initialization function) Returns: A2 -- The sigmoid output of the second activation cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\" \"\"\" # Retrieve each parameter from the dictionary \"parameters\" ### START CODE HERE ### (≈ 4 lines of code) W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] ### END CODE HERE ### # Implement Forward Propagation to calculate A2 (probabilities) ### START CODE HERE ### (≈ 4 lines of code) Z1 = np.dot(W1,X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2,A1) + b2 A2 = sigmoid(Z2) ### END CODE HERE ### assert(A2.shape == (1, X.shape[1])) cache = &#123;\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2&#125; return A2, cache 3.2 cost接下来，在得到A2的值后，就可以根据公式来计算损失函数了。 $$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small$$ 在这里需要注意的是交叉熵的计算，交叉熵使用np.multiply()来计算，然后用np.sum()，求和。 而单单计算logprobs = np.multiply(np.log(A2),Y)是不够的，因为这个只得到了公式的前一半的部分，Y=0的部分在元素相乘中就相当于没有了，所以还要再后面加一项np.multiply(np.log(1-A2),1-Y) 123456789101112131415161718192021222324252627# GRADED FUNCTION: compute_costdef compute_cost(A2, Y, parameters): \"\"\" Computes the cross-entropy cost given in equation (13) Arguments: A2 -- The sigmoid output of the second activation, of shape (1, number of examples) Y -- \"true\" labels vector of shape (1, number of examples) parameters -- python dictionary containing your parameters W1, b1, W2 and b2 Returns: cost -- cross-entropy cost given equation (13) \"\"\" m = Y.shape[1] # number of example # Compute the cross-entropy cost ### START CODE HERE ### (≈ 2 lines of code) logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),1-Y) cost = -1 / m * np.sum(logprobs) ### END CODE HERE ### cost = np.squeeze(cost) # makes sure cost is the dimension we expect. # E.g., turns [[17]] into 17 assert(isinstance(cost, float)) return cost 3.3 backworad propagationNG说神经网络中最难理解的是这个，但是现在公式已经帮我们推倒好了。 其中， $g^{[1]’}(Z^{[1]})$ using (1 - np.power(A1, 2)) 可以看到，公式中需要的变量有X,Y,A,W,然后输出一个字典结构的grads 12345678910111213141516171819202122232425262728293031323334353637383940414243def backward_propagation(parameters, cache, X, Y): \"\"\" Implement the backward propagation using the instructions above. Arguments: parameters -- python dictionary containing our parameters cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\". X -- input data of shape (2, number of examples) Y -- \"true\" labels vector of shape (1, number of examples) Returns: grads -- python dictionary containing your gradients with respect to different parameters \"\"\" m = X.shape[1] # First, retrieve W1 and W2 from the dictionary \"parameters\". ### START CODE HERE ### (≈ 2 lines of code) W1 = parameters['W1'] W2 = parameters['W2'] ### END CODE HERE ### # Retrieve also A1 and A2 from dictionary \"cache\". ### START CODE HERE ### (≈ 2 lines of code) A1 = cache['A1'] A2 = cache['A2'] ### END CODE HERE ### # Backward propagation: calculate dW1, db1, dW2, db2. ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above) dZ2 = A2 - Y dW2 = 1 / m * np.dot(dZ2, A1.T) db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True) dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) dW1 = 1 / m * np.dot(dZ1, X.T) db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True) ### END CODE HERE ### grads = &#123;\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2&#125; return grads 3.4 update parameters最后根据得到的grads，乘上学习速率，就可以更新参数了。 12345678910111213141516171819202122232425262728293031323334353637383940414243# GRADED FUNCTION: update_parametersdef update_parameters(parameters, grads, learning_rate = 1.2): \"\"\" Updates parameters using the gradient descent update rule given above Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients Returns: parameters -- python dictionary containing your updated parameters \"\"\" # Retrieve each parameter from the dictionary \"parameters\" ### START CODE HERE ### (≈ 4 lines of code) W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] ### END CODE HERE ### # Retrieve each gradient from the dictionary \"grads\" ### START CODE HERE ### (≈ 4 lines of code) dW1 = grads['dW1'] db1 = grads['db1'] dW2 = grads['dW2'] db2 = grads['db2'] ## END CODE HERE ### # Update rule for each parameter ### START CODE HERE ### (≈ 4 lines of code) W1 = W1 - learning_rate * dW1 b1 = b1 - learning_rate * db1 W2 = W2 - learning_rate * dW2 b2 = b2 - learning_rate * db2 ### END CODE HERE ### parameters = &#123;\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2&#125; return parameters 然后把更新完的参数再传入前面的循环中，不断循环，直到达到循环的次数。 nn_model把前面的函数都调用过来。 模型中传入的参数是，X,Y，和迭代次数 首先需要得到你要设计的神经网络结构，调用layer_sizes()得到了n_x,n_y，也就是输入层和输出层。 初始化参数initialize_parameters(n_x, n_h, n_y),得到初始化的 W1, b1, W2, b2 然后开始循环 使用forward_propagation(X, parameters),先得到各个神经元的计算值。 然后compute_cost(A2, Y, parameters),得到cost backward_propagation(parameters, cache, X, Y)计算出每一步的梯度 update_parameters(parameters, grads)更新一下参数 返回训练完的parameters 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# GRADED FUNCTION: nn_modeldef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False): \"\"\" Arguments: X -- dataset of shape (2, number of examples) Y -- labels of shape (1, number of examples) n_h -- size of the hidden layer num_iterations -- Number of iterations in gradient descent loop print_cost -- if True, print the cost every 1000 iterations Returns: parameters -- parameters learnt by the model. They can then be used to predict. \"\"\" np.random.seed(3) n_x = layer_sizes(X, Y)[0] n_y = layer_sizes(X, Y)[2] # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\". ### START CODE HERE ### (≈ 5 lines of code) parameters = initialize_parameters(n_x, n_h, n_y) W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] ### END CODE HERE ### # Loop (gradient descent) for i in range(0, num_iterations): ### START CODE HERE ### (≈ 4 lines of code) # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\". A2, cache = forward_propagation(X, parameters) # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\". cost = compute_cost(A2, Y, parameters) # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\". grads = backward_propagation(parameters, cache, X, Y) # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\". parameters = update_parameters(parameters, grads) ### END CODE HERE ### # Print the cost every 1000 iterations if print_cost and i % 1000 == 0: print (\"Cost after iteration %i: %f\" %(i, cost)) return parameters 预测得到训练后的parameters，再用forward_propagation(X, parameters)计算出输出层最终的值A2，以0.5为分界，分为0和1。 123456789101112131415161718192021# GRADED FUNCTION: predictdef predict(parameters, X): \"\"\" Using the learned parameters, predicts a class for each example in X Arguments: parameters -- python dictionary containing your parameters X -- input data of size (n_x, m) Returns predictions -- vector of predictions of our model (red: 0 / blue: 1) \"\"\" # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold. ### START CODE HERE ### (≈ 2 lines of code) A2, cache = forward_propagation(X, parameters) predictions = (A2 &gt; 0.5) ### END CODE HERE ### return predictions 123456# Build a model with a n_h-dimensional hidden layerparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)# Plot the decision boundaryplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)plt.title(\"Decision Boundary for hidden layer size \" + str(4)) 可以看到，训练后神经网络得到的分界线更为合理。 123# Print accuracypredictions = predict(parameters, X)print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%') 准确率高达90% 优化参数这个时候就可以设置不同的hidden_layer的维度大小[1, 2, 3, 4, 5, 20, 50] 123456789101112# This may take about 2 minutes to runplt.figure(figsize=(16, 32))hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]for i, n_h in enumerate(hidden_layer_sizes): plt.subplot(5, 2, i+1) plt.title('Hidden Layer of size %d' % n_h) parameters = nn_model(X, Y, n_h, num_iterations = 5000) plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y) predictions = predict(parameters, X) accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) print (\"Accuracy for &#123;&#125; hidden units: &#123;&#125; %\".format(n_h, accuracy)) 1234567Accuracy for 1 hidden units: 67.5 %Accuracy for 2 hidden units: 67.25 %Accuracy for 3 hidden units: 90.75 %Accuracy for 4 hidden units: 90.5 %Accuracy for 5 hidden units: 91.25 %Accuracy for 20 hidden units: 90.0 %Accuracy for 50 hidden units: 90.25 % 得到的结果在n_h = 5时有最大值。","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"},{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"}]},{"title":"DeepLearning.ai笔记:(1-3)-- 浅层神经网络（Shallow neural networks）","slug":"dl-ai-1-3","date":"2018-09-12T07:34:23.000Z","updated":"2018-09-13T03:02:42.857Z","comments":true,"path":"2018/2018091215/","link":"","permalink":"http://fangzh.top/2018/2018091215/","excerpt":"前面两周讲的是一些logistic和向量化的内容，以及numpy的基本使用，在他之前的机器学习课程中已经讲过了，这里就不再赘述。Week3主要讲了如何搭建两层的神经网络。","text":"前面两周讲的是一些logistic和向量化的内容，以及numpy的基本使用，在他之前的机器学习课程中已经讲过了，这里就不再赘述。Week3主要讲了如何搭建两层的神经网络。 神经网络的表示 这周的内容就围绕着这一张图来讲。 $$a_{j}^{[i]}$$ 这就是每一层神经元的表达方式，上标中括号[]，表示是第几层的神经元；下标表示这个是某一层的第几个神经元。 Input Layer：输入层，也用$a_{j}^{[0]}$，表示第0层 Hidden Layer：表示除了最后一层输出层以外的内部隐藏层 Output Layer：输出层，表示最后一层 而通常神经网络的层数一般不包括输入层。 $w^{[i]}$：每一层的参数$w$的维度是（该层神经元个数，前面一层神经元个数） $b^{[i]}$：为（每一层的神经元个数，1） 计算单个数据的神经网络 由此得到，计算单个数据的神经网络只需要4步： $$z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$$ $$a^{[1]} = \\sigma(z^{[1]})$$ $$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$ $$a^{[2]} = \\sigma(z^{[2]})$$ 多数据的向量化表示我们知道，多个数据的表示就是$x^{(i)}$，使用小括号的上标。神经元也是一样。 如$a^{[1] (i)}$表示第1层神经元的第i个样本。 那么如果有m个样本，一直做for循环来计算出这些神经元的值，实在是太慢了，所以跟logistic一样，可以直接用向量化来表示，这个时候用大写字母来表示。 $$Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]}$$ $$A^{[1]} = \\sigma(Z^{[1]})$$ $$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$$ $$A^{[2]} = \\sigma(Z^{[2]})$$ 这个时候，例如$A^{[1]}$是一个$(n,m)$的矩阵，m是样本数，每一列表示一个样本，n是该层的神经元个数。 从水平上看，矩阵 A代表了各个训练样本。竖直上看，A的不同索引对应不用的隐藏单元。 对矩阵Z和X也是类似，水平方向对应不同的样本，竖直方向上对应不同的输入特征，也就是神经网络输入层的各个节点。 激活函数 在此前都是用sigmoid作为激活函数的。但是激活函数不只有这一种，常用的有4种，分别是：sigmoid, tanh, ReLu, Leaky ReLu。 sigmoid: $a = \\frac{1}{1 + e^{-z}}$ 导数：$a^{\\prime} = a(1-a)$ tanh: $a = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ 导数：$a^{\\prime} = 1 - a^2$ ReLu(修正线性单元): $a = max(0, z)$ Leaky ReLu: $a = max(0.01z, z)$ tips: tanh函数在值域上处于-1和+1之间，所以均值更接近0，使用tanh比sigmoid更能够中心化数据，使得平均值接近0，而不是0.5。 tanh在大多数场合都是优于sigmoid的。 但是sigmoid和tanh有共同的缺点就是z在特别大或者特别小的时候，梯度很小，收敛速度很慢。 而ReLu弥补了两者的不足，在$z &gt; 0$时，梯度始终为1，提高了速度。 Leaky ReLu保证了$z &lt; 0$时，梯度不为0，但是实际上效果差不多。 结论： sigmoid：除了输出层是一个二分类问题的时候使用，不然基本不用 tanh：几乎适用于任何场合 ReLu：默认使用这个，如果不确定你要用哪个激活函数，那就选ReLu或者Leaky ReLu 为什么要使用非线性的激活函数如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与只有一个隐藏层效果相当，这种情况就是多层感知机（MLP）了。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。 梯度下降法公式 这里给出了浅层神经网络的梯度下降法公式。其中$g^{[1]’}(Z^{[1]})$表示你的激活函数的导数。 参数随机初始化在神经网络中,如果将参数全部初始化为0 会导致一个问题，例如对于上面的神经网络的例子，如果将参数全部初始化为0，在每轮参数更新的时候，与输入单元相关的两个隐藏单元的结果将是相同的。 所以初始化时，W要随机初始化，b不存在对称性问题，所以可以设置为0 12W = np.random.rand((2,2))* 0.01b = np.zero((2,1)) 将W乘以0.01是为了让W初始化足够小，因为如果很大的话，Z就很大，用sigmoid或者tanh时，所得到的梯度就会很小，训练过程会变慢。 ReLU和Leaky ReLU作为激活函数时，不存在这种问题，因为在大于0的时候，梯度均为1。 好好做作业，才能有更深的体会！","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"hexo中输入数学公式","slug":"mathjax","date":"2018-09-12T05:39:33.000Z","updated":"2018-09-12T06:00:33.399Z","comments":true,"path":"2018/2018091212/","link":"","permalink":"http://fangzh.top/2018/2018091212/","excerpt":"hexo通过MathJax渲染Latex公式。","text":"hexo通过MathJax渲染Latex公式。 开启hueman主题比较简单，在主题配置文件中找到mathjax： 1mathjax: True 这样就可以了。 页面插入公式插入有两种形式，一种是在行内直接插入，不居中显示： 1$math$ 另一种是在行间插入公式，居中显示： 1$$math$$ 基本语法上下标 ^上标，_表示下标 1234$$a_&#123;1&#125; x^&#123;2&#125; $$$$e^&#123;-\\alpha t&#125; $$$$a^&#123;i&#125;_&#123;ij&#125;$$$$e^&#123;x^2&#125; \\neq &#123;e^x&#125;^2$$ $$a_{1} x^{2}$$$$e^{-\\alpha t}$$$$a^{i}_{ij}$$$$e^{x^2} \\neq {e^x}^2$$ 此外，如果左右两边都有上下标，则使用 \\sideset 命令，效果如下：1\\sideset&#123;^xy&#125;&#123;^xy&#125;\\bigotimes $$\\sideset{^xy}{^xy}\\bigotimes$$ 平方根 平方根输入命令为 \\sqrt，n次方根命令为 \\sqrt[n]，其符号大小由LaTeX 自动给定：12$$\\sqrt&#123;x&#125;$$ $$\\sqrt&#123;x^2+\\sqrt&#123;y&#125;$$ $$\\sqrt[3]&#123;2&#125;$$$$\\sqrt&#123;x&#125;$$ $$ \\sqrt{x^2+\\sqrt{y}}$$$$\\sqrt[3]{2}$$ 水平线使用 \\overline 和 \\underline 分别在表达式上下方画出水平线：12$$\\overline&#123;m + n&#125;$$$$\\underline&#123;m + n&#125;$$ $$\\overline{m + n}$$$$\\underline{m + n}$$ 水平大括号命令 \\overbrace 和 \\underrace，效果如下：12$$\\underbrace&#123;a+b+\\cdots+z&#125;$$$$\\overbrace&#123;a+b+\\cdots+z&#125;$$ $$\\overbrace{a+b+\\cdots+z}$$$$\\underbrace{a+b+\\cdots+z}$$ 矢量矢量的命令是 \\vec，用于单个字母的向量表示。\\overrightarrow 和\\overleftarrow 分别表示向右和向左的向量箭头：123$$\\vec&#123;a&#125;$$$$\\overrightarrow&#123;AB&#125;$$$$\\overleftarrow&#123;BA&#125;$$ $$\\vec{a}$$$$\\overrightarrow{AB}$$$$\\overleftarrow{BA}$$ 分数分数使用 \\frac{…}{…} 进行排版：123$$1\\frac&#123;1&#125;&#123;2&#125;$$$$\\frac&#123;x^2&#125;&#123;k+1&#125;$$$$x^&#123;1/2&#125;$$ $$1\\frac{1}{2}$$$$\\frac{x^2}{k+1}$$$$x^{1/2}$$ 积分运算符积分运算符使用 \\int 生成。求和运算符使用 \\sum 生成。乘积运算符使用 \\prod 生成。上下限使用^ 和_ 命令，类似 上下标：123$$\\sum_&#123;i=1&#125;^&#123;n&#125;$$$$\\int_&#123;0&#125;^&#123;\\frac&#123;\\pi&#125;&#123;2&#125;&#125;$$$$\\prod_\\epsilon$$ $$\\sum_{i=1}^{n}$$$$\\int_{0}^{\\frac{\\pi}{2}}$$$$\\prod_\\epsilon$$ 希腊字母 $\\alpha$ \\alpha $\\beta$ \\beta $\\gamma$ \\gamma $\\delta$ \\delta $\\epsilon$ \\epsilon 字体转换要对公式的某一部分字符进行字体转换，可以用{\\rm需转换的部分字符}命令，其中\\rm可以参照下表选择合适的字体。一般情况下，公式默认为意大利体。 123456789\\rm 罗马体 \\rm test \\it 意大利体 \\it test\\bf 黑体 \\bf test \\cal 花体 \\cal test\\sl 倾斜体 \\sl test \\sf 等线体 \\sf test\\mit 数学斜体 \\mit test \\tt 打字机字体 \\tt test\\sc 小体大写字母 \\sc test","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"博客搭建","slug":"日常技术/博客搭建","permalink":"http://fangzh.top/categories/日常技术/博客搭建/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://fangzh.top/tags/hexo/"},{"name":"blog","slug":"blog","permalink":"http://fangzh.top/tags/blog/"}]},{"title":"hexo教程:搜索SEO+阅读量统计+访问量统计+评论系统(3)","slug":"hexo-3","date":"2018-09-10T10:09:38.000Z","updated":"2018-09-10T13:40:42.532Z","comments":true,"path":"2018/2018090918/","link":"","permalink":"http://fangzh.top/2018/2018090918/","excerpt":"网站做完之后，可以为网站添加一些常用的功能，如能被搜索引擎收录的SEO优化，网站访问量和文章阅读量统计，以及评论系统。","text":"网站做完之后，可以为网站添加一些常用的功能，如能被搜索引擎收录的SEO优化，网站访问量和文章阅读量统计，以及评论系统。 本文参考了: visugar.com这里面说的很详细了。 1. SEO优化推广是很麻烦的事情，怎么样别人才能知道我们呢，首先需要让搜索引擎收录你的这个网站，别人才能搜索的到。那么这就需要SEO优化了。 SEO是由英文Search Engine Optimization缩写而来， 中文意译为“搜索引擎优化”。SEO是指通过站内优化比如网站结构调整、网站内容建设、网站代码优化等以及站外优化。 百度seo刚建站的时候是没有搜索引擎收录我们的网站的。可以在搜索引擎中输入site:&lt;域名&gt; 来查看一下。 1. 登录百度站长平台添加网站 登录百度站长平台，在站点管理中添加你自己的网站。 验证网站有三种方式：文件验证、HTML标签验证、CNAME验证。 第三种方式最简单，只要将它提供给你的那个xxxxx使用CNAME解析到xxx.baidu.com就可以了。也就是登录你的阿里云，把这个解析填进去就OK了。 2. 提交链接 我们需要使用npm自动生成网站的sitemap，然后将生成的sitemap提交到百度和其他搜索引擎 12npm install hexo-generator-sitemap --save npm install hexo-generator-baidu-sitemap --save 这时候你需要在你的根目录下_config.xml中看看url有没有改成你自己的： 重新部署后，就可以在public文件夹下看到生成的sitemap.xml和baidusitemap.xml了。 然后就可以向百度提交你的站点地图了。 这里建议使用自动提交。 自动提交又分为三种：主动推送、自动推送、sitemap。 可以三个一起提交不要紧，我选择的是后两种。 自动推送：把百度生成的自动推送代码，放在主题文件/layout/common/head.ejs的适当位置，然后验证一下就可以了。 sitemap：把两个sitemap地址，提交上去，看到状态正常就OK了。 ps: 百度收录比较慢，慢慢等个十天半个月再去site:&lt;域名&gt;看看有没有被收录。 google的SEO流程一样，google更简单，而且收录更快，进入google站点地图，提交网站和sitemap.xml，就可以了。 如果你这个域名在google这里出了问题，那你就提交 yourname.github.io，这个链接，效果是一样的。 不出意外的话一天内google就能收录你的网站了。 其他的搜索，如搜狗搜索，360搜索，流程是一样的，这里就不再赘述。 2. 评论系统评论系统有很多，但是很多都是墙外的用不了，之前说过这个valine好像集成在hueman和next主题里面了，但是我还没有研究过，我看的是visugar这个博主用的来比力评论系统，感觉也还不错。 来比力官网，注册好后，点击管理页面，在代码管理中找到安装代码： 获取安装代码后，在主题的comment下新建一个文件放入刚刚那段代码，再找到article文件，找到如下代码，若没有则直接在footer后面添加即可。livebe即为刚刚所创文件名称。 1&lt;%- partial(&apos;comment/livebe&apos;) %&gt; 然后可以自己设置一些东西： 还可以设置评论提醒，这样别人评论你的时候就可以及时知道了。 3. 添加百度统计百度统计可以在后台上看到你网站的访问数，浏览量，浏览链接分布等很重要的信息。所以添加百度统计能更有效的让你掌握你的网站情况。 百度统计，注册一下，这里的账号好像和百度账号不是一起的。 照样把代码复制到head.ejs文件中，然后再进行一下安装检查，半小时左右就可以在百度统计里面看到自己的网站信息了。 4. 文章阅读量统计leanCloudleanCloud，进去后注册一下，进入后创建一个应用： 在存储中创建Class，命名为Counter, 然后在设置页面看到你的应用Key，在主题的配置文件中： 1234leancloud_visitors: enable: true app_id: 你的id app_key: 你的key 在article.ejs中适当的位置添加如下，这要看你让文章的阅读量统计显示在哪个地方了， 1阅读数量:&lt;span id=&quot;&lt;%= url_for(post.path) %&gt;&quot; class=&quot;leancloud_visitors&quot; data-flag-title=&quot;&lt;%- post.title %&gt;&quot;&gt;&lt;/span&gt;次 然后在footer.ejs的最后，添加： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;script src=&quot;//cdn1.lncld.net/static/js/2.5.0/av-min.js&quot;&gt;&lt;/script&gt;&lt;script&gt; var APP_ID = &apos;你的app id&apos;; var APP_KEY = &apos;你的app key&apos;; AV.init(&#123; appId: APP_ID, appKey: APP_KEY &#125;); // 显示次数 function showTime(Counter) &#123; var query = new AV.Query(&quot;Counter&quot;); if($(&quot;.leancloud_visitors&quot;).length &gt; 0)&#123; var url = $(&quot;.leancloud_visitors&quot;).attr(&apos;id&apos;).trim(); // where field query.equalTo(&quot;words&quot;, url); // count query.count().then(function (number) &#123; // There are number instances of MyClass where words equals url. $(document.getElementById(url)).text(number? number : &apos;--&apos;); &#125;, function (error) &#123; // error is an instance of AVError. &#125;); &#125; &#125; // 追加pv function addCount(Counter) &#123; var url = $(&quot;.leancloud_visitors&quot;).length &gt; 0 ? $(&quot;.leancloud_visitors&quot;).attr(&apos;id&apos;).trim() : &apos;icafebolger.com&apos;; var Counter = AV.Object.extend(&quot;Counter&quot;); var query = new Counter; query.save(&#123; words: url &#125;).then(function (object) &#123; &#125;) &#125; $(function () &#123; var Counter = AV.Object.extend(&quot;Counter&quot;); addCount(Counter); showTime(Counter); &#125;);&lt;/script&gt; 重新部署后就可以了。 5. 引入不蒜子访问量和访问人次统计不蒜子的添加非常非常方便，不蒜子 在footer.ejs中的合适位置，看你要显示在哪个地方，添加： 1234&lt;!--这一段是不蒜子的访问量统计代码--&gt;&lt;script async src=&quot;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;&lt;span id=&quot;busuanzi_container_site_pv&quot;&gt;本站总访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次 &amp;nbsp; &lt;/span&gt;&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt;访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人次&lt;/span&gt; 就可以了。 总结到这里就基本做完了。其实都是参考别的博主的设置的，不一定仅限于hueman主题，其他主题的设置也是大体相同的，所以如果你希望设置别的主题，那么仔细看一下这个主题的代码结构，也能够把上边的功能添加进去。 多看看别的博主的那些功能，如果有你能找到自己喜欢的功能，那么好好发动搜索技能，很快就能找到怎么做了。加油吧！","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"博客搭建","slug":"日常技术/博客搭建","permalink":"http://fangzh.top/categories/日常技术/博客搭建/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://fangzh.top/tags/hexo/"},{"name":"blog","slug":"blog","permalink":"http://fangzh.top/tags/blog/"},{"name":"教程","slug":"教程","permalink":"http://fangzh.top/tags/教程/"}]},{"title":"你见过什么样的云霄？","slug":"yx","date":"2018-09-10T07:21:32.000Z","updated":"2018-09-10T12:58:01.887Z","comments":true,"path":"2018/2018091015/","link":"","permalink":"http://fangzh.top/2018/2018091015/","excerpt":"","text":"花了三年时间做的家乡的航拍视频，有点生涩，顺便放上来试试hexo的视频嵌入。 你见过什么样的云霄？ 123456789&lt;iframe height=300 width=510 src=&apos;http://player.youku.com/embed/XMzc4NzA3Njg0MA==&apos; frameborder=0 allowfullscreen&gt;&lt;/iframe&gt;","categories":[{"name":"生活","slug":"生活","permalink":"http://fangzh.top/categories/生活/"},{"name":"旅行","slug":"生活/旅行","permalink":"http://fangzh.top/categories/生活/旅行/"}],"tags":[{"name":"航拍","slug":"航拍","permalink":"http://fangzh.top/tags/航拍/"},{"name":"旅行","slug":"旅行","permalink":"http://fangzh.top/tags/旅行/"},{"name":"视频","slug":"视频","permalink":"http://fangzh.top/tags/视频/"}]},{"title":"hexo教程:基本配置+更换主题+多终端工作+coding page部署分流(2)","slug":"hexo-2","date":"2018-09-07T07:18:31.000Z","updated":"2018-09-10T12:58:01.887Z","comments":true,"path":"2018/2018090715/","link":"","permalink":"http://fangzh.top/2018/2018090715/","excerpt":"上次介绍了hexo的基本搭建和部署。但是还有很多事情没有解决，这次先来看看hexo的基本配置文件，还有如何在多平台部署实现国内外分流，以及换电脑后如何无缝的衔接工作。","text":"上次介绍了hexo的基本搭建和部署。但是还有很多事情没有解决，这次先来看看hexo的基本配置文件，还有如何在多平台部署实现国内外分流，以及换电脑后如何无缝的衔接工作。 1. hexo基本配置在文件根目录下的_config.yml，就是整个hexo框架的配置文件了。可以在里面修改大部分的配置。详细可参考官方的配置描述。 网站 参数 描述 title 网站标题 subtitle 网站副标题 description 网站描述 author 您的名字 language 网站使用的语言 timezone 网站时区。Hexo 默认使用您电脑的时区。时区列表。比如说：America/New_York, Japan, 和 UTC 。 其中，description主要用于SEO，告诉搜索引擎一个关于您站点的简单描述，通常建议在其中包含您网站的关键词。author参数用于主题显示文章的作者。 网址 参数 描述 url 网址 root 网站根目录 permalink 文章的 永久链接 格式 permalink_defaults 永久链接中各部分的默认值 在这里，你需要把url改成你的网站域名。 permalink，也就是你生成某个文章时的那个链接格式。 比如我新建一个文章叫temp.md，那么这个时候他自动生成的地址就是http://yoursite.com/2018/09/05/temp。 以下是官方给出的示例，关于链接的变量还有很多，需要的可以去官网上查找 永久链接 。 参数 结果 :year/:month/:day/:title/ 2013/07/14/hello-world :year-:month-:day-:title.html 2013-07-14-hello-world.html :category/:title foo/bar/hello-world 再往下翻，中间这些都默认就好了。 12345678theme: landscape# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: &lt;repository url&gt; branch: [branch] theme就是选择什么主题，也就是在theme这个文件夹下，在官网上有很多个主题，默认给你安装的是lanscape这个主题。当你需要更换主题时，在官网上下载，把主题的文件放在theme文件夹下，再修改这个参数就可以了。 接下来这个deploy就是网站的部署的，repo就是仓库(Repository)的简写。branch选择仓库的哪个分支。这个在之前进行github page部署的时候已经修改过了，不再赘述。而这个在后面进行双平台部署的时候会再次用到。 Front-matterFront-matter 是文件最上方以 --- 分隔的区域，用于指定个别文件的变量，举例来说： 123title: Hello Worlddate: 2013/7/13 20:46:25--- 下是预先定义的参数，您可在模板中使用这些参数值并加以利用。 参数 描述 layout 布局 title 标题 date 建立日期 updated 更新日期 comments 开启文章的评论功能 tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章网址 其中，分类和标签需要区别一下，分类具有顺序性和层次性，也就是说 Foo, Bar 不等于 Bar, Foo；而标签没有顺序和层次。 12345categories:- Diarytags:- PS3- Games layout（布局）当你每一次使用代码 1hexo new paper 它其实默认使用的是post这个布局，也就是在source文件夹下的_post里面。 Hexo 有三种默认布局：post、page 和 draft，它们分别对应不同的路径，而您自定义的其他布局和 post 相同，都将储存到 source/_posts 文件夹。 布局 路径 post source/_posts page source draft source/_drafts 而new这个命令其实是： 1hexo new [layout] &lt;title&gt; 只不过这个layout默认是post罢了。 page如果你想另起一页，那么可以使用 1hexo new page board 系统会自动给你在source文件夹下创建一个board文件夹，以及board文件夹中的index.md，这样你访问的board对应的链接就是http://xxx.xxx/board draftdraft是草稿的意思，也就是你如果想写文章，又不希望被看到，那么可以 1hexo new draft newpage 这样会在source/_draft中新建一个newpage.md文件，如果你的草稿文件写的过程中，想要预览一下，那么可以使用 1hexo server --draft 在本地端口中开启服务预览。 如果你的草稿文件写完了，想要发表到post中， 1hexo publish draft newpage 就会自动把newpage.md发送到post中。 2. 更换主题到这一步，如果你觉得默认的landscape主题不好看，那么可以在官网的主题中，选择你喜欢的一个主题进行修改就可以啦。点这里 这里有200多个主题可以选。不过最受欢迎的就是那么几个，比如NexT主题，非常的简洁好看，大多数人都选择这个，关于这个的教程也比较多。不过我选择的是hueman这个主题，好像是从WordPress移植过来的，展示效果如下： 不管怎么样，至少是符合我个人的审美。 直接在github链接上下载下来，然后放到theme文件夹下就行了，然后再在刚才说的配置文件中把theme换成那个主题文件夹的名字，它就会自动在theme文件夹中搜索你配置的主题。 而后进入hueman这个文件夹，可以看到里面也有一个配置文件_config.xml，貌似它默认是_config.xml.example，把它复制一份，重命名为_config.xml就可以了。这个配置文件是修改你整个主题的配置文件。 menu（菜单栏）也就是上面菜单栏上的这些东西。 其中，About这个你是找不到网页的，因为你的文章中没有about这个东西。如果你想要的话，可以执行命令 1hexo new page about 它就会在根目录下source文件夹中新建了一个about文件夹，以及index.md，在index.md中写上你想要写的东西，就可以在网站上展示出来了。 如果你想要自己再自定义一个菜单栏的选项，那么就 1hexo new page yourdiy 然后在主题配置文件的menu菜单栏添加一个 Yourdiy : /yourdiy，注意冒号后面要有空格，以及前面的空格要和menu中默认的保持整齐。然后在languages文件夹中，找到zh-CN.yml，在index中添加yourdiy: &#39;中文意思&#39;就可以显示中文了。 customize(定制)在这里可以修改你的个人logo，默认是那个hueman，在source/css/images文件夹中放入自己要的logo，再改一下url的链接名字就可以了。 favicon是网站中出现的那个小图标的icon，找一张你喜欢的logo，然后转换成ico格式，放在images文件夹下，配置一下路径就行。 social_links ，可以显示你的社交链接，而且是有logo的。 tips: 在这里可以添加一个rss功能，也就是那个符号像wifi一样的东西。 添加RSS1. 什么是RSS？ RSS也就是订阅功能，你可以理解为类似与订阅公众号的功能，来订阅各种博客，杂志等等。 2. 为什么要用RSS？ 就如同订阅公众号一样，你对某个公众号感兴趣，你总不可能一直时不时搜索这个公众号来看它的文章吧。博客也是一样，如果你喜欢某个博主，或者某个平台的内容，你可以通过RSS订阅它们，然后在RSS阅读器上可以实时推送这些消息。现在网上的垃圾消息太多了，如果你每一天都在看这些消息中度过，漫无目的的浏览，只会让你的时间一点一点的流逝，太不值得了。如果你关注的博主每次都发的消息都是精华，而且不是每一天十几条几十条的轰炸你，那么这个博主就值得你的关注，你就可以通过RSS订阅他。 在我的理解中，如果你不想每天都被那些没有质量的消息轰炸，只想安安静静的关注几个博主，每天看一些有质量的内容也不用太多，那么RSS订阅值得你的拥有。 3. 添加RSS功能 先安装RSS插件 1npm i hexo-generator-feed 而后在你整个项目的_config.yml中找到Extensions，添加： 12345678910# Extensions## Plugins: https://hexo.io/plugins/#RSS订阅plugin:- hexo-generator-feed#Feed Atomfeed: type: atom path: atom.xml limit: 20 这个时候你的RSS链接就是 域名/atom.xml了。 所以，在主题配置文件中的这个social links，开启RSS的页面功能，这样你网站上就有那个像wifi一样符号的RSS logo了，注意空格。 1rss: /atom.xml 4. 如何关注RSS？ 首先，你需要一个RSS阅读器，在这里我推荐inoreader，宇宙第一RSS阅读器，而且中文支持的挺好。不过它没有PC端的程序，只有网页版，chrome上有插件。在官网上用google账号或者自己注册账号登录，就可以开始你的关注之旅了。 每次需要关注某个博主时，就点开他的RSS链接，把链接复制到inoreader上，就能关注了，当然，如果是比较大众化的很厉害的博主，你直接搜名字也可以的，比如每个人都非常佩服的阮一峰大师，直接在阅读器上搜索阮一峰，应该就能出来了。 我关注的比如，阮一峰的网络日志，月光博客，知乎精选等，都很不错。当然，还有我！！赶快关注我吧！你值得拥有：http://fangzh.top/atom.xml 在安卓端，inoreader也有下载，不过因为国内google是登录不了的，你需要在inoreader官网上把你的密码修改了，然后就可以用账户名和密码登录了。 在IOS端，没用过，好像是reader 3可以支持inoreader账户，还有个readon也不错，可以去试试。 widgets(侧边栏)侧边栏的小标签，如果你想自己增加一个，比如我增加了一个联系方式，那么我把communication写在上面，在zh-CN.yml中的sidebar，添加communication: &#39;中文&#39;。 然后在hueman/layout/widget中添加一个communicaiton.ejs，填入模板： 12345678&lt;% if (site.posts.length) &#123; %&gt; &lt;div class=\"widget-wrap widget-list\"&gt; &lt;h3 class=\"widget-title\"&gt;&lt;%= __('sidebar.communiation') %&gt;&lt;/h3&gt; &lt;div class=\"widget\"&gt; &lt;!--这里添加你要写的内容--&gt; &lt;/div&gt; &lt;/div&gt;&lt;% &#125; %&gt; search(搜索框)默认搜索框是不能够用的， you need to install hexo-generator-json-content before using Insight Search 它已经告诉你了，如果想要使用，就安装这个插件。 comment(评论系统)这里的多数都是国外的，基本用不了。这个valine好像不错，还能统计文章阅读量，可以自己试一试，链接。 miscellaneous(其他)这里我就改了一个links，可以添加友链。注意空格要对！不然会报错！ 总结：整个主题看起来好像很复杂的样子，但是仔细捋一捋其实也比较流畅， languages: 顾名思义 layout：布局文件，其实后期想要修改自定义网站上的东西，添加各种各样的信息，主要是在这里修改，其中comment是评论系统，common是常规的布局，最常修改的在这里面，比如修改页面head和footer的内容。 scripts：js脚本，暂时没什么用 source：里面放了一些css的样式，以及图片 3. git分支进行多终端工作问题来了，如果你现在在自己的笔记本上写的博客，部署在了网站上，那么你在家里用台式机，或者实验室的台式机，发现你电脑里面没有博客的文件，或者要换电脑了，最后不知道怎么移动文件，怎么办？ 在这里我们就可以利用git的分支系统进行多终端工作了，这样每次打开不一样的电脑，只需要进行简单的配置和在github上把文件同步下来，就可以无缝操作了。 机制机制是这样的，由于hexo d上传部署到github的其实是hexo编译后的文件，是用来生成网页的，不包含源文件。 也就是上传的是在本地目录里自动生成的.deploy_git里面。 其他文件 ，包括我们写在source 里面的，和配置文件，主题文件，都没有上传到github 所以可以利用git的分支管理，将源文件上传到github的另一个分支即可。 上传分支首先，先在github上新建一个hexo分支，如图： 然后在这个仓库的settings中，选择默认分支为hexo分支（这样每次同步的时候就不用指定分支，比较方便）。 然后在本地的任意目录下，打开git bash， 1git clone git@github.com:ZJUFangzh/ZJUFangzh.github.io.git 将其克隆到本地，因为默认分支已经设成了hexo，所以clone时只clone了hexo。 接下来在克隆到本地的ZJUFangzh.github.io中，把除了.git 文件夹外的所有文件都删掉 把之前我们写的博客源文件全部复制过来，除了.deploy_git。这里应该说一句，复制过来的源文件应该有一个.gitignore，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下，表示这些类型文件不需要git： 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 注意，如果你之前克隆过theme中的主题文件，那么应该把主题文件中的.git文件夹删掉，因为git不能嵌套上传，最好是显示隐藏文件，检查一下有没有，否则上传的时候会出错，导致你的主题文件无法上传，这样你的配置在别的电脑上就用不了了。 而后 123git add .git commit –m \"add branch\"git push 这样就上传完了，可以去你的github上看一看hexo分支有没有上传上去，其中node_modules、public、db.json已经被忽略掉了，没有关系，不需要上传的，因为在别的电脑上需要重新输入命令安装 。 这样就上传完了。 更换电脑操作一样的，跟之前的环境搭建一样， 安装git 1sudo apt-get install git 设置git全局邮箱和用户名 12git config --global user.name &quot;yourgithubname&quot;git config --global user.email &quot;yourgithubemail&quot; 设置ssh key 12345ssh-keygen -t rsa -C &quot;youremail&quot;#生成后填到github和coding上（有coding平台的话）#验证是否成功ssh -T git@github.comssh -T git@git.coding.net #(有coding平台的话) 安装nodejs 12sudo apt-get install nodejssudo apt-get install npm 安装hexo 1sudo npm install hexo-cli -g 但是已经不需要初始化了， 直接在任意文件夹下， 1git clone git@……………… 然后进入克隆到的文件夹： 123cd xxx.github.ionpm installnpm install hexo-deployer-git --save 生成，部署： 12hexo ghexo d 然后就可以开始写你的新博客了 1hexo new newpage Tips: 不要忘了，每次写完最好都把源文件上传一下 123git add .git commit –m &quot;xxxx&quot;git push 如果是在已经编辑过的电脑上，已经有clone文件夹了，那么，每次只要和远端同步一下就行了 1git pull 4. coding page上部署实现国内外分流之前我们已经把hexo托管在github了，但是github是国外的，而且百度的爬虫是不能够爬取github的，所以如果你希望你做的博客能够在百度引擎上被收录，而且想要更快的访问，那么可以在国内的coding page做一个托管，这样在国内访问就是coding page，国外就走github page。 1. 申请coding账户，新建项目 先申请一个账户，然后创建新的项目，这一步项目名称应该是随意的。 2. 添加ssh key 这一步跟github一样。 添加后，检查一下是不是添加成功 1ssh -T git@git.coding.net 3. 修改_config.yml hexo官方文档是这样的： 123456deploy: type: git message: [message] repo: github: &lt;repository url&gt;,[branch] coding: &lt;repository url&gt;,[branch] 那么，我们只需要： 12345deploy: type: git repo: coding: git@git.coding.net:ZJUFangzh/ZJUFangzh.git,master github: git@github.com:ZJUFangzh/ZJUFangzh.github.io.git,master 4. 部署 保存一下，直接 12hexo ghexo d 这样就可以在coding的项目上看到你部署的文件了。 5. 开启coding pages服务，绑定域名 如图： 6. 阿里云添加解析 这个时候就可以把之前github的解析改成境外，把coding的解析设为默认了。 7. 去除coding page的跳转广告 coding page的一个比较恶心人的地方就是，你只是银牌会员的话，访问会先跳转到一个广告，再到你自己的域名。那么它也给出了消除的办法。右上角切换到coding的旧版界面，默认新版是不行的。然后再来到pages服务这里。 这里： 只要你在页面上添加一行文字，写Hosted by Coding Pages，然后点下面的小勾勾，两个工作日内它就会审核通过了。 1&lt;p&gt;Hosted by &lt;a href=&quot;https://pages.coding.me&quot; style=&quot;font-weight: bold&quot;&gt;Coding Pages&lt;/a&gt;&lt;/p&gt; 我的选择是把这一行代码放在主题文件夹/layout/common/footer.ejs里面，也就是本来在页面中看到的页脚部分。 当然，为了统一，我又在后面加上了and Github哈哈，可以不加。 1&lt;p&gt;&lt;span&gt;Hosted by &lt;a href=&quot;https://pages.coding.me&quot; style=&quot;font-weight: bold&quot;&gt;Coding Pages&lt;/a&gt;&lt;/span&gt; and &lt;span&gt;&lt;a href=&quot;https://github.com&quot; style=&quot;font-weight: bold&quot;&gt;Github&lt;/a&gt;&lt;/span&gt;&lt;/p&gt; 这是最终加上去的代码。 至此，关于hexo的基本文件配置，主题更换，多终端同步，多平台部署已经介绍完了。 这一次就先到这里了，下回再讲讲如何优化网站的SEO、以及在主题中添加评论系统、阅读量统计等等，谢谢大家。","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"博客搭建","slug":"日常技术/博客搭建","permalink":"http://fangzh.top/categories/日常技术/博客搭建/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://fangzh.top/tags/hexo/"},{"name":"blog","slug":"blog","permalink":"http://fangzh.top/tags/blog/"},{"name":"教程","slug":"教程","permalink":"http://fangzh.top/tags/教程/"}]},{"title":"Linux安装shadowcocks","slug":"Linux安装shadowcocks","date":"2018-09-05T13:53:29.000Z","updated":"2018-09-10T12:58:01.887Z","comments":true,"path":"2018/2018090522/","link":"","permalink":"http://fangzh.top/2018/2018090522/","excerpt":"对于windows来说，只要下载一个shadowsocks的应用程序就行了。 github上一大堆shadowsocks-windows Linux上，可以用shell命令行解决的，绝不用GUI。","text":"对于windows来说，只要下载一个shadowsocks的应用程序就行了。 github上一大堆shadowsocks-windows Linux上，可以用shell命令行解决的，绝不用GUI。 123sudo apt-get install python-pippip install shadowsocks 接下来配置文件 shadowsocks.json，随便找个地方，你记得住的地方保存。123456789101112131415161718&#123; \"server\":\"my_server_ip\", \"local_address\": \"127.0.0.1\", \"local_port\":1080, \"server_port\":my_server_port, \"password\":\"my_password\", \"timeout\":300, \"method\":\"aes-256-cfb\"&#125; my_server_ip:你的账户ip my_server_port:你的账户端口 my_password:你的账户密码 method:输入你账户的加密方式 配置完成后，分前端启动和后端启动 前端启动就是你那个窗口得一直开着 后面这一段是你刚才建立的json文件地址1sudo sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json 后端启动在后端自己挂着（推荐）1sudo sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json -d start 后端停止1sudo sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json -d stop 重启（修改配置后要重启才能生效） 1sudo sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json -d restart 在此，建议把命令行做成一个.sh文件，放在桌面，想开的时候就可以随时执行shadowsocks.sh 123#! /bin/bashsudo sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json -d start 配置好后，还需要在chrome浏览器中配置switchomega（插件），如果没有，自己去下一个。因为我们肯定是希望在指定的国外网站进行科学上网，而在国内的网站，就不需要用shadowsocks做转发了，这样很慢。所以配置一个有一定规则的列表，是很有必要的。详细的switchomega配置过程网上一大堆，这里就不详细说明了。 当然，如果你嫌麻烦，觉得以上用shell配置shadowsocks的方法太复杂，那直接下一个linux下的shadowsocks-Qt5吧。 还有安卓版的： shadowsocks-android","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"Linux","slug":"日常技术/Linux","permalink":"http://fangzh.top/categories/日常技术/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://fangzh.top/tags/Linux/"},{"name":"install","slug":"install","permalink":"http://fangzh.top/tags/install/"},{"name":"shadowsocks","slug":"shadowsocks","permalink":"http://fangzh.top/tags/shadowsocks/"}]},{"title":"Linux安装anaconda","slug":"Linux安装anaconda","date":"2018-09-05T13:52:53.000Z","updated":"2018-09-10T12:58:01.887Z","comments":true,"path":"2018/2018090521/","link":"","permalink":"http://fangzh.top/2018/2018090521/","excerpt":"Anaconda是python的一个很好的发行版，安装了anaconda就可以解决很多python第三方库的问题。","text":"Anaconda是python的一个很好的发行版，安装了anaconda就可以解决很多python第三方库的问题。 首先，检查一下电脑中的python版本。 123$ which python3/usr/bin/python3 此时调用的python3版本在/usr/bin/中。 1. Download AnacondaDownload Anaconda 2. 安装 Anaconda这里选择你下载的那个文件（可以用tab自动补全） 1bash ~/Download/Anaconda3-5.2.0-Linux-x86_64.sh 3. 添加入path输入： 1source ~/.bashrc 自动添加完毕。 如果不行，可以手动添加（慎用） 12echo &apos;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&apos; &gt;&gt; ~/.bashrcsource ~/.bashrc 这个时候，pip已经可以使用了。用which pip可以显示在anaconda的pip。 输入 python3，也显示的是anaconda的python3。 这时候如果需要调用系统自带的python 则需要输入 12345sudo python3 # 3.6.5#或者sudo python # 2.7 具体可以查看anaconda的使用帮助。","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"Linux","slug":"日常技术/Linux","permalink":"http://fangzh.top/categories/日常技术/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://fangzh.top/tags/Linux/"},{"name":"install","slug":"install","permalink":"http://fangzh.top/tags/install/"},{"name":"python","slug":"python","permalink":"http://fangzh.top/tags/python/"}]},{"title":"Linux安装selenium+chromedriver","slug":"Linux安装selenium-chromedriver","date":"2018-09-05T13:51:41.000Z","updated":"2018-09-10T12:58:01.887Z","comments":true,"path":"2018/2018090520/","link":"","permalink":"http://fangzh.top/2018/2018090520/","excerpt":"Selenium是爬虫中用来模拟JS的利器。 下面介绍一下Linux安装selenium和chromedriver的具体做法。","text":"Selenium是爬虫中用来模拟JS的利器。 下面介绍一下Linux安装selenium和chromedriver的具体做法。 1. install selenium首先确保已经安装了pip命令，接下来： 1sudo pip install -U selenium 2. install chromedriver在Chromedriver网站上找到对应的版本，一般是最新版，如果你选的版本和电脑上的Chrome不互相匹配的话，在运行爬虫的时候会报错。（在网站里面的LATEST_RELEASE中可以找到最新版，不一定按那个序号来的） 找到后，把下面的2.41改成你要安装的版本。1wget -N http://chromedriver.storage.googleapis.com/2.41/chromedriver_linux64.zip 然后 1234567unzip chromedriver_linux64.zip #解压你下载的那个包chmod +x chromedriver #修改用户权限为可执行sudo mv -f chromedriver /usr/local/share/chromedriver #将解压后的文件移动到指定目录#在指定目录link到别的目录sudo ln -s /usr/local/share/chromedriver /usr/local/bin/chromedriver sudo ln -s /usr/local/share/chromedriver /usr/bin/chromedriver 一通操作后，你的selenium和chromedriver应该可以正常使用了。 123456from selenium import webdriverdriver = webdriver.Chrome()driver.get('https://www.baidu.com/')print('打开浏览器')print(driver.title)driver.quit()","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"Linux","slug":"日常技术/Linux","permalink":"http://fangzh.top/categories/日常技术/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://fangzh.top/tags/Linux/"},{"name":"install","slug":"install","permalink":"http://fangzh.top/tags/install/"},{"name":"python","slug":"python","permalink":"http://fangzh.top/tags/python/"}]},{"title":"hexo教程：github page+独立域名搭建(1)","slug":"hexo(1)","date":"2018-09-05T05:38:44.000Z","updated":"2018-09-10T12:58:01.887Z","comments":true,"path":"2018/2018090514/","link":"","permalink":"http://fangzh.top/2018/2018090514/","excerpt":"","text":"喜欢写Blog的人，会经历三个阶段。 第一阶段，刚接触Blog，觉得很新鲜，试着选择一个免费空间来写。 第二阶段，发现免费空间限制太多，就自己购买域名和空间，搭建独立博客。 第三阶段，觉得独立博客的管理太麻烦，最好在保留控制权的前提下，让别人来管，自己只负责写文章。 ——阮一峰 现在市面上的博客很多，如CSDN，博客园，简书等平台，可以直接在上面发表，用户交互做的好，写的文章百度也能搜索的到。缺点是比较不自由，会受到平台的各种限制和恶心的广告。 而自己购买域名和服务器，搭建博客的成本实在是太高了，不光是说这些购买成本，单单是花力气去自己搭这么一个网站，还要定期的维护它，对于我们大多数人来说，实在是没有这样的精力和时间。 那么就有第三种选择，直接在github page平台上托管我们的博客。这样就可以安心的来写作，又不需要定期维护，而且hexo作为一个快速简洁的博客框架，用它来搭建博客真的非常容易。 Hexo简介Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。大家可以进入hexo官网进行详细查看，因为Hexo的创建者是台湾人，对中文的支持很友好，可以选择中文进行查看。 Hexo搭建步骤 安装Git 安装Node.js 安装Hexo GitHub创建个人仓库 生成SSH添加到GitHub 将hexo部署到GitHub 设置个人域名 发布文章 1. 安装GitGit是目前世界上最先进的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。也就是用来管理你的hexo博客文章，上传到GitHub的工具。Git非常强大，我觉得建议每个人都去了解一下。廖雪峰老师的Git教程写的非常好，大家可以了解一下。Git教程 windows：到git官网上下载,Download git,下载后会有一个Git Bash的命令行工具，以后就用这个工具来使用git。 linux：对linux来说实在是太简单了，因为最早的git就是在linux上编写的，只需要一行代码 1sudo apt-get install git 安装好后，用git --version 来查看一下版本 2. 安装nodejsHexo是基于nodeJS编写的，所以需要安装一下nodeJs和里面的npm工具。 windows：nodejs选择LTS版本就行了。 linux：12sudo apt-get install nodejssudo apt-get install npm 安装完后，打开命令行12node -vnpm -v 检查一下有没有安装成功 顺便说一下，windows在git安装完后，就可以直接使用git bash来敲命令行了，不用自带的cmd，cmd有点难用。 3. 安装hexo前面git和nodejs安装好后，就可以安装hexo了，你可以先创建一个文件夹blog，然后cd到这个文件夹下（或者在这个文件夹下直接右键git bash打开）。 输入命令 1npm install -g hexo-cli 依旧用hexo -v查看一下版本 至此就全部安装完了。 接下来初始化一下hexo 1hexo init myblog 这个myblog可以自己取什么名字都行，然后12cd myblog //进入这个myblog文件夹npm install 新建完成后，指定文件夹目录下有： node_modules: 依赖包 public：存放生成的页面 scaffolds：生成文章的一些模板 source：用来存放你的文章 themes：主题 _config.yml: 博客的配置文件 12hexo ghexo server 打开hexo的服务，在浏览器输入localhost:4000就可以看到你生成的博客了。 大概长这样：使用ctrl+c可以把服务关掉。 4. GitHub创建个人仓库首先，你先要有一个GitHub账户，去注册一个吧。 注册完登录后，在GitHub.com中看到一个New repository，新建仓库 创建一个和你用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHub page的时候，才会被识别，也就是xxxx.github.io，其中xxx就是你注册GitHub的用户名。我这里是已经建过了。 点击create repository。 5. 生成SSH添加到GitHub回到你的git bash中，12git config --global user.name &quot;yourname&quot;git config --global user.email &quot;youremail&quot; 这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。 可以用以下两条，检查一下你有没有输对12git config user.namegit config user.email 然后创建SSH,一路回车1ssh-keygen -t rsa -C &quot;youremail&quot; 这个时候它会告诉你已经生成了.ssh的文件夹。在你的电脑中找到这个文件夹。 ssh，简单来讲，就是一个秘钥，其中，id_rsa是你这台电脑的私人秘钥，不能给别人看的，id_rsa.pub是公共秘钥，可以随便给别人看。把这个公钥放在GitHub上，这样当你链接GitHub自己的账户时，它就会根据公钥匹配你的私钥，当能够相互匹配时，才能够顺利的通过git上传你的文件到GitHub上。 而后在GitHub的setting中，找到SSH keys的设置选项，点击New SSH key把你的id_rsa.pub里面的信息复制进去。 在gitbash中，查看是否成功1ssh -T git@github.com 6. 将hexo部署到GitHub这一步，我们就可以将hexo和GitHub关联起来，也就是将hexo生成的文章部署到GitHub上，打开站点配置文件 _config.yml，翻到最后，修改为YourgithubName就是你的GitHub账户1234deploy: type: git repo: https://github.com/YourgithubName/YourgithubName.github.io.git branch: master 这个时候需要先安装deploy-git ，也就是部署的命令,这样你才能用命令部署到GitHub。1npm install hexo-deployer-git --save 然后123hexo cleanhexo generatehexo deploy 其中 hexo clean清除了你之前生成的东西，也可以不加。hexo generate 顾名思义，生成静态文章，可以用 hexo g缩写hexo deploy 部署文章，可以用hexo d缩写 注意deploy时可能要你输入username和password。 得到下图就说明部署成功了，过一会儿就可以在http://yourname.github.io 这个网站看到你的博客了！！ 7. 设置个人域名现在你的个人网站的地址是 yourname.github.io，如果觉得这个网址逼格不太够，这就需要你设置个人域名了。但是需要花钱。 注册一个阿里云账户,在阿里云上买一个域名，我买的是 fangzh.top，各个后缀的价格不太一样，比如最广泛的.com就比较贵，看个人喜好咯。 你需要先去进行实名认证,然后在域名控制台中，看到你购买的域名。 点解析进去，添加解析。 其中，192.30.252.153 和 192.30.252.154 是GitHub的服务器地址。注意，解析线路选择默认，不要像我一样选境外。这个境外是后面来做国内外分流用的,在后面的博客中会讲到。记得现在选择默认！！ 登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入你的域名fangzh.top 然后在你的博客文件source中创建一个名为CNAME文件，不要后缀。写上你的域名。 最后，在gitbash中，输入123hexo cleanhexo ghexo d 过不了多久，再打开你的浏览器，输入你自己的域名，就可以看到搭建的网站啦！ 接下来你就可以正式开始写文章了。 1hexo new newpapername 然后在source/_post中打开markdown文件，就可以开始编辑了。当你写完的时候，再123hexo cleanhexo ghexo d 就可以看到更新了。 至于更换网站主题，还有添加各种各样的功能等等，在往后的系列博客中，再进行介绍。","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"博客搭建","slug":"日常技术/博客搭建","permalink":"http://fangzh.top/categories/日常技术/博客搭建/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://fangzh.top/tags/hexo/"},{"name":"blog","slug":"blog","permalink":"http://fangzh.top/tags/blog/"},{"name":"教程","slug":"教程","permalink":"http://fangzh.top/tags/教程/"}]},{"title":"开篇博客","slug":"开篇博客","date":"2018-09-04T05:38:44.000Z","updated":"2018-09-10T12:58:01.887Z","comments":true,"path":"2018/2018090413/","link":"","permalink":"http://fangzh.top/2018/2018090413/","excerpt":"开个博客，写写东西。","text":"开个博客，写写东西。 很早之前就想建一个博客来写点东西，毕竟已经很久没有写过文章了，所以现在再提笔显得十分生涩，不像几年前那样能随意自在。 花了三天时间搭建了这个博客，此前原以为很复杂的，没有想到这么快就能搭好，一方面是自己的技术水平提升，很多以前不懂的技术，在这半年的自学以来都慢慢入门，另一方面，其实这就是一层窗户纸，对于有勇气的人来说，很快就能搭好了，因为困难总是可以解决的，怕的是畏惧这些困难。 中国的教育总是让你好好学习，其他的都不要管，好好高考，考个好大学。然而并没有教你应该做一个什么样的人，长大了应该做什么。所以我们大多数人在高考填志愿的时候并不知道要填什么专业，就这样稀里糊涂的跟着大部队走了。再者上了大学，也没有人告诉你接下来要做什么，所以中国的大学教育是极其失败的，很多人都是白白浪费了4年的光阴。 有很长一段时间，我都不知道自己要做什么。看着眼前的一个个项目，啥事也不想干，只能水水手机，水水b站，根本就没有想要干活的欲望。加上老师也不管事，就这样又水过了很多时间。不过人生总是有这么些时候的，能够及时纠正自己，就能更远的前行。 自学人工智能，机器学习等知识也有半年了，我知道很多人都在往这方向转，我是觉得人工智能挺有趣的，能够拯救世界。而且很多半路转行的人，今年找工作的时候都被刷了，算法岗一抓一大把，每个人都想进来，就连我那些很厉害的CS同学也在为找工作而焦虑。 不过不管怎么样，好好提高自己的技术水平和能力，总是不错的，不管之后能找到什么样的工作，慢慢变成一个更厉害的人，不要惶惶然无所事事，就能对得起自己。 搭建博客是很简单的，但是想坚持下来就没那么容易了。立个flag，希望自己能够坚持写博客，发文章，不管是技术教程，还是解决了哪一些问题，亦或是旅途中的美景、人生感悟都可以，希望能坚持到毕业。 一起加油吧。年轻的时候，看到一座山，总想知道山后面是什么。其实翻过山后，山的后面并没有什么特别的，但是我依然会去越过山丘，因为在越过山丘的过程中看到的风景，才是最重要的。","categories":[{"name":"生活","slug":"生活","permalink":"http://fangzh.top/categories/生活/"},{"name":"随笔","slug":"生活/随笔","permalink":"http://fangzh.top/categories/生活/随笔/"}],"tags":[{"name":"生活","slug":"生活","permalink":"http://fangzh.top/tags/生活/"}]}]}