{"meta":{"title":"Fangzh的个人博客 | 人工智能拯救世界","subtitle":"人工智能、人生感悟","description":"人工智能、计算机、机器学习、linux、程序员","author":"Fangzh","url":"http://fangzh.top"},"pages":[{"title":"","date":"2018-09-10T07:05:02.669Z","updated":"2018-09-10T07:05:02.669Z","comments":true,"path":"todo.html","permalink":"http://fangzh.top/todo.html","excerpt":"","text":"将要写的博客 hexo(3) done layout的几种形式 阅读量统计 评论系统 访问量统计 seo设置 图床 done git总结 sql总结 shell命令总结 李宏毅DL NG，DL.AI"},{"title":"小黑板","date":"2018-09-10T07:05:02.669Z","updated":"2018-09-10T07:05:02.669Z","comments":true,"path":"board/index.html","permalink":"http://fangzh.top/board/index.html","excerpt":"","text":"小伙伴们敬请留言吧！"}],"posts":[{"title":"DeepLearning.ai作业:(4-4)-- 特殊应用:人脸识别和神经风格转换","slug":"dl-ai-4-4h","date":"2018-10-12T10:55:20.000Z","updated":"2018-10-12T12:44:11.843Z","comments":true,"path":"2018/dl-ai-4-4h/","link":"","permalink":"http://fangzh.top/2018/dl-ai-4-4h/","excerpt":"","text":"本周作业分为了两个部分: 人脸识别 风格迁移 Part1：人脸识别训练FaceNet很不现实，所以模型已经都训练好了，我们只是学习一下loss函数，然后调用模型来进行简单的识别而已。 先计算triplet_loss函数，分为4步： 1234567891011121314151617181920212223242526272829303132# GRADED FUNCTION: triplet_lossdef triplet_loss(y_true, y_pred, alpha = 0.2): \"\"\" Implementation of the triplet loss as defined by formula (3) Arguments: y_true -- true labels, required when you define a loss in Keras, you don't need it in this function. y_pred -- python list containing three objects: anchor -- the encodings for the anchor images, of shape (None, 128) positive -- the encodings for the positive images, of shape (None, 128) negative -- the encodings for the negative images, of shape (None, 128) Returns: loss -- real number, value of the loss \"\"\" anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2] ### START CODE HERE ### (≈ 4 lines) # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1 pos_dist = tf.reduce_sum(tf.square(anchor - positive),axis=-1) # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1 neg_dist = tf.reduce_sum(tf.square(anchor - negative),axis=-1) # Step 3: subtract the two previous distances and add alpha. basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha) # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples. loss = tf.reduce_sum(tf.maximum(basic_loss, 0.)) ### END CODE HERE ### return loss 进行单个人脸验证： 123456789101112131415161718192021222324252627282930313233343536# GRADED FUNCTION: verifydef verify(image_path, identity, database, model): \"\"\" Function that verifies if the person on the \"image_path\" image is \"identity\". Arguments: image_path -- path to an image identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house. database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors). model -- your Inception model instance in Keras Returns: dist -- distance between the image_path and the image of \"identity\" in the database. door_open -- True, if the door should open. False otherwise. \"\"\" ### START CODE HERE ### # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line) encoding = img_to_encoding(image_path,model) # Step 2: Compute distance with identity's image (≈ 1 line) dist = np.linalg.norm(encoding-database[identity]) # Step 3: Open the door if dist &lt; 0.7, else don't open (≈ 3 lines) if dist &lt; 0.7: print(\"It's \" + str(identity) + \", welcome home!\") door_open = True else: print(\"It's not \" + str(identity) + \", please go away\") door_open = False ### END CODE HERE ### return dist, door_open 进行人脸识别： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: who_is_itdef who_is_it(image_path, database, model): \"\"\" Implements face recognition for the happy house by finding who is the person on the image_path image. Arguments: image_path -- path to an image database -- database containing image encodings along with the name of the person on the image model -- your Inception model instance in Keras Returns: min_dist -- the minimum distance between image_path encoding and the encodings from the database identity -- string, the name prediction for the person on image_path \"\"\" ### START CODE HERE ### ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line) encoding = img_to_encoding(image_path,model) ## Step 2: Find the closest encoding ## # Initialize \"min_dist\" to a large value, say 100 (≈1 line) min_dist = 100 # Loop over the database dictionary's names and encodings. for (name, db_enc) in database.items(): # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (≈ 1 line) dist = np.linalg.norm(encoding-database[name]) # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines) if dist &lt; min_dist: min_dist = dist identity = name ### END CODE HERE ### if min_dist &gt; 0.7: print(\"Not in the database.\") else: print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist)) return min_dist, identity Part2：风格迁移模型也都是训练好的了，用的是VGG-19的网络。这里只是体验一下cost function的实现罢了。 计算J_content(C,G) $$J_{content}(C,G) = \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2 $$ 在这过程中需要把三维的矩阵先展开成2维的矩阵进行计算（虽然不展开也是可以计算的，但是风格损失函数需要计算） 1234567891011121314151617181920212223242526def compute_content_cost(a_C, a_G): \"\"\" Computes the content cost Arguments: a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G Returns: J_content -- scalar that you compute using equation 1 above. \"\"\" ### START CODE HERE ### # Retrieve dimensions from a_G (≈1 line) m, n_H, n_W, n_C = a_G.get_shape().as_list() # Reshape a_C and a_G (≈2 lines) a_C_unrolled = tf.reshape(a_C,[n_H * n_W, n_C]) a_G_unrolled = tf.reshape(a_G,[n_H * n_W, n_C]) # compute the cost with tensorflow (≈1 line) J_content = tf.reduce_sum(tf.square(a_C_unrolled - a_G_unrolled)) / (n_H * n_W * n_C * 4) ### END CODE HERE ### return J_content 计算J_style(S,G) 需要把三维矩阵展开，然后转置，做矩阵乘法，才能得到相关系数矩阵 12345678910111213141516# GRADED FUNCTION: gram_matrixdef gram_matrix(A): \"\"\" Argument: A -- matrix of shape (n_C, n_H*n_W) Returns: GA -- Gram matrix of A, of shape (n_C, n_C) \"\"\" ### START CODE HERE ### (≈1 line) GA = tf.matmul(A,tf.transpose(A)) ### END CODE HERE ### return GA $$J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times n_{C}^{2} \\times (n_H \\times n_W)^2} \\sum_{i=1}^{n_C} \\sum_{j=1}^{n_C} (G^{(S)}_{ij} - G^{(G)} _ {ij})^{2} $$ 123456789101112131415161718192021222324252627282930# GRADED FUNCTION: compute_layer_style_costdef compute_layer_style_cost(a_S, a_G): \"\"\" Arguments: a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G Returns: J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2) \"\"\" ### START CODE HERE ### # Retrieve dimensions from a_G (≈1 line) m, n_H, n_W, n_C = a_G.get_shape().as_list() # Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines) a_S = tf.transpose(tf.reshape(a_S,[n_H*n_W, n_C])) a_G = tf.transpose(tf.reshape(a_G,[n_H*n_W, n_C])) # Computing gram_matrices for both images S and G (≈2 lines) GS = gram_matrix(a_S) GG = gram_matrix(a_G) # Computing the loss (≈1 line) J_style_layer = 1 / (4 * (n_C*n_W*n_H)**2) * tf.reduce_sum(tf.square(tf.subtract(GS,GG))) ### END CODE HERE ### return J_style_layer 123456789101112131415161718192021# GRADED FUNCTION: total_costdef total_cost(J_content, J_style, alpha = 10, beta = 40): \"\"\" Computes the total cost function Arguments: J_content -- content cost coded above J_style -- style cost coded above alpha -- hyperparameter weighting the importance of the content cost beta -- hyperparameter weighting the importance of the style cost Returns: J -- total cost as defined by the formula above. \"\"\" ### START CODE HERE ### (≈1 line) J = alpha * J_content + beta * J_style ### END CODE HERE ### return J 123### START CODE HERE ### (1 line)J = total_cost(J_content, J_style, alpha = 10, beta = 40)### END CODE HERE ### 123456789101112131415161718192021222324252627282930313233343536373839def model_nn(sess, input_image, num_iterations = 200): # Initialize global variables (you need to run the session on the initializer) ### START CODE HERE ### (1 line) sess.run(tf.global_variables_initializer()) ### END CODE HERE ### # Run the noisy input image (initial generated image) through the model. Use assign(). ### START CODE HERE ### (1 line) generated_image = sess.run(model['input'].assign(input_image)) ### END CODE HERE ### for i in range(num_iterations): # Run the session on the train_step to minimize the total cost ### START CODE HERE ### (1 line) sess.run(train_step) ### END CODE HERE ### # Compute the generated image by running the session on the current model['input'] ### START CODE HERE ### (1 line) generated_image = sess.run(model['input']) ### END CODE HERE ### # Print every 20 iteration. if i%20 == 0: Jt, Jc, Js = sess.run([J, J_content, J_style]) print(\"Iteration \" + str(i) + \" :\") print(\"total cost = \" + str(Jt)) print(\"content cost = \" + str(Jc)) print(\"style cost = \" + str(Js)) # save current generated image in the \"/output\" directory save_image(\"output/\" + str(i) + \".png\", generated_image) # save last generated image save_image('output/generated_image.jpg', generated_image) return generated_image","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"},{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(4-3)-- 目标检测（Object detection）","slug":"dl-ai-4-3h","date":"2018-10-11T12:15:58.000Z","updated":"2018-10-12T02:12:56.964Z","comments":true,"path":"2018/dl-ai-4-3h/","link":"","permalink":"http://fangzh.top/2018/dl-ai-4-3h/","excerpt":"","text":"本周的作业实现了YOLO算法，并用于自动驾驶的目标检测中。 Model details输入： (m, 608, 608, 3) 输出： (m, 19, 19, 5, 85) IMAGE (m, 608, 608, 3) -&gt; DEEP CNN -&gt; ENCODING (m, 19, 19, 5, 85) 也就是有5个Anchor boxes，一共有80个分类。 所以，每个box的scores也就是等于每个类预测的可能性： Filtering with a threshold on class scores这个时候开始创建一个函数，得到每一个box中scores最大的那个类，分数，以及位置，去掉其他没用的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: yolo_filter_boxesdef yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6): \"\"\"Filters YOLO boxes by thresholding on object and class confidence. Arguments: box_confidence -- tensor of shape (19, 19, 5, 1) boxes -- tensor of shape (19, 19, 5, 4) box_class_probs -- tensor of shape (19, 19, 5, 80) threshold -- real value, if [ highest class probability score &lt; threshold], then get rid of the corresponding box Returns: scores -- tensor of shape (None,), containing the class probability score for selected boxes boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes Note: \"None\" is here because you don't know the exact number of selected boxes, as it depends on the threshold. For example, the actual output size of scores would be (10,) if there are 10 boxes. \"\"\" # Step 1: Compute box scores ### START CODE HERE ### (≈ 1 line) box_scores = box_confidence * box_class_probs ### END CODE HERE ### # Step 2: Find the box_classes thanks to the max box_scores, keep track of the corresponding score ### START CODE HERE ### (≈ 2 lines) box_classes = K.argmax(box_scores, axis=-1) #得到box的类别 (19,19,5) box_class_scores = K.max(box_scores, axis=-1, keepdims=False) #得到box这个类别的分数(19,19,5) ### END CODE HERE ### # Step 3: Create a filtering mask based on \"box_class_scores\" by using \"threshold\". The mask should have the # same dimension as box_class_scores, and be True for the boxes you want to keep (with probability &gt;= threshold) ### START CODE HERE ### (≈ 1 line) filtering_mask = box_class_scores &gt;= threshold ### END CODE HERE ### # Step 4: Apply the mask to scores, boxes and classes ### START CODE HERE ### (≈ 3 lines) scores = tf.boolean_mask(box_class_scores, filtering_mask) boxes = tf.boolean_mask(boxes, filtering_mask) classes = tf.boolean_mask(box_classes, filtering_mask) ### END CODE HERE ### return scores, boxes, classes Non-max suppression找到了这些boxes后，还需要进行筛选过滤掉。先完成一个IOU算法： 1234567891011121314151617181920212223242526272829303132# GRADED FUNCTION: ioudef iou(box1, box2): \"\"\"Implement the intersection over union (IoU) between box1 and box2 Arguments: box1 -- first box, list object with coordinates (x1, y1, x2, y2) box2 -- second box, list object with coordinates (x1, y1, x2, y2) \"\"\" # Calculate the (y1, x1, y2, x2) coordinates of the intersection of box1 and box2. Calculate its Area. ### START CODE HERE ### (≈ 5 lines) xi1 = np.maximum(box1[0], box2[0]) yi1 = np.maximum(box1[1], box2[1]) xi2 = np.minimum(box1[2], box2[2]) yi2 = np.minimum(box1[3], box2[3]) inter_area = max(xi2 - xi1,0) * max(yi2 - yi1,0) ### END CODE HERE ### # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B) ### START CODE HERE ### (≈ 3 lines) box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1]) box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1]) union_area = box1_area + box2_area - inter_area ### END CODE HERE ### # compute the IoU ### START CODE HERE ### (≈ 1 line) iou = inter_area / union_area ### END CODE HERE ### return iou tensorflow已经帮你实现了iou算法了，不用用自己刚才写的了： tf.image.non_max_suppression() K.gather() 思想就是拿掉IOU比较大的那些box 1234567891011121314151617181920212223242526272829303132333435363738# GRADED FUNCTION: yolo_non_max_suppressiondef yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5): \"\"\" Applies Non-max suppression (NMS) to set of boxes Arguments: scores -- tensor of shape (None,), output of yolo_filter_boxes() boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later) classes -- tensor of shape (None,), output of yolo_filter_boxes() max_boxes -- integer, maximum number of predicted boxes you'd like iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering Returns: scores -- tensor of shape (, None), predicted score for each box boxes -- tensor of shape (4, None), predicted box coordinates classes -- tensor of shape (, None), predicted class for each box Note: The \"None\" dimension of the output tensors has obviously to be less than max_boxes. Note also that this function will transpose the shapes of scores, boxes, classes. This is made for convenience. \"\"\" max_boxes_tensor = K.variable(max_boxes, dtype='int32') # tensor to be used in tf.image.non_max_suppression() K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep ### START CODE HERE ### (≈ 1 line) nms_indices = tf.image.non_max_suppression(boxes,scores,max_boxes,iou_threshold) ### END CODE HERE ### # Use K.gather() to select only nms_indices from scores, boxes and classes ### START CODE HERE ### (≈ 3 lines) scores = K.gather(scores,nms_indices) boxes = K.gather(boxes,nms_indices) classes = K.gather(classes,nms_indices) ### END CODE HERE ### return scores, boxes, classes 而后结合刚才的函数，先去掉scores低的，然后运算NMS算法 12345678910111213141516171819202122232425262728293031323334353637383940414243# GRADED FUNCTION: yolo_evaldef yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5): \"\"\" Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes. Arguments: yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors: box_confidence: tensor of shape (None, 19, 19, 5, 1) box_xy: tensor of shape (None, 19, 19, 5, 2) box_wh: tensor of shape (None, 19, 19, 5, 2) box_class_probs: tensor of shape (None, 19, 19, 5, 80) image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype) max_boxes -- integer, maximum number of predicted boxes you'd like score_threshold -- real value, if [ highest class probability score &lt; threshold], then get rid of the corresponding box iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering Returns: scores -- tensor of shape (None, ), predicted score for each box boxes -- tensor of shape (None, 4), predicted box coordinates classes -- tensor of shape (None,), predicted class for each box \"\"\" ### START CODE HERE ### # Retrieve outputs of the YOLO model (≈1 line) box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs # Convert boxes to be ready for filtering functions boxes = yolo_boxes_to_corners(box_xy, box_wh) # Use one of the functions you've implemented to perform Score-filtering with a threshold of score_threshold (≈1 line) scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, score_threshold) # Scale boxes back to original image shape. boxes = scale_boxes(boxes, image_shape) # Use one of the functions you've implemented to perform Non-max suppression with a threshold of iou_threshold (≈1 line) scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold ) ### END CODE HERE ### return scores, boxes, classes 进行预测: 1234567891011121314151617181920212223242526272829303132333435363738def predict(sess, image_file): \"\"\" Runs the graph stored in \"sess\" to predict boxes for \"image_file\". Prints and plots the preditions. Arguments: sess -- your tensorflow/Keras session containing the YOLO graph image_file -- name of an image stored in the \"images\" folder. Returns: out_scores -- tensor of shape (None, ), scores of the predicted boxes out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes out_classes -- tensor of shape (None, ), class index of the predicted boxes Note: \"None\" actually represents the number of predicted boxes, it varies between 0 and max_boxes. \"\"\" # Preprocess your image image, image_data = preprocess_image(\"images/\" + image_file, model_image_size = (608, 608)) # Run the session with the correct tensors and choose the correct placeholders in the feed_dict. # You'll need to use feed_dict=&#123;yolo_model.input: ... , K.learning_phase(): 0&#125;) ### START CODE HERE ### (≈ 1 line) out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes], feed_dict = &#123;yolo_model.input:image_data, K.learning_phase(): 0&#125;) ### END CODE HERE ### # Print predictions info print('Found &#123;&#125; boxes for &#123;&#125;'.format(len(out_boxes), image_file)) # Generate colors for drawing bounding boxes. colors = generate_colors(class_names) # Draw bounding boxes on the image file draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors) # Save the predicted bounding box on the image image.save(os.path.join(\"out\", image_file), quality=90) # Display the results in the notebook output_image = scipy.misc.imread(os.path.join(\"out\", image_file)) imshow(output_image) return out_scores, out_boxes, out_classes","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"},{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai笔记:(4-3)-- 目标检测（Object detection）","slug":"dl-ai-4-3","date":"2018-10-11T09:02:09.000Z","updated":"2018-10-11T13:00:12.827Z","comments":true,"path":"2018/dl-ai-4-3/","link":"","permalink":"http://fangzh.top/2018/dl-ai-4-3/","excerpt":"这一周主要讲了卷积神经网络的进一步应用：目标检测。 主要内容有：目标定位、特征点检测、目标检测、滑动窗口、Bounding Box，IOU，NMS，Anchor Boxes，Yolo算法。","text":"这一周主要讲了卷积神经网络的进一步应用：目标检测。 主要内容有：目标定位、特征点检测、目标检测、滑动窗口、Bounding Box，IOU，NMS，Anchor Boxes，Yolo算法。 目标定位（Object localization）在进行目标检测之前，需要先了解一下目标定位。 因为进行目标检测的时候需要预测出目标的具体位置，所以在训练的时候需要先标定一下这个目标的实际位置。 假设我们需要分类的类别有3个，行人，汽车，自行车，如果什么都没有，那么就是背景。可以看到，y一共有8个参数： $P_c$：是否有目标 $b_x,b_y,b_h,b_w$：目标的位置x,y，高宽h,w $c_1,c_2,c_3$：行人、汽车、自行车 如果$P_c = 0$那么表示没有目标，那么我们就不关心后面的其他参数了。 特征点检测(Landmark detection) 如果是要检测人脸，那么可以在人脸上标注若干个特征点，假设有64个特征点，那么这个时候就有128个参数了，再加上判断是否有人脸，就有129个参数。 假设要检测的是人体肢体的动作，那么同样也可以标注若干个肢体上的特征点。 注意，这些都是需要人工进行标注的。 目标检测滑动窗口 目标检测通常采用的是滑动窗口的方法来检测的。也就是用一定窗口的大小，按照指定的步长，遍历整个图像；而后再选取更大的窗口，再次遍历，依次循环。这样子，每个窗口都相当于一张小图片，对这个小图片进行图像识别，从而得到预测结果。 但是这个方法有个很明显的问题，就是每个窗口都要进行一次图像识别，速度太慢了。因此就有了滑动窗口的卷积实现。在此之前，我们需要知道如何把全连接层变为卷积层。 全连接层转化为卷积层 如图，在经过Max pool后，我们得到了$5 \\times 5 \\times 16$的图像，经过第一个FC层后变成了400个节点。 而此时我们可以使用400个$5\\times5$的卷积核，进行卷积，得到了$1\\times1\\times400$ 而后再使用400个$1\\times1$的卷积核，再得到了$1\\times1\\times400$矩阵，所以我们就将全连接层转化成了卷积层。 卷积滑动窗口的实现 因为之前的滑动窗口每一次都要进行一次计算，太慢了。所以利用上面的全连接层转化为卷积层的做法，可以一次性把滑动窗口的结果都计算出来。 为了方面观察，这里把三维图像画成了平面。 假设滑动的窗口是$14\\times14\\times3$，原图像大小是$16\\times\\times16\\times3$。 蓝色表示滑动窗口，如果步数是2的话，很容易可以得到$2\\times2$的图像，不难看出，在图中最后输出的左上角的蓝色部分就是原图中蓝色部分的计算结果，以此类推。 也就是说，只需要原图进行一次运算，就可以一次性得到多个滑动窗口的输出值。 具体例子如下图： 可以看到，原图为$28\\times28$，最后得到了$8\\times8 = 64$个滑动窗口。 Bounding Box上面介绍的滑动窗口的方法有一个问题，就是很多情况下并不能检测出窗口的精确位置。 那么如何找到这个准确的边界框呢？有一个很快的算法叫做YOLO(you only look once)，只需要计算一次便可以检测出物体的位置。 如图，首先，将图片分为$n \\times n$个部分，如图是划分成了$3\\times3=9$份，而每一份都由一个向量y来表示。 因此最终得到了$3\\times3\\times8$的矩阵。 要得到这个$3\\times3\\times8$的矩阵，只要选择适当的卷积神经网络，让输出矩阵为这个矩阵就行，而每一个小图像都有一个目标标签y，这个时候y中的$b_x,b_y$都是这个小图像的相对位置，在0-1之间，而$b_h,b_w$是可以大于1的，因为整个大目标有可能在框框外。 在实际过程中可以选用更精细的划分，如$19\\times19$。 交并比(Intersection over Union, IoU)如何判断框框是否正确呢？ 如图红色为车身的框，而紫色为检测到的框，那么紫色的框到底算不算有车呢？ 这个时候可以用交并比来判断，也就是两个框框的交集和并集之比。 $$IoU = \\frac{交集面积}{并集面积}$$ 一般来说 $IoU \\geq 0.5$，那么说明检测正确，当然，这个阈值可以自己设定。 非最大值抑制（NMS）在实际过程中，很可能很多个框框都检测出同一个物体，那么如何判断这些边界框检测的是同一个对象呢？ 首先，每一个框都会返回一个概率$P_c$，我们需要先去掉那些概率比较低的框，如去掉$P_c \\leq 0.55$的框。 而后，在$P_c$中找到概率最大的框，然后用算法遍历其他的边框，找出并取消掉和这个边框IoU大于0.5的框（因为如果IoU大于0.5，我们可以认为是同一个物体） 循环第二步的操作 如果有多个目标类别的检测，那么对每个类别分别进行上面的NMS算法。 Anchor Box如果一张格子中有多个目标，那怎么办？这时候就需要Anchor Box了，可以同时检测出多个对象。 我们预先定义了两个不同形状的Anchor box，如比较高的来检测人，比较宽的来检测汽车，然后重新定义了目标向量y： 这个时候最后输出的矩阵从原来的$3\\times3\\times8$变成了$3\\times3\\times16$，也可以是$3\\times3\\times2\\times8$ 在计算的时候就可以根据不同的box输出了，？号表示我们不关系这个值。 问题： 如果使用两个Box，那么如果出现3个目标怎么办，这时候需要别的手段了 如果同一个格子的两个对象的box相同怎么办，那也需要别的手段来处理了。 因为这两种情况出现的几率都比较少，所以问题不大。 注意： Anchor box的形状都是人工指定的，一般可以选择5-10种不同的形状，来涵盖我们想要检测的不同对象 更高级一点的使用k-means聚类算法，将不同的对象形状进行聚类，然后得到一组比较具有代表性的boxes YOLO算法假设我们需要检测三种目标：行人、汽车、摩托车，使用两种anchor box 在训练集中： 输入同样大小的图片X 每张图片的输出Y是$3\\times3\\times16$的矩阵 人工标定输出Y 预测： 输入图片和训练集大小相同，得到$3\\times3\\times16$的输出结果 这个时候得到了很多个框框，如果是两个Anchor box，那么就有$2\\times9=18$个预测框框，那么就需要把没用的框框都去掉，也就用到了上面的NMS非最大值抑制算法。 进行NMS算法： 去掉$P_c$小于某个阈值的框框 对于每个对象分别使用NMS算法得到最终的边界框。 候选区域这里还介绍了其他的目标检测算法，不过貌似都是比较慢的。 R-CNN： 原本的滑动窗口，只有在少部分的区域是可以检测到目标的，很多区域都是背景，所以运算很慢，用R-CNN后，只选择一些候选的窗口，不需要对整个图片进行滑动。 R-CNN使用的是图像分割算法，将图片分割成很多个色块，从而减少了窗口数量。 是对每个候选区域进行分类，输出的标签和bounding box Fast R-CNN： 候选区域，使用滑动窗口在区分所有的候选区域。 Faster R-CNN： 使用卷积神经网络而不是图像分割来获得候选区域。","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(4-2)-- 深度卷积网络实例探究（Deep convolutional models:case studies）","slug":"dl-ai-4-2h","date":"2018-10-09T11:20:57.000Z","updated":"2018-10-09T11:49:10.706Z","comments":true,"path":"2018/dl-ai-4-2h/","link":"","permalink":"http://fangzh.top/2018/dl-ai-4-2h/","excerpt":"本周作业分为两部分，一部分是keras的基本使用，另一部分是ResNet的构建。","text":"本周作业分为两部分，一部分是keras的基本使用，另一部分是ResNet的构建。 Part1: Keras – TutorialKeras是TensorFlow的高层封装，可以更高效的实现神经网络的搭建。 先导入库 123456789101112131415161718192021import numpy as npfrom keras import layersfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2Dfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2Dfrom keras.models import Modelfrom keras.preprocessing import imagefrom keras.utils import layer_utilsfrom keras.utils.data_utils import get_filefrom keras.applications.imagenet_utils import preprocess_inputimport pydotfrom IPython.display import SVGfrom keras.utils.vis_utils import model_to_dotfrom keras.utils import plot_modelfrom kt_utils import *import keras.backend as KK.set_image_data_format('channels_last')import matplotlib.pyplot as pltfrom matplotlib.pyplot import imshow%matplotlib inline 构建模型 1234567891011121314151617181920212223242526272829def HappyModel(input_shape): \"\"\" Implementation of the HappyModel. Arguments: input_shape -- shape of the images of the dataset Returns: model -- a Model() instance in Keras \"\"\" ### START CODE HERE ### # Feel free to use the suggested outline in the text above to get started, and run through the whole # exercise (including the later portions of this notebook) once. The come back also try out other # network architectures as well. X_input = Input(input_shape) X = ZeroPadding2D((3, 3))(X_input) X = Conv2D(32,(7,7),strides=(1,1),name=\"Conv0\")(X) X = BatchNormalization(axis = 3, name = 'bn0')(X) X = Activation('relu')(X) X = MaxPooling2D((2, 2), name='max_pool')(X) X = Flatten()(X) X = Dense(1, activation='sigmoid', name='fc')(X) model = Model(inputs = X_input, outputs = X, name='HappyModel') ### END CODE HERE ### return model 然后实例化这个模型 123### START CODE HERE ### (1 line)happyModel = HappyModel(X_train.shape[1:])### END CODE HERE ### 进行优化器和loss的选择 123### START CODE HERE ### (1 line)happyModel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])### END CODE HERE ### 训练 123### START CODE HERE ### (1 line)happyModel.fit(x=X_train,y = Y_train,epochs=10,batch_size=32)### END CODE HERE ### 预测： 123456### START CODE HERE ### (1 line)preds = happyModel.evaluate(X_test,Y_test)### END CODE HERE ###print()print (\"Loss = \" + str(preds[0]))print (\"Test Accuracy = \" + str(preds[1])) 可以用summary()来看看详细信息： 1happyModel.summary() 1234567891011121314151617181920212223_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_1 (InputLayer) (None, 64, 64, 3) 0 _________________________________________________________________zero_padding2d_1 (ZeroPaddin (None, 70, 70, 3) 0 _________________________________________________________________Conv0 (Conv2D) (None, 64, 64, 32) 4736 _________________________________________________________________bn0 (BatchNormalization) (None, 64, 64, 32) 128 _________________________________________________________________activation_1 (Activation) (None, 64, 64, 32) 0 _________________________________________________________________max_pool (MaxPooling2D) (None, 32, 32, 32) 0 _________________________________________________________________flatten_1 (Flatten) (None, 32768) 0 _________________________________________________________________fc (Dense) (None, 1) 32769 =================================================================Total params: 37,633Trainable params: 37,569Non-trainable params: 64_________________________________________________________________ 用plot_model()来得到详细的graph 12plot_model(happyModel, to_file='HappyModel.png')SVG(model_to_dot(happyModel).create(prog='dot', format='svg')) Part2: Residual Networks主要有两个步骤： 构建基本的ResNet的块 将块放到一起，变成一个网络，来做图像分类 1 - The problem of very deep neural networks这一部分非常深的神经网络的一些问题，主要是参数会变得很小或者爆炸，这样子训练的时候就会收敛的很慢，因此，用残差网络可以有效的改善这个问题。 2 - Building a Residual Network 根据输入输入的维度不同，分为两种块： 1. identity block（一致块） 可以看到，identity block的前后两端维度是一致的，可以直接相加。 在这里我们实现了一个跳跃三层的块。 基本结构是: First component of main path: The first CONV2D has F1F1 filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be conv_name_base + &#39;2a&#39;. Use 0 as the seed for the random initialization. The first BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2a&#39;. Then apply the ReLU activation function. This has no name and no hyperparameters. Second component of main path: The second CONV2D has F2F2 filters of shape (f,f)(f,f) and a stride of (1,1). Its padding is “same” and its name should be conv_name_base + &#39;2b&#39;. Use 0 as the seed for the random initialization. The second BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2b&#39;. Then apply the ReLU activation function. This has no name and no hyperparameters. Third component of main path: The third CONV2D has F3F3 filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be conv_name_base + &#39;2c&#39;. Use 0 as the seed for the random initialization. The third BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2c&#39;. Note that there is no ReLU activation function in this component. Final step: The shortcut and the input are added together. Then apply the ReLU activation function. This has no name and no hyperparameters. 注意在跳跃相加部分要用函数keras的函授Add()，不能用加号，不然会出错。 这里f是卷积核的大小，filters是这三层卷积层的深度的list，stage指的是哪一大层的网络，用来取名字的，后面有用，block是在stage下的某一层的网络，用a,b,c,d等字母表示。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# GRADED FUNCTION: identity_blockdef identity_block(X, f, filters, stage, block): \"\"\" Implementation of the identity block as defined in Figure 3 Arguments: X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev) f -- integer, specifying the shape of the middle CONV's window for the main path filters -- python list of integers, defining the number of filters in the CONV layers of the main path stage -- integer, used to name the layers, depending on their position in the network block -- string/character, used to name the layers, depending on their position in the network Returns: X -- output of the identity block, tensor of shape (n_H, n_W, n_C) \"\"\" # defining name basis conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' # Retrieve Filters F1, F2, F3 = filters # Save the input value. You'll need this later to add back to the main path. X_shortcut = X # First component of main path X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X) X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X) X = Activation('relu')(X) ### START CODE HERE ### # Second component of main path (≈3 lines) X = Conv2D(filters = F2, kernel_size = (f, f), strides= (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X) X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X) X = Activation('relu')(X) # Third component of main path (≈2 lines) X = Conv2D(filters = F3, kernel_size = (1, 1), strides= (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X) X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X) # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines) X = Add()([X,X_shortcut]) X = Activation('relu')(X) ### END CODE HERE ### return X 2. The convolutional block(卷积块) 当两端的维度不一致时，可以加一个卷积核来转化维度，这时候没有激活函数。 First component of main path: The first CONV2D has F1F1 filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be conv_name_base + &#39;2a&#39;. The first BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2a&#39;. Then apply the ReLU activation function. This has no name and no hyperparameters. Second component of main path: The second CONV2D has F2F2 filters of (f,f) and a stride of (1,1). Its padding is “same” and it’s name should be conv_name_base + &#39;2b&#39;. The second BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2b&#39;. Then apply the ReLU activation function. This has no name and no hyperparameters. Third component of main path: The third CONV2D has F3F3 filters of (1,1) and a stride of (1,1). Its padding is “valid” and it’s name should be conv_name_base + &#39;2c&#39;. The third BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2c&#39;. Note that there is no ReLU activation function in this component. Shortcut path: The CONV2D has F3F3 filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be conv_name_base + &#39;1&#39;. The BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;1&#39;. Final step: The shortcut and the main path values are added together. Then apply the ReLU activation function. This has no name and no hyperparameters. 这里参数新增了s是stride每一步数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def convolutional_block(X, f, filters, stage, block, s = 2): \"\"\" Implementation of the convolutional block as defined in Figure 4 Arguments: X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev) f -- integer, specifying the shape of the middle CONV's window for the main path filters -- python list of integers, defining the number of filters in the CONV layers of the main path stage -- integer, used to name the layers, depending on their position in the network block -- string/character, used to name the layers, depending on their position in the network s -- Integer, specifying the stride to be used Returns: X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C) \"\"\" # defining name basis conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' # Retrieve Filters F1, F2, F3 = filters # Save the input value X_shortcut = X ##### MAIN PATH ##### # First component of main path X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X) X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X) X = Activation('relu')(X) ### START CODE HERE ### # Second component of main path (≈3 lines) X = Conv2D(F2, (f, f), strides = (1,1), name = conv_name_base + '2b', padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X) X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X) X = Activation('relu')(X) # Third component of main path (≈2 lines) X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c', padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X) X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X) ##### SHORTCUT PATH #### (≈2 lines) X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X_shortcut) X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut) # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines) X = Add()([X,X_shortcut]) X = Activation('relu')(X) ### END CODE HERE ### return X 3 - Building your first ResNet model (50 layers)构建一个50层的网络，分为5块，结构如下： The details of this ResNet-50 model are: Zero-padding pads the input with a pad of (3,3) Stage 1: The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is “conv1”. BatchNorm is applied to the channels axis of the input. MaxPooling uses a (3,3) window and a (2,2) stride. Stage 2: The convolutional block uses three set of filters of size [64,64,256], “f” is 3, “s” is 1 and the block is “a”. The 2 identity blocks use three set of filters of size [64,64,256], “f” is 3 and the blocks are “b” and “c”. Stage 3: The convolutional block uses three set of filters of size [128,128,512], “f” is 3, “s” is 2 and the block is “a”. The 3 identity blocks use three set of filters of size [128,128,512], “f” is 3 and the blocks are “b”, “c” and “d”. Stage 4: The convolutional block uses three set of filters of size [256, 256, 1024], “f” is 3, “s” is 2 and the block is “a”. The 5 identity blocks use three set of filters of size [256, 256, 1024], “f” is 3 and the blocks are “b”, “c”, “d”, “e” and “f”. Stage 5: The convolutional block uses three set of filters of size [512, 512, 2048], “f” is 3, “s” is 2 and the block is “a”. The 2 identity blocks use three set of filters of size [512, 512, 2048], “f” is 3 and the blocks are “b” and “c”. The 2D Average Pooling uses a window of shape (2,2) and its name is “avg_pool”. The flatten doesn’t have any hyperparameters or name. The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be &#39;fc&#39; + str(classes). Exercise: Implement the ResNet with 50 layers described in the figure above. We have implemented Stages 1 and 2. Please implement the rest. (The syntax for implementing Stages 3-5 should be quite similar to that of Stage 2.) Make sure you follow the naming convention in the text above. You’ll need to use this function: Average pooling see reference 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# GRADED FUNCTION: ResNet50def ResNet50(input_shape = (64, 64, 3), classes = 6): \"\"\" Implementation of the popular ResNet50 the following architecture: CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3 -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER Arguments: input_shape -- shape of the images of the dataset classes -- integer, number of classes Returns: model -- a Model() instance in Keras \"\"\" # Define the input as a tensor with shape input_shape X_input = Input(input_shape) # Zero-Padding X = ZeroPadding2D((3, 3))(X_input) # Stage 1 X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X) X = BatchNormalization(axis = 3, name = 'bn_conv1')(X) X = Activation('relu')(X) X = MaxPooling2D((3, 3), strides=(2, 2))(X) # Stage 2 X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1) X = identity_block(X, 3, [64, 64, 256], stage=2, block='b') X = identity_block(X, 3, [64, 64, 256], stage=2, block='c') ### START CODE HERE ### # Stage 3 (≈4 lines) X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2) X = identity_block(X, 3, [128, 128, 512], stage=3, block='b') X = identity_block(X, 3, [128, 128, 512], stage=3, block='c') X = identity_block(X, 3, [128, 128, 512], stage=3, block='d') # Stage 4 (≈6 lines) X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2) X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b') X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c') X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d') X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e') X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f') # Stage 5 (≈3 lines) X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2) X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b') X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c') # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\" X = AveragePooling2D(pool_size=(2,2),strides=(1,1),padding='valid')(X) ### END CODE HERE ### # output layer X = Flatten()(X) X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X) # Create model model = Model(inputs = X_input, outputs = X, name='ResNet50') return model","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"},{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai笔记:(4-2)-- 深度卷积网络实例探究（Deep convolutional models:case studies）","slug":"dl-ai-4-2","date":"2018-10-09T09:17:04.000Z","updated":"2018-10-09T11:49:06.379Z","comments":true,"path":"2018/dl-ai-4-2/","link":"","permalink":"http://fangzh.top/2018/dl-ai-4-2/","excerpt":"本周主要讲了深度卷积网络的一些模型：LeNet,AlexNet,VGGNet,ResNet,Inception,1×1卷积，迁移学习等。","text":"本周主要讲了深度卷积网络的一些模型：LeNet,AlexNet,VGGNet,ResNet,Inception,1×1卷积，迁移学习等。 经典的卷积网络经典的卷及网络有三种：LeNet、AlexNet、VGGNet。 LeNet-5 LeNet-5主要是单通道的手写字体的识别，这是80年代提出的算法，当时没有用padding，而且pooling用的是average pooling，但是现在大家都用max pooling了。 论文中的最后预测用的是sigmoid和tanh，而现在都用了softmax。 AlexNet AlexNet是2012年提出的算法。用来对彩色的图片进行处理，其实大致的结构和LeNet-5是很相似的，但是网络更大，参数更多了。 这个时候已经用Relu来作为激活函数了，而且用了多GPU进行计算。 VGG-16 VGG-16是2015的论文，比较简化的是，卷积层和池化层都是用相同的卷积核大小，卷积核都是3×3，stride=1，same padding，池化层用的maxpooling，为2×2，stride=2。只是在卷积的时候改变了每一层的通道数。 网络很大，参数有1.38亿个参数。 建议阅读论文顺序：AlexNet-&gt;VGG-&gt;LeNet Residual Network(残差网络)残差网络是由若干个残差块组成的。 因为在非常深的网络中会存在梯度消失和梯度爆炸的问题，为此，引入了Skip Connection来解决，也就是残差网络的实现。 上图即为一个残差块的基本原理，在原本的传播过程(称为主线)中，加上了$a^{[l]}$到$z^{[l+2]}$的连接，成为’short cut’或者’skip connetction’。 所以输出的表达式变成了:$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$ 残差网络是由多个残差块组成的： 没有残差网络和加上残差网络的效果对比，可以看到，随着layers的增加，ResNet表现的更好： ResNet为何有用？ 假设我们已经经过了一个很大的神经网络Big NN,得到了$a^{[l]}$ 那么这个时候再经过两层的神经网络得到$a^{[l+2]}$,那么表达式为： $$a^{[l+2]} = g(z^{[l+2]} + a^{[l]}) = g(W^{[l+2]} a^{[l+2]} + b^{[l+2]} + a^{[l]})$$ 如果加上正则化，那么权值就会很小，假设$W^{[l+2]},b^{[l+2]} = 0$， 因为激活函数是Relu，所以 $$a^{[l+2]} = g(a^{[l]}) = a^{[l]}$$ 所以可以看到，加上残差块以后，更深的网络最差也只是和前面的效果一样，何况还有可能更好。 如果只是普通的两层网络，那么结果可能更好，也可能更差。 注意的是$a^{[l+2]}$要和$a^{[l]}$的维度一样，可以使用same padding，来保持维度。 1×1卷积 用1×1的卷积核可以来减少通道数，从而减少参数个数。 Inception NetworkInception的主要好处就是不需要人工来选择filter的大小和是否要添加池化层的问题。 如图可以一次性把各个卷积核的大小和max pool一起加进去，然后让机器自己学习里面的参数。 但是这样有一个问题，就是计算量太大了，假设是上面的$5 \\times 5 \\times 192$的卷积核，有32个，这样一共要进行$28\\times\\28\\times32\\times5\\times5\\times192=120M$的乘法次数，运算量是很大的。 如何解决这个问题呢？就需要用到前面的1×1的卷积核了。 可以看到经过维度压缩，计算次数少了十倍。 Inception 网络单个的inception模块如下： 构成的google net如下： 使用开源的实现方案别人已经实现的网络已经很厉害了，我觉得重复造轮子很没有必要，而且浪费时间，何况你水平也没有别人高。。还不如直接用别人的网络，然后稍加改造，这样可以很快的实现你的想法。 在GitHub上找到自己感兴趣的网络结构fork过来，好好研究！ 迁移学习之前已经讲过迁移学习了，也就是用别人训练好的网络，固定他们已经训练好的网络参数，然后套到自己的训练集上，完成训练。 如果你只有很少的数据集，那么，改变已有网络的最后一层softmax就可以了，比如原来别人的模型是有1000个分类，现在你只需要有3个分类。然后freeze冻结前面隐藏层的所有参数不变。这样就好像是你自己在训练一个很浅的神经网络，把隐藏层看做一个函数来映射，只需要训练最后的softmax层就可以了。 如果你有一定量的数据，那么freeze的范围可以减少，你可以训练后面的几层隐藏层，或者自己设计后面的隐藏层。 数据扩充数据不够的话，进行数据扩充是很有用的。 可以采用 镜像 随机裁剪 色彩转换color shifting（如三通道：R+20,G-20,B+20）等等 tips: 在数据比赛中 ensembling：训练多个网络模型，然后平均结果，或者加权平均 测试时使用muti-crop，也就是在把单张测试图片用数据扩充的形式变成很多张，然后运行分类器，得到的结果进行平均。","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(4-1)-- 卷积神经网络（Foundations of CNN）","slug":"dl-ai-4-1h","date":"2018-09-30T08:07:23.000Z","updated":"2018-09-30T09:53:32.577Z","comments":true,"path":"2018/dl-ai-4-1h/","link":"","permalink":"http://fangzh.top/2018/dl-ai-4-1h/","excerpt":"本周的作业分为了两部分： 卷积神经网络的模型搭建 用TensorFlow来训练卷积神经网络","text":"本周的作业分为了两部分： 卷积神经网络的模型搭建 用TensorFlow来训练卷积神经网络 Part1：Convolutional Neural Networks: Step by Step主要内容： convolution funtions: Zero Padding Convolve window Convolution forward Convolution backward (optional) Pooling functions： Pooling forward Create mask Distribute value Pooling backward (optional) Convolutional Neural Networks创建CNN的主要函数 1. Zero Padding 先创建一个padding函数，用来输入图像X，输出padding后的图像，这里使用的是np.pad()函数， 12a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..))表示a有5个维度，在第1维的两边都填上1个pad，和第3维的两边都填上3个pad，constant_values表示两边要填充的值 12345678910111213141516171819def zero_pad(X, pad): \"\"\" Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, as illustrated in Figure 1. Argument: X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images pad -- integer, amount of padding around each image on vertical and horizontal dimensions Returns: X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C) \"\"\" ### START CODE HERE ### (≈ 1 line) X_pad = np.pad(X, ((0,0),(pad,pad),(pad,pad),(0,0)), 'constant', constant_values=(0,0)) ### END CODE HERE ### return X_pad 2.Single step of convolution 创建一个单步的卷积运算，也就是一次输入一个切片，大小和卷积核相同，对应元素相乘再求和，最后再加个bias项。 1234567891011121314151617181920212223242526# GRADED FUNCTION: conv_single_stepdef conv_single_step(a_slice_prev, W, b): \"\"\" Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation of the previous layer. Arguments: a_slice_prev -- slice of input data of shape (f, f, n_C_prev) W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev) b -- Bias parameters contained in a window - matrix of shape (1, 1, 1) Returns: Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data \"\"\" ### START CODE HERE ### (≈ 2 lines of code) # Element-wise product between a_slice and W. Do not add the bias yet. s = a_slice_prev * W # Sum over all entries of the volume s. Z = np.sum(s) # Add bias b to Z. Cast b to a float() so that Z results in a scalar value. Z = Z + float(b) ### END CODE HERE ### return Z 3.Convolutional Neural Networks - Forward pass 创建一次完整的卷积过程，也就是利用上面的一次卷积，进行for循环。进行切片的时候，注意边界vert_start, vert_end, horiz_start and horiz_end 这一步应该先弄清楚A_prev，A，W，b的维度，超参数项包括了stride和pad $$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$$$ n_C = \\text{number of filters used in the convolution}$$ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# GRADED FUNCTION: conv_forwarddef conv_forward(A_prev, W, b, hparameters): \"\"\" Implements the forward propagation for a convolution function Arguments: A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev) W -- Weights, numpy array of shape (f, f, n_C_prev, n_C) b -- Biases, numpy array of shape (1, 1, 1, n_C) hparameters -- python dictionary containing \"stride\" and \"pad\" Returns: Z -- conv output, numpy array of shape (m, n_H, n_W, n_C) cache -- cache of values needed for the conv_backward() function \"\"\" ### START CODE HERE ### # Retrieve dimensions from A_prev's shape (≈1 line) (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape # Retrieve dimensions from W's shape (≈1 line) (f, f, n_C_prev, n_C) = W.shape # Retrieve information from \"hparameters\" (≈2 lines) stride = hparameters['stride'] pad = hparameters['pad'] # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines) n_H = int((n_H_prev + 2 * pad - f) / stride + 1) n_W = int((n_W_prev + 2 * pad - f) / stride + 1) # Initialize the output volume Z with zeros. (≈1 line) Z = np.zeros((m, n_H, n_W, n_C)) # Create A_prev_pad by padding A_prev A_prev_pad = zero_pad(A_prev, pad) for i in range(m): # loop over the batch of training examples a_prev_pad = A_prev_pad[i] # Select ith training example's padded activation for h in range(n_H): # loop over vertical axis of the output volume for w in range(n_W): # loop over horizontal axis of the output volume for c in range(n_C): # loop over channels (= #filters) of the output volume # Find the corners of the current \"slice\" (≈4 lines) vert_start = h * stride vert_end = h * stride + f horiz_start = w * stride horiz_end = w * stride + f # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line) a_slice_prev = a_prev_pad[vert_start : vert_end, horiz_start : horiz_end] # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line) Z[i, h, w, c] = conv_single_step(a_slice_prev,W[:,:,:,c],b[:,:,:,c]) ### END CODE HERE ### # Making sure your output shape is correct assert(Z.shape == (m, n_H, n_W, n_C)) # Save information in \"cache\" for the backprop cache = (A_prev, W, b, hparameters) return Z, cache Pooling layer创建池化层，注意得到的维度需要向下取整，用int()对float()进行转换 $$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$$$ n_C = n_{C_{prev}}$$ 同样需要先进行切边，而后分为max和average两种，分别用np.max和np.mean 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def pool_forward(A_prev, hparameters, mode = \"max\"): \"\"\" Implements the forward pass of the pooling layer Arguments: A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev) hparameters -- python dictionary containing \"f\" and \"stride\" mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\") Returns: A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C) cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \"\"\" # Retrieve dimensions from the input shape (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape # Retrieve hyperparameters from \"hparameters\" f = hparameters[\"f\"] stride = hparameters[\"stride\"] # Define the dimensions of the output n_H = int(1 + (n_H_prev - f) / stride) n_W = int(1 + (n_W_prev - f) / stride) n_C = n_C_prev # Initialize output matrix A A = np.zeros((m, n_H, n_W, n_C)) ### START CODE HERE ### for i in range(m): # loop over the training examples for h in range(n_H): # loop on the vertical axis of the output volume for w in range(n_W): # loop on the horizontal axis of the output volume for c in range (n_C): # loop over the channels of the output volume # Find the corners of the current \"slice\" (≈4 lines) vert_start = h * stride vert_end = vert_start + f horiz_start = w * stride horiz_end = horiz_start + f # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line) a_prev_slice = A_prev[i, vert_start : vert_end, horiz_start : horiz_end, c] # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean. if mode == \"max\": A[i, h, w, c] = np.max(a_prev_slice) elif mode == \"average\": A[i, h, w, c] = np.mean(a_prev_slice) ### END CODE HERE ### # Store the input and hparameters in \"cache\" for pool_backward() cache = (A_prev, hparameters) # Making sure your output shape is correct assert(A.shape == (m, n_H, n_W, n_C)) return A, cache Backpropagation in convolutional neural networks卷积神经网络的求导是比较难以理解的，这里有卷积层的求导和池化层的求导。 1.Convolutional layer backward pass假设经过卷积层后我们的输出$Z = W \\times A +b$ 那么反向传播过程中需要求的就是$dA,dW,db$，其中$dA$是原输入的数据，包含了原图像中的每一个像素， 而这个时候假设从后面传过来的$dZ$是已经知道的。 1.计算dA 从公式可以看出，$dA = W \\times dZ$，具体一点，$dA$的每一个切片就是$W_c$乘上$dZ$在输出图片的每一个像素的求和结果，从矩阵的角度，每一次$W_c\\times dZ_{hw}$得到的就是从单个输出的图片像素到输入图片切片（大小为W）的映射。因此公式为： $$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} $$ 1da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c] 2.计算dW $dW = A \\times dZ$，而更具体一点，因为W对Z的每一个像素都是有作用的，所以就等于每一个输入图片的切片乘以对应输出图片像素的导数，然后再求和！ $$ dW_c += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw} $$ 1dW[:,:,:,c] += a_slice * dZ[i, h, w, c] 3.计算db $$ db = \\sum_h \\sum_w dZ_{hw} $$ 1db[:,:,:,c] += dZ[i, h, w, c] 所以得到以下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576def conv_backward(dZ, cache): \"\"\" Implement the backward propagation for a convolution function Arguments: dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C) cache -- cache of values needed for the conv_backward(), output of conv_forward() Returns: dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev), numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev) dW -- gradient of the cost with respect to the weights of the conv layer (W) numpy array of shape (f, f, n_C_prev, n_C) db -- gradient of the cost with respect to the biases of the conv layer (b) numpy array of shape (1, 1, 1, n_C) \"\"\" ### START CODE HERE ### # Retrieve information from \"cache\" (A_prev, W, b, hparameters) = cache # Retrieve dimensions from A_prev's shape (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape # Retrieve dimensions from W's shape (f, f, n_C_prev, n_C) = W.shape # Retrieve information from \"hparameters\" stride = hparameters['stride'] pad = hparameters['pad'] # Retrieve dimensions from dZ's shape (m, n_H, n_W, n_C) = dZ.shape # Initialize dA_prev, dW, db with the correct shapes dA_prev = np.zeros(A_prev.shape) dW = np.zeros(W.shape) db = np.zeros(b.shape) # Pad A_prev and dA_prev A_prev_pad = zero_pad(A_prev, pad) dA_prev_pad = zero_pad(dA_prev, pad) for i in range(m): # loop over the training examples # select ith training example from A_prev_pad and dA_prev_pad a_prev_pad = A_prev_pad[i] da_prev_pad = dA_prev_pad[i] for h in range(n_H): # loop over vertical axis of the output volume for w in range(n_W): # loop over horizontal axis of the output volume for c in range(n_C): # loop over the channels of the output volume # Find the corners of the current \"slice\" vert_start = h * stride vert_end = h * stride + f horiz_start = w * stride horiz_end = w * stride + f # Use the corners to define the slice from a_prev_pad a_slice = a_prev_pad[vert_start : vert_end, horiz_start : horiz_end, : ] # Update gradients for the window and the filter's parameters using the code formulas given above da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[ i, h, w ,c] dW[:,:,:,c] += a_slice * dZ[ i, h, w ,c] db[:,:,:,c] += dZ[ i, h, w ,c] # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :]) dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :] ### END CODE HERE ### # Making sure your output shape is correct assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev)) return dA_prev, dW, db Pooling layer - backward pass这里max pooling和average poolling要分开处理。 1. Max pooling - backward pass 假设pool size是$2 \\times 2$的，那么，4个像素中只有1个留下来了，其余的都没有效果了，所以在max pooling中，从后面传递过来的导数值，只作用在max的那个元素，而且继续往前传递，不做任何改动，在其余3个元素的导数都是0。 创建一个mask矩阵，让最大值为1，其余的都为0，这样子就可以作为一个映射矩阵向前映射了。 $$ X = \\begin{bmatrix}1 &amp;&amp; 3 \\\\ 4 &amp;&amp; 2 \\end{bmatrix} \\quad \\rightarrow \\quad M =\\begin{bmatrix}0 &amp;&amp; 0 \\\\1 &amp;&amp; 0\\end{bmatrix}$$ 12345678910111213141516def create_mask_from_window(x): \"\"\" Creates a mask from an input matrix x, to identify the max entry of x. Arguments: x -- Array of shape (f, f) Returns: mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x. \"\"\" ### START CODE HERE ### (≈1 line) mask = (x == np.max(x)) ### END CODE HERE ### return mask 2. Average pooling - backward pass 和max不同，average pooling相当于把backward传过来的值分成了$n_H \\times n_W$等分。所以要计算的参数就比max pooling多很多了，这也就是为什么一般都用max pooling，不用average pooling $$ dZ = 1 \\quad \\rightarrow \\quad dZ =\\begin{bmatrix}1/4 &amp;&amp; 1/4 \\\\1/4 &amp;&amp; 1/4\\end{bmatrix}$$ 123456789101112131415161718192021222324def distribute_value(dz, shape): \"\"\" Distributes the input value in the matrix of dimension shape Arguments: dz -- input scalar shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz Returns: a -- Array of size (n_H, n_W) for which we distributed the value of dz \"\"\" ### START CODE HERE ### # Retrieve dimensions from shape (≈1 line) (n_H, n_W) = shape # Compute the value to distribute on the matrix (≈1 line) average = n_H * n_W # Create a matrix where every entry is the \"average\" value (≈1 line) a = dz / average * np.ones((n_H, n_W)) ### END CODE HERE ### return a 结合两种方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def pool_backward(dA, cache, mode = \"max\"): \"\"\" Implements the backward pass of the pooling layer Arguments: dA -- gradient of cost with respect to the output of the pooling layer, same shape as A cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\") Returns: dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev \"\"\" ### START CODE HERE ### # Retrieve information from cache (≈1 line) (A_prev, hparameters) = cache # Retrieve hyperparameters from \"hparameters\" (≈2 lines) stride = hparameters['stride'] f = hparameters['f'] # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines) m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape m, n_H, n_W, n_C = dA.shape # Initialize dA_prev with zeros (≈1 line) dA_prev = np.zeros(A_prev.shape) for i in range(m): # loop over the training examples # select training example from A_prev (≈1 line) a_prev = A_prev[i] for h in range(n_H): # loop on the vertical axis for w in range(n_W): # loop on the horizontal axis for c in range(n_C): # loop over the channels (depth) # Find the corners of the current \"slice\" (≈4 lines) vert_start = h * stride vert_end = vert_start + f horiz_start = w * stride horiz_end = horiz_start + f # Compute the backward propagation in both modes. if mode == \"max\": # Use the corners and \"c\" to define the current slice from a_prev (≈1 line) a_prev_slice = a_prev[vert_start : vert_end, horiz_start : horiz_end, c] # Create the mask from a_prev_slice (≈1 line) mask = create_mask_from_window(a_prev_slice) # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line) dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i, h, w, c] elif mode == \"average\": # Get the value a from dA (≈1 line) da = dA[i, h, w, c] # Define the shape of the filter as fxf (≈1 line) shape = (f, f) # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line) dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape) ### END CODE ### # Making sure your output shape is correct assert(dA_prev.shape == A_prev.shape) return dA_prev Part2：Convolutional Neural Networks: Application用TensorFlow来搭建卷积神经网络。 1.Create placeholders先创建placeholders，用来训练中传递X,Y 12345678910111213141516171819202122def create_placeholders(n_H0, n_W0, n_C0, n_y): \"\"\" Creates the placeholders for the tensorflow session. Arguments: n_H0 -- scalar, height of an input image n_W0 -- scalar, width of an input image n_C0 -- scalar, number of channels of the input n_y -- scalar, number of classes Returns: X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\" Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\" \"\"\" ### START CODE HERE ### (≈2 lines) X = tf.placeholder(tf.float32, shape=(None,n_H0, n_W0, n_C0)) Y = tf.placeholder(tf.float32, shape=(None,n_y)) ### END CODE HERE ### return X, Y 2.Initialize parameters用来初始化参数，主要是W1,W2,在这里就没有用b了 用W = tf.get_variable(&quot;W&quot;, [1,2,3,4], initializer = ...) initializer 用tf.contrib.layers.xavier_initializer 12345678910111213141516171819202122# GRADED FUNCTION: initialize_parametersdef initialize_parameters(): \"\"\" Initializes weight parameters to build a neural network with tensorflow. The shapes are: W1 : [4, 4, 3, 8] W2 : [2, 2, 8, 16] Returns: parameters -- a dictionary of tensors containing W1, W2 \"\"\" tf.set_random_seed(1) # so that your \"random\" numbers match ours ### START CODE HERE ### (approx. 2 lines of code) W1 = tf.get_variable('W1', [4, 4, 3, 8],initializer= tf.contrib.layers.xavier_initializer(seed = 0 )) W2 = tf.get_variable('W2', [2, 2, 8, 16],initializer= tf.contrib.layers.xavier_initializer(seed = 0)) ### END CODE HERE ### parameters = &#123;\"W1\": W1, \"W2\": W2&#125; return parameters 记得这只是创建了图而已，并没有真正的初始化参数，在执行中还需要 init = tf.global_variables_initializer() sess_test.run(init) 3. Forward propagation模型为：CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED 12345678- Conv2D: stride 1, padding is &quot;SAME&quot;- ReLU- Max pool: Use an 8 by 8 filter size and an 8 by 8 stride, padding is &quot;SAME&quot;- Conv2D: stride 1, padding is &quot;SAME&quot;- ReLU- Max pool: Use a 4 by 4 filter size and a 4 by 4 stride, padding is &quot;SAME&quot;- Flatten the previous output.- FULLYCONNECTED (FC) layer：这里全连接层不需要有激活函数，因为后面计算cost的时候会加上softmax，因此这里不需要加 用到的函数： tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = ‘SAME’): given an input $X$ and a group of filters $W1$, this function convolves $W1$’s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation here tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = ‘SAME’): given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. You can read the full documentation here tf.nn.relu(Z1): computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation here. tf.contrib.layers.flatten(P): given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k]. You can read the full documentation here. tf.contrib.layers.fully_connected(F, num_outputs): given a the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation here. In the last function above (tf.contrib.layers.fully_connected), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters. 1234567891011121314151617181920212223242526272829303132333435363738394041# GRADED FUNCTION: forward_propagationdef forward_propagation(X, parameters): \"\"\" Implements the forward propagation for the model: CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED Arguments: X -- input dataset placeholder, of shape (input size, number of examples) parameters -- python dictionary containing your parameters \"W1\", \"W2\" the shapes are given in initialize_parameters Returns: Z3 -- the output of the last LINEAR unit \"\"\" # Retrieve the parameters from the dictionary \"parameters\" W1 = parameters['W1'] W2 = parameters['W2'] ### START CODE HERE ### # CONV2D: stride of 1, padding 'SAME' Z1 = tf.nn.conv2d(X, filter=W1, strides=[1,1,1,1],padding='SAME') # RELU A1 = tf.nn.relu(Z1) # MAXPOOL: window 8x8, sride 8, padding 'SAME' P1 = tf.nn.max_pool(A1,ksize=[1, 8, 8, 1], strides=[1, 8, 8, 1],padding='SAME') # CONV2D: filters W2, stride 1, padding 'SAME' Z2 = tf.nn.conv2d(P1, filter=W2, strides=[1, 1, 1, 1],padding='SAME') # RELU A2 = tf.nn.relu(Z2) # MAXPOOL: window 4x4, stride 4, padding 'SAME' P2 = tf.nn.max_pool(A2,ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1],padding='SAME') # FLATTEN P2 = tf.contrib.layers.flatten(P2) # FULLY-CONNECTED without non-linear activation function (not not call softmax). # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" Z3 = tf.contrib.layers.fully_connected(P2, 6,activation_fn=None) ### END CODE HERE ### return Z3 4. Compute cost tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y): computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation here.这个函数已经包含了计算softmax，还有求cross-entropy两件事了。 tf.reduce_mean: computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation here. 12345678910111213141516171819# GRADED FUNCTION: compute_cost def compute_cost(Z3, Y): \"\"\" Computes the cost Arguments: Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples) Y -- \"true\" labels vector placeholder, same shape as Z3 Returns: cost - Tensor of the cost function \"\"\" ### START CODE HERE ### (1 line of code) cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3,labels=Y)) ### END CODE HERE ### return cost 5. Model把前面的函数都结合起来，创建一个完整的模型。 其中random_mini_batches()已经给我们了，优化器使用了 optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113# GRADED FUNCTION: modeldef model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009, num_epochs = 100, minibatch_size = 64, print_cost = True): \"\"\" Implements a three-layer ConvNet in Tensorflow: CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED Arguments: X_train -- training set, of shape (None, 64, 64, 3) Y_train -- test set, of shape (None, n_y = 6) X_test -- training set, of shape (None, 64, 64, 3) Y_test -- test set, of shape (None, n_y = 6) learning_rate -- learning rate of the optimization num_epochs -- number of epochs of the optimization loop minibatch_size -- size of a minibatch print_cost -- True to print the cost every 100 epochs Returns: train_accuracy -- real number, accuracy on the train set (X_train) test_accuracy -- real number, testing accuracy on the test set (X_test) parameters -- parameters learnt by the model. They can then be used to predict. \"\"\" ops.reset_default_graph() # to be able to rerun the model without overwriting tf variables tf.set_random_seed(1) # to keep results consistent (tensorflow seed) seed = 3 # to keep results consistent (numpy seed) (m, n_H0, n_W0, n_C0) = X_train.shape n_y = Y_train.shape[1] costs = [] # To keep track of the cost # Create Placeholders of the correct shape ### START CODE HERE ### (1 line) X, Y = create_placeholders(n_H0, n_W0,n_C0,n_y) ### END CODE HERE ### # Initialize parameters ### START CODE HERE ### (1 line) parameters = initialize_parameters() ### END CODE HERE ### # Forward propagation: Build the forward propagation in the tensorflow graph ### START CODE HERE ### (1 line) Z3 = forward_propagation(X,parameters) ### END CODE HERE ### # Cost function: Add cost function to tensorflow graph ### START CODE HERE ### (1 line) cost = compute_cost(Z3, Y) ### END CODE HERE ### # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost. ### START CODE HERE ### (1 line) optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) ### END CODE HERE ### # Initialize all the variables globally init = tf.global_variables_initializer() # Start the session to compute the tensorflow graph with tf.Session() as sess: # Run the initialization sess.run(init) # Do the training loop for epoch in range(num_epochs): minibatch_cost = 0. num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set seed = seed + 1 minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed) for minibatch in minibatches: # Select a minibatch (minibatch_X, minibatch_Y) = minibatch # IMPORTANT: The line that runs the graph on a minibatch. # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y). ### START CODE HERE ### (1 line) _ , temp_cost = sess.run([optimizer,cost],feed_dict=&#123;X:minibatch_X,Y:minibatch_Y&#125;) ### END CODE HERE ### minibatch_cost += temp_cost / num_minibatches # Print the cost every epoch if print_cost == True and epoch % 5 == 0: print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost)) if print_cost == True and epoch % 1 == 0: costs.append(minibatch_cost) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() # Calculate the correct predictions predict_op = tf.argmax(Z3, 1) correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1)) # Calculate accuracy on the test set accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) print(accuracy) train_accuracy = accuracy.eval(&#123;X: X_train, Y: Y_train&#125;) test_accuracy = accuracy.eval(&#123;X: X_test, Y: Y_test&#125;) print(\"Train Accuracy:\", train_accuracy) print(\"Test Accuracy:\", test_accuracy) return train_accuracy, test_accuracy, parameters 得到效果如图：","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"},{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai笔记:(4-1)-- 卷积神经网络（Foundations of CNN）","slug":"dl-ai-4-1","date":"2018-09-30T02:20:54.000Z","updated":"2018-09-30T07:49:24.135Z","comments":true,"path":"2018/dl-ai-4-1/","link":"","permalink":"http://fangzh.top/2018/dl-ai-4-1/","excerpt":"第四门课开始就学习深度学习关于计算机视觉的重要应用—卷积神经网络。 第一周主要是对卷积神经网络的基本构造和原理做了介绍。","text":"第四门课开始就学习深度学习关于计算机视觉的重要应用—卷积神经网络。 第一周主要是对卷积神经网络的基本构造和原理做了介绍。 计算机视觉计算机视觉是深度学习的一个非常重要的应用。比如图像分类，目标检测，图片风格迁移等。 用传统的深度学习算法，假设你有一张$64×64$的猫片，又有RGB三通道，那么这个时候是$64×64×3=12288$，input layer的维度就是12288，这样其实也还可以，因为图片很小。那么如果你有$1000×1000$的照片呢，你的向量就会有300万！假设有1000个隐藏神经元，那么就是第一层的参数矩阵$W$有30亿个参数！算到地老天荒。所以用传统的深度学习算法是不现实的。 边缘检测如图，这些边缘检测中，用水平检测和垂直检测会得到不同的结果。 垂直检测如下图，用一个$3×3$的过滤器（filter），也叫卷积核，在原图片$6×6$的对应地方按元素相乘，得到$4×4$的图片。 可以看到，用垂直边缘的filter可以将原图片中间的边缘区分出来，也就是得到了最右图中最亮的部分即为检测到的边缘。 当然，如果左图的亮暗分界线反过来，则输出图片中最暗的部分表示边缘。 也自然有水平的边缘分类器。 还有更复杂的，但是我们不需要进行人工的决定这些filter是什么，因为我们可以通过训练，让机器自己学到这些参数。 paddingpadding是填充的意思。 我们可以从之前的例子看到，每经过一次卷积运算，图片的像素都会变小，从$6×6 —&gt; 4×4$，这样子图片就会越来越小，后面就毛都不剩了。 还有一点就是，从卷积的运算方法来看，边缘和角落的位置卷积的次数少，会丢失有用信息。 所以就有padding的想法了，也就是在图片四周填补上像素。 计算方法如下， 原数据是$n \\times n$，filter为$f \\times f$,padding为$p \\times p$， 那么得到的矩阵大小是$(n + 2p -f +1)\\times(n + 2p -f +1)$ padding有两种： valid：也就是不填充 same：输入与输出大小相同的图片, $p=(f - 1) / 2$，一般padding为奇数，因为filter是奇数 stride（步长）卷积的步长也就是每一次运算后平移的距离，之前使用都是stride=1。 假设stride=2，就会得到： 得到的矩阵大小是 $$\\lfloor \\frac{n+2p-f}{s}+1\\rfloor \\times \\lfloor \\frac{n+2p-f}{s}+1\\rfloor$$ 向下取整: 59/60 = 0 立体卷积之前都是单通道的图片进行卷积，如果有RGB三种颜色的话，就要使用立体卷积了。 这个时候的卷积核就变成了$3 \\times 3 \\times 3$的三维卷积核，一共27个参数，每次对应着原图片上的RGB一共27个像素运算，然后求和得到输出图片的一个像素。因为只有一个卷积核，这个时候输出的还是$4 \\times 4 \\times 1$的图片。 多个卷积核 因为不同的卷积核可以提取不同的图片特征，所以可以有很多个卷积核，同时提取图片的特征，如分别提取图片的水平和垂直边缘特征。 因为有了两个卷积核，这时候输出的图片就是有两通道的图片$4\\times 4 \\times 2$。 这里要搞清两个概念，卷积核的通道数和个数： 通道数channel：即卷积核要作用在原图片上，原图片的通道处$n_c$，卷积核的通道数必须和原图片通道数相同 个数：即要使用多少个这样的卷积核，使用$n_{c}^{\\prime}$表示，卷积核的个数也就是输出图片的通道数，如有两个卷积核，那么生成了$4\\times 4 \\times 2$的图片，2 就是卷积核的个数 即 $n \\times n \\times n_c$ ，乘上的$n_{c}^{\\prime}$个卷积核 $ f \\times f \\times n_c$，得到$(n -f +1)\\times (n - f +1 ) \\times n_{c}^{\\prime}$的新图片 卷积神经网络单层卷积网络 如图是单层卷积的基本过程，先经过两个卷积核，然后再加上bias进行relu激活函数。 那么假设某层卷积层有10个$3 \\times 3 \\times 3$的卷积核，那么一共有$(3\\times3\\times3+1) \\times10=280$个参数，加1是加上了bias 在这里总结了各个参数的表示方法： 简单神经网络 一般卷积神经网络层的类型有： convolution卷积层 pool池化层 fully connected全连接层 池化层pooling 的作用就是用来压缩数据，加速运算，提高提取特征的鲁棒性 Max pooling 在范围内取最大值 Average Pooling 取平均值 卷积神经网络示例 一般conv后都会进行pooling，所以可以把conv和pooling当做一层。 如上图就是$conv-pool-conv-pool-fc-fc-fc-softmax$的卷积神经网络结构。 各个层的参数是这样的： 可以看到，在卷积层的参数非常少，池化层没有参数，大量的参数在全连接层。 为何用卷积神经网络？这里给出了两点主要原因： 参数共享：卷积核的参数是原图片中各个像素之间共享的，所以大大减少了参数 连接的稀疏性：每个输出值，实际上只取决于很少量的输入而已。","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"cs231n作业：assignment1 - features","slug":"cs231n-1h-5","date":"2018-09-27T09:25:31.000Z","updated":"2018-09-30T10:21:09.494Z","comments":true,"path":"2018/cs231n-1h-5/","link":"","permalink":"http://fangzh.top/2018/cs231n-1h-5/","excerpt":"GitHub地址：https://github.com/ZJUFangzh/cs231n 抽取图像的HOG和HSV特征。","text":"GitHub地址：https://github.com/ZJUFangzh/cs231n 抽取图像的HOG和HSV特征。 对于每张图，我们会计算梯度方向直方图(HOG)特征和用HSV（Hue色调，Saturation饱和度,Value明度）颜色空间的色调特征。把每张图的梯度方向直方图和颜色直方图特征合并形成我们最后的特征向量。 粗略的讲呢，HOG应该可以捕捉到图像的纹理特征而忽略了颜色信息，颜色直方图会表示图像的颜色特征而忽略了纹理特征(详细见这篇)。所以我们预估把两者结合起来得到的效果应该是比用其中一种得到的效果好。对于后面的bonus，验证一下这个设想是不错的选择。 hog_feature和color_histogram_hsv两个函数都只对一张图做操作并返回这张图片的特征向量。extract_features函数接收一堆图片和一个list的特征函数，然后用每个特征函数在每张图片上过一遍，把结果存到一个矩阵里面，矩阵的每一行都是一张图片的所有特征的合并。 在features.py中写了两个特征的计算方法，HOG是改写了scikit-image的fog接口，并且首先要转换成灰度图。颜色直方图是实现用matplotlib.colors.rgb_to_hsv的接口把图片从RGB变成HSV，再提取明度(value)，把value投射到不同的bin当中去。关于HOG的原理请谷歌百度。 如果出错： orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[cx/2::cx, cy/2::cy].T这行报错,“TypeError: slice indices must be integers or None or have an index method”,可以把代码改成: orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[int(cx/2)::cx, int(cy/2)::cy].T 通过这一步，把原来的数据集都提取出了特征，换成了X_train_feats,X_val_feats,X_test_feats 12345678910111213141516171819202122232425from cs231n.features import *num_color_bins = 10 # Number of bins in the color histogramfeature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]X_train_feats = extract_features(X_train, feature_fns, verbose=True)X_val_feats = extract_features(X_val, feature_fns)X_test_feats = extract_features(X_test, feature_fns)# Preprocessing: Subtract the mean featuremean_feat = np.mean(X_train_feats, axis=0, keepdims=True)X_train_feats -= mean_featX_val_feats -= mean_featX_test_feats -= mean_feat# Preprocessing: Divide by standard deviation. This ensures that each feature# has roughly the same scale.std_feat = np.std(X_train_feats, axis=0, keepdims=True)X_train_feats /= std_featX_val_feats /= std_featX_test_feats /= std_feat# Preprocessing: Add a bias dimensionX_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))]) SVM跟之前都一样的，把训练集换成 ***_feats就行了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# Use the validation set to tune the learning rate and regularization strengthfrom cs231n.classifiers.linear_classifier import LinearSVMlearning_rates = [1e-9, 1e-8, 1e-7]regularization_strengths = [5e4, 5e5, 5e6]results = &#123;&#125;best_val = -1best_svm = Nonelearning_rates =[5e-9, 7.5e-9, 1e-8]regularization_strengths = [(5+i)*1e6 for i in range(-3,4)]################################################################################# TODO: ## Use the validation set to set the learning rate and regularization strength. ## This should be identical to the validation that you did for the SVM; save ## the best trained classifer in best_svm. You might also want to play ## with different numbers of bins in the color histogram. If you are careful ## you should be able to get accuracy of near 0.44 on the validation set. #################################################################################for learning_rate in learning_rates: for regularization_strength in regularization_strengths: svm = LinearSVM() loss_hist = svm.train(X_train_feats, y_train, learning_rate=learning_rate, reg=regularization_strength, num_iters=1500, verbose=False) y_train_pred = svm.predict(X_train_feats) y_val_pred = svm.predict(X_val_feats) y_train_acc = np.mean(y_train_pred==y_train) y_val_acc = np.mean(y_val_pred==y_val) results[(learning_rate,regularization_strength)] = [y_train_acc, y_val_acc] if y_val_acc &gt; best_val: best_val = y_val_acc best_svm = svm################################################################################# END OF YOUR CODE ################################################################################## Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val) Neural Network on image features1234567891011121314151617181920212223242526272829303132333435363738394041424344from cs231n.classifiers.neural_net import TwoLayerNetinput_dim = X_train_feats.shape[1]hidden_dim = 500num_classes = 10net = TwoLayerNet(input_dim, hidden_dim, num_classes)best_net = None################################################################################# TODO: Train a two-layer neural network on image features. You may want to ## cross-validate various parameters as in previous sections. Store your best ## model in the best_net variable. #################################################################################best_val = -1learning_rates = [1.2e-3, 1.5e-3, 1.75e-3]regularization_strengths = [1, 1.25, 1.5 , 2]for lr in learning_rates: for reg in regularization_strengths:## net = TwoLayerNet(input_dim, hidden_dim, num_classes) loss_hist = net.train(X_train_feats, y_train, X_val_feats, y_val, num_iters=1000, batch_size=200, learning_rate=lr, learning_rate_decay=0.95, reg=reg, verbose=False) y_train_pred = net.predict(X_train_feats) y_val_pred = net.predict(X_val_feats) y_train_acc = np.mean(y_train_pred==y_train) y_val_acc = np.mean(y_val_pred==y_val) results[(lr,reg)] = [y_train_acc, y_val_acc] if y_val_acc &gt; best_val: best_val = y_val_acc best_net = netfor lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val)################################################################################# END OF YOUR CODE #################################################################################","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://fangzh.top/tags/cs231n/"},{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"}]},{"title":"cs231n作业：assignment1 - two_layer_net","slug":"cs231n-1h-4","date":"2018-09-27T09:05:59.000Z","updated":"2018-09-30T10:19:48.026Z","comments":true,"path":"2018/cs231n-1h-4/","link":"","permalink":"http://fangzh.top/2018/cs231n-1h-4/","excerpt":"github地址：https://github.com/ZJUFangzh/cs231n 搭建一个两层的神经网络。","text":"github地址：https://github.com/ZJUFangzh/cs231n 搭建一个两层的神经网络。 Forward pass先计算前向传播过程，编辑cs231n/classifiers/neural_net.py的TwoLayerNet.loss函数 这个就和之前的svm和softmax一样了： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def loss(self, X, y=None, reg=0.0): \"\"\" Compute the loss and gradients for a two layer fully connected neural network. Inputs: - X: Input data of shape (N, D). Each X[i] is a training sample. - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it is not passed then we only return scores, and if it is passed then we instead return the loss and gradients. - reg: Regularization strength. Returns: If y is None, return a matrix scores of shape (N, C) where scores[i, c] is the score for class c on input X[i]. If y is not None, instead return a tuple of: - loss: Loss (data loss and regularization loss) for this batch of training samples. - grads: Dictionary mapping parameter names to gradients of those parameters with respect to the loss function; has the same keys as self.params. \"\"\" # Unpack variables from the params dictionary W1, b1 = self.params['W1'], self.params['b1'] W2, b2 = self.params['W2'], self.params['b2'] N, D = X.shape # Compute the forward pass scores = None ############################################################################# # TODO: Perform the forward pass, computing the class scores for the input. # # Store the result in the scores variable, which should be an array of # # shape (N, C). # ############################################################################# Z1 = X.dot(W1) + b1 A1 = np.maximum(0, Z1) scores = A1.dot(W2) + b2 ############################################################################# # END OF YOUR CODE # ############################################################################# # If the targets are not given then jump out, we're done if y is None: return scores # Compute the loss loss = None ############################################################################# # TODO: Finish the forward pass, and compute the loss. This should include # # both the data loss and L2 regularization for W1 and W2. Store the result # # in the variable loss, which should be a scalar. Use the Softmax # # classifier loss. # ############################################################################# scores -= np.max(scores, axis=1,keepdims=True) exp_scores = np.exp(scores) probs = exp_scores / np.sum(exp_scores,axis=1,keepdims=True) y_label = np.zeros((N,probs.shape[1])) y_label[np.arange(N),y] = 1 loss = (-1) * np.sum(np.multiply(np.log(probs),y_label)) / N loss += reg * (np.sum(W1 * W1) + np.sum(W2 * W2)) ############################################################################# # END OF YOUR CODE # ############################################################################# 检验一下： 123456loss, _ = net.loss(X, y, reg=0.05)correct_loss = 1.30378789133# should be very small, we get &lt; 1e-12print(&apos;Difference between your loss and correct loss:&apos;)print(np.sum(np.abs(loss - correct_loss))) 12Difference between your loss and correct loss:1.7985612998927536e-13 Backword pass依旧是这个loss函数里面，根据W1,b1,W2,b2，求出grads，求导的公式课程里没给，不过NG老师给了，shallow neural networks，但是表示的维度不太一样，需要做稍微的修改: 1234567891011121314151617181920212223# Backward pass: compute gradientsgrads = &#123;&#125;############################################################################## TODO: Compute the backward pass, computing the derivatives of the weights ## and biases. Store the results in the grads dictionary. For example, ## grads['W1'] should store the gradient on W1, and be a matrix of same size ##############################################################################dZ2 = probs-y_labeldW2 = A1.T.dot(dZ2)dW2 /= NdW2 += 2 * reg*W2db2 = np.sum(dZ2,axis=0) / NdZ1 = (dZ2).dot(W2.T) * (A1 &gt; 0)dW1 = X.T.dot(dZ1) / N + 2 * reg * W1db1 = np.sum(dZ1,axis=0) / Ngrads['W2'] = dW2grads['b2'] = db2grads['W1'] = dW1grads['b1'] = db1############################################################################## END OF YOUR CODE ############################################################################## 检验一下： 1234W2 max relative error: 3.440708e-09b2 max relative error: 3.865091e-11W1 max relative error: 3.561318e-09b1 max relative error: 1.555471e-09 train network补全train()函数，其实是一样的，先创建一个minibatch，然后计算得到loss和grads，更新params： 12345678910111213141516171819202122232425262728########################################################################## TODO: Create a random minibatch of training data and labels, storing ## them in X_batch and y_batch respectively. ##########################################################################batch_inx = np.random.choice(num_train,batch_size)X_batch = X[batch_inx,:]y_batch = y[batch_inx]########################################################################## END OF YOUR CODE ########################################################################### Compute loss and gradients using the current minibatchloss, grads = self.loss(X_batch, y=y_batch, reg=reg)loss_history.append(loss)########################################################################## TODO: Use the gradients in the grads dictionary to update the ## parameters of the network (stored in the dictionary self.params) ## using stochastic gradient descent. You'll need to use the gradients ## stored in the grads dictionary defined above. ##########################################################################self.params['W1'] -= learning_rate * grads['W1']self.params['b1'] -= learning_rate * grads['b1']self.params['W2'] -= learning_rate * grads['W2']self.params['b2'] -= learning_rate * grads['b2']########################################################################## END OF YOUR CODE ########################################################################## 再补全predict()函数 12345678############################################################################ TODO: Implement this function; it should be VERY simple! ############################################################################score = self.loss(X)y_pred = np.argmax(score,axis=1)############################################################################ END OF YOUR CODE ############################################################################ 然后可以计算画图了： 12345678910111213net = init_toy_model()stats = net.train(X, y, X, y, learning_rate=1e-1, reg=5e-6, num_iters=100, verbose=False)print('Final training loss: ', stats['loss_history'][-1])# plot the loss historyplt.plot(stats['loss_history'])plt.xlabel('iteration')plt.ylabel('training loss')plt.title('Training Loss history')plt.show() 载入数据集接下来就可以载入大的数据集，进行训练了，代码都写好了， 得到的准确度是:0.287 画个图： 调超参数1234567891011121314151617181920212223242526272829303132333435363738394041best_net = None # store the best model into this results = &#123;&#125;best_val = -1learning_rates = [1.2e-3, 1.5e-3, 1.75e-3]regularization_strengths = [1, 1.25, 1.5 , 2]################################################################################## TODO: Tune hyperparameters using the validation set. Store your best trained ## model in best_net. ## ## To help debug your network, it may help to use visualizations similar to the ## ones we used above; these visualizations will have significant qualitative ## differences from the ones we saw above for the poorly tuned network. ## ## Tweaking hyperparameters by hand can be fun, but you might find it useful to ## write code to sweep through possible combinations of hyperparameters ## automatically like we did on the previous exercises. ##################################################################################for lr in learning_rates: for reg in regularization_strengths: net = TwoLayerNet(input_size, hidden_size, num_classes) loss_hist = net.train(X_train, y_train, X_val, y_val, num_iters=1000, batch_size=200, learning_rate=lr, learning_rate_decay=0.95, reg=reg, verbose=False) y_train_pred = net.predict(X_train) y_val_pred = net.predict(X_val) y_train_acc = np.mean(y_train_pred==y_train) y_val_acc = np.mean(y_val_pred==y_val) results[(lr,reg)] = [y_train_acc, y_val_acc] if y_val_acc &gt; best_val: best_val = y_val_acc best_net = netfor lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val)################################################################################## END OF YOUR CODE ##################################################################################","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://fangzh.top/tags/cs231n/"},{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"}]},{"title":"cs231n作业：assignment1 - softmax","slug":"cs231n-1h-3","date":"2018-09-27T08:02:57.000Z","updated":"2018-09-30T10:21:01.555Z","comments":true,"path":"2018/cs231n-1h-3/","link":"","permalink":"http://fangzh.top/2018/cs231n-1h-3/","excerpt":"GitHub地址：https://github.com/ZJUFangzh/cs231n softmax是最常用的分类器之一。","text":"GitHub地址：https://github.com/ZJUFangzh/cs231n softmax是最常用的分类器之一。 softmax和svm都是常用的分类器，而softmax更为常用。 具体可以参考我这篇的最后，ng老师有讲，softmax 前面数据集的都跟SVM的一样。 直接进入loss和grads推导环节。 $$L_i = -log(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}})$$ 可以看到，计算的公式也就是cross-entropy，即 $$H(p,q) = - \\sum_i y_i log(y_{i}^{hat})$$ 但是，这样有一个缺点，就是指数$e^{f_{y_i}}$可能会特别大，这样可能导致内存不足，计算不稳定等问题。那么可以在分子分母同乘一个常数C，一般C取为$logC = -max f_j$ 123456f = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大p = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸# 那么将f中的值平移到最大值为0：f -= np.max(f) # f becomes [-666, -333, 0]p = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果 精确地说，SVM分类器使用的是折叶损失（hinge loss），有时候又被称为最大边界损失（max-margin loss）。Softmax分类器使用的是交叉熵损失（corss-entropy loss）。Softmax分类器的命名是从softmax函数那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。 求导过程参考：cs231n softmax求导 最终得到的公式是： softmax代码实现 编辑cs231n/classifiers/softmax.py,先写一下softmax_loss_naive函数，依旧是循环： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def softmax_loss_naive(W, X, y, reg): \"\"\" Softmax loss function, naive implementation (with loops) Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W \"\"\" # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) ############################################################################# # TODO: Compute the softmax loss and its gradient using explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# (N, D) = X.shape C = W.shape[1] #遍历每个样本 for i in range(N): f_i = X[i].dot(W) #进行公式的指数修正 f_i -= np.max(f_i) sum_j = np.sum(np.exp(f_i)) #得到样本中每个类别的概率 p = lambda k : np.exp(f_i[k]) / sum_j loss += - np.log(p(y[i])) #根据softmax求导公式 for k in range(C): p_k = p(k) dW[:, k] += (p_k - (k == y[i])) * X[i] loss /= N loss += 0.5 * reg * np.sum(W * W) dW /= N dW += reg*W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW 验证一下loss和grad得到： 1234567891011121314151617181920numerical: -0.621593 analytic: -0.621593, relative error: 7.693773e-09numerical: -2.576505 analytic: -2.576505, relative error: 4.492083e-09numerical: -1.527801 analytic: -1.527801, relative error: 4.264914e-08numerical: 1.101379 analytic: 1.101379, relative error: 9.735173e-09numerical: 2.375620 analytic: 2.375620, relative error: 3.791861e-08numerical: 3.166961 analytic: 3.166960, relative error: 8.526285e-09numerical: -1.440997 analytic: -1.440998, relative error: 4.728898e-08numerical: 0.563304 analytic: 0.563304, relative error: 2.409996e-08numerical: -2.057292 analytic: -2.057292, relative error: 1.820335e-08numerical: -0.450338 analytic: -0.450338, relative error: 8.075985e-08numerical: -0.233090 analytic: -0.233090, relative error: 4.136546e-08numerical: 0.251391 analytic: 0.251391, relative error: 4.552523e-08numerical: 0.787031 analytic: 0.787031, relative error: 5.036469e-08numerical: -1.801593 analytic: -1.801594, relative error: 3.159903e-08numerical: -0.294108 analytic: -0.294109, relative error: 1.792497e-07numerical: -1.974307 analytic: -1.974307, relative error: 1.160708e-08numerical: 2.986921 analytic: 2.986921, relative error: 2.788065e-08numerical: -0.247281 analytic: -0.247281, relative error: 8.957573e-08numerical: 0.569337 analytic: 0.569337, relative error: 2.384912e-08numerical: -1.579298 analytic: -1.579298, relative error: 1.728733e-08 向量化softmax 12345678910111213141516171819202122232425262728293031323334353637383940def softmax_loss_vectorized(W, X, y, reg): \"\"\" Softmax loss function, vectorized version. Inputs and outputs are the same as softmax_loss_naive. \"\"\" # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) ############################################################################# # TODO: Compute the softmax loss and its gradient using no explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# (N, D) = X.shape C = W.shape[1] f = X.dot(W) #在列方向进行指数修正 f -= np.max(f,axis=1,keepdims=True) #求得softmax各个类的概率 p = np.exp(f) / np.sum(np.exp(f),axis=1,keepdims=True) y_lable = np.zeros((N,C)) #y_lable就是(N,C)维的矩阵，每一行中只有对应的那个正确类别 = 1，其他都是0 y_lable[np.arange(N),y] = 1 #cross entropy loss = -1 * np.sum(np.multiply(np.log(p),y_lable)) / N loss += 0.5 * reg * np.sum( W * W) #求导公式，很清晰 dW = X.T.dot(p-y_lable) dW /= N dW += reg*W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW 检验一下向量化和非向量化的时间： 1234naive loss: 2.357905e+00 computed in 0.091724svectorized loss: 2.357905e+00 computed in 0.002995sLoss difference: 0.000000Gradient difference: 0.000000 softmax的函数已经编写完成了，接下来调一下学习率和正则化两个超参数： 123456789101112131415161718192021222324252627282930313233343536373839# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of over 0.35 on the validation set.from cs231n.classifiers import Softmaxresults = &#123;&#125;best_val = -1best_softmax = Nonelearning_rates = [1e-7, 5e-7]regularization_strengths = [2.5e4, 5e4]################################################################################# TODO: ## Use the validation set to set the learning rate and regularization strength. ## This should be identical to the validation that you did for the SVM; save ## the best trained softmax classifer in best_softmax. #################################################################################for lr in learning_rates: for reg in regularization_strengths: softmax = Softmax() loss_hist = softmax.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=1500, verbose=True) y_train_pred = softmax.predict(X_train) y_val_pred = softmax.predict(X_val) y_train_acc = np.mean(y_train_pred==y_train) y_val_acc = np.mean(y_val_pred==y_val) results[(lr,reg)] = [y_train_acc, y_val_acc] if y_val_acc &gt; best_val: best_val = y_val_acc best_softmax = softmax################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val) 12345lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.350592 val accuracy: 0.354000lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.329551 val accuracy: 0.342000lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.347286 val accuracy: 0.359000lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.328551 val accuracy: 0.337000best validation accuracy achieved during cross-validation: 0.359000","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://fangzh.top/tags/cs231n/"},{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"}]},{"title":"cs231n作业：assignment1 - svm","slug":"cs231n-1h-2","date":"2018-09-27T06:17:45.000Z","updated":"2018-09-30T10:20:51.363Z","comments":true,"path":"2018/cs231n-1h-2/","link":"","permalink":"http://fangzh.top/2018/cs231n-1h-2/","excerpt":"GitHub地址：https://github.com/ZJUFangzh/cs231n 完成了一个基于SVM的损失函数。","text":"GitHub地址：https://github.com/ZJUFangzh/cs231n 完成了一个基于SVM的损失函数。 数据集载入的数据集依旧是: 123456Train data shape: (49000, 32, 32, 3)Train labels shape: (49000,)Validation data shape: (1000, 32, 32, 3)Validation labels shape: (1000,)Test data shape: (1000, 32, 32, 3)Test labels shape: (1000,) 而后进行32 32 3的图像拉伸，得到： 1234Training data shape: (49000, 3072)Validation data shape: (1000, 3072)Test data shape: (1000, 3072)dev data shape: (500, 3072) 进行一下简单的预处理，减去图像的平均值 1234567# Preprocessing: subtract the mean image# first: compute the image mean based on the training datamean_image = np.mean(X_train, axis=0)print(mean_image[:10]) # print a few of the elementsplt.figure(figsize=(4,4))plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean imageplt.show() 12345# second: subtract the mean image from train and test dataX_train -= mean_imageX_val -= mean_imageX_test -= mean_imageX_dev -= mean_image 12345678# third: append the bias dimension of ones (i.e. bias trick) so that our SVM# only has to worry about optimizing a single weight matrix W.X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape) 1(49000, 3073) (1000, 3073) (1000, 3073) (500, 3073) SVM分类器然后就可以开始来编写cs231n/classifiers/linear_svm.py的SVM分类器了。在这里先介绍一下SVM的基本公式和原理。 参考CS231n:线性分类 SVM损失函数想要SVM在正确分类上的比分始终比不正确的比分高出一个边界值$\\triangle$ 第i个数据图像为$x_i$，正确分类为$y_i$，然后根据$f(x_i,W)$来计算不同分类的值，将分类简写为$s$，那么第j类的得分就是$s_j = f(x_i,W)_j$，针对第i个数据的多类SVM的损失函数定义为： $$L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} + \\triangle)$$ 如：假设有3个分类，$s = [ 13,-7,11]$，第一个分类是正确的，也就是$y_i = 0$，假设$\\triangle=10$，那么把所有不正确的分类加起来($j \\neq y_i$)， $$L_i = max(0,-7-13+10)+max(0,11-13+10)$$ 因为SVM只关心差距至少要大于10，所以$L_i = 8$ 那么把公式套入： $$L_i = \\sum_{j \\neq y_i} max(0, w_j x_i - w_{y_i} x_i + \\triangle)$$ 加入正则后： $$L = \\frac{1}{N} \\sum_i \\sum_{j \\neq y_i}max(0, f(x_i ;W)_{j} - f(x_i ; W)_{y_i} + \\triangle) + \\lambda \\sum_k \\sum_l W^{2}_{k,l}$$ 到目前为止计算了loss，然后还需要计算梯度下降的grads， 官方并没有给推导过程，这才是cs231n作业难的地方所在。。。 详细可以看这一篇文章CS 231 SVM 求导 总之就是两个公式： 而后开始编写compute_loss_naive 函数，先用循环来感受一下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def svm_loss_naive(W, X, y, reg): \"\"\" Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W \"\"\" dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 #逐个计算每个样本的loss for i in xrange(num_train): #计算每个样本的各个分类得分 scores = X[i].dot(W) correct_class_score = scores[y[i]] #计算每个分类的得分，计入loss中 for j in xrange(num_classes): # 根据公式，j==y[i]的就是本身的分类，不用算了 if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 #如果计算的margin &gt; 0，那么就要算入loss， if margin &gt; 0: loss += margin #公式2 dW[:,y[i]] += -X[i,:].T #公式1 dW[:,j] += X[i, :].T # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. loss += reg * np.sum(W * W) dW += reg * W ############################################################################# # TODO: # # Compute the gradient of the loss function and store it dW. # # Rather that first computing the loss and then computing the derivative, # # it may be simpler to compute the derivative at the same time that the # # loss is being computed. As a result you may need to modify some of the # # code above to compute the gradient. # ############################################################################# return loss, dW 写完后，用梯度检验检查一下: 123456789101112131415161718# Once you've implemented the gradient, recompute it with the code below# and gradient check it with the function we provided for you# Compute the loss and its gradient at W.loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.0)# Numerically compute the gradient along several randomly chosen dimensions, and# compare them with your analytically computed gradient. The numbers should match# almost exactly along all dimensions.from cs231n.gradient_check import grad_check_sparsef = lambda w: svm_loss_naive(w, X_dev, y_dev, 0.0)[0]grad_numerical = grad_check_sparse(f, W, grad)# do the gradient check once again with regularization turned on# you didn't forget the regularization gradient did you?loss, grad = svm_loss_naive(W, X_dev, y_dev, 5e1)f = lambda w: svm_loss_naive(w, X_dev, y_dev, 5e1)[0]grad_numerical = grad_check_sparse(f, W, grad) 1234567891011121314151617181920numerical: 34.663598 analytic: 34.663598, relative error: 6.995024e-13numerical: 21.043334 analytic: 21.043334, relative error: 5.147242e-12numerical: 1.334055 analytic: 1.334055, relative error: 5.315420e-11numerical: 16.611704 analytic: 16.611704, relative error: 6.908581e-12numerical: 25.327188 analytic: 25.327188, relative error: 1.552987e-11numerical: -12.867717 analytic: -12.867717, relative error: 1.966004e-11numerical: 15.066285 analytic: 15.066285, relative error: 7.012975e-12numerical: -3.752014 analytic: -3.752014, relative error: 7.502607e-11numerical: 9.927043 analytic: 9.927043, relative error: 9.010584e-13numerical: 33.071345 analytic: 33.071345, relative error: 1.305438e-12numerical: -19.227144 analytic: -19.227851, relative error: 1.836495e-05numerical: 31.392728 analytic: 31.391611, relative error: 1.778034e-05numerical: -10.450509 analytic: -10.456860, relative error: 3.037629e-04numerical: -1.346690 analytic: -1.345625, relative error: 3.953276e-04numerical: 7.843501 analytic: 7.846486, relative error: 1.902216e-04numerical: 20.635011 analytic: 20.628368, relative error: 1.609761e-04numerical: 23.654254 analytic: 23.652696, relative error: 3.294745e-05numerical: 37.706709 analytic: 37.703260, relative error: 4.573495e-05numerical: 9.558804 analytic: 9.566079, relative error: 3.804143e-04numerical: 20.450011 analytic: 20.451451, relative error: 3.521650e-05 向量化SVM 套循环肯定是最菜的做法，我们在处理图像的时候肯定都要用矩阵算的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def svm_loss_vectorized(W, X, y, reg): \"\"\" Structured SVM loss function, vectorized implementation. Inputs and outputs are the same as svm_loss_naive. \"\"\" loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero ############################################################################# # TODO: # # Implement a vectorized version of the structured SVM loss, storing the # # result in loss. # ############################################################################# #scores (N,C) scores = X.dot(W) #num_classes = W.shape[1] num_train = X.shape[0] #利用np.arange(),correct_class_score变成了 (num_train,y)的矩阵 correct_class_score = scores[np.arange(num_train),y] correct_class_score = np.reshape(correct_class_score,(num_train,-1)) margins = scores - correct_class_score + 1 margins = np.maximum(0, margins) #然后这里计算了j=y[i]的情形，所以把他们置为0 margins[np.arange(num_train),y] = 0 loss += np.sum(margins) / num_train loss += reg * np.sum( W * W) ############################################################################# # END OF YOUR CODE # ############################################################################# ############################################################################# # TODO: # # Implement a vectorized version of the gradient for the structured SVM # # loss, storing the result in dW. # # # # Hint: Instead of computing the gradient from scratch, it may be easier # # to reuse some of the intermediate values that you used to compute the # # loss. # ############################################################################# margins[margins &gt; 0] = 1 #因为j=y[i]的那一个元素的grad要计算 &gt;0 的那些次数次 row_sum = np.sum(margins,axis=1) margins[np.arange(num_train),y] = -row_sum.T #把公式1和2合到一起计算了 dW = np.dot(X.T,margins) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW 计算一下两者的时间差： 123Naive loss: 8.577034e+00 computed in 0.084761sVectorized loss: 8.577034e+00 computed in 0.001029sdifference: -0.000000 123Naive loss and gradient: computed in 0.082744sVectorized loss and gradient: computed in 0.002027sdifference: 0.000000 Stochastic Gradient Descent 编辑一下classifiers/linear_classifier/LinearClassifier.train() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172class LinearClassifier(object): def __init__(self): self.W = None def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): \"\"\" Train this linear classifier using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label 0 &lt;= c &lt; C for C classes. - learning_rate: (float) learning rate for optimization. - reg: (float) regularization strength. - num_iters: (integer) number of steps to take when optimizing - batch_size: (integer) number of training examples to use at each step. - verbose: (boolean) If true, print progress during optimization. Outputs: A list containing the value of the loss function at each training iteration. \"\"\" num_train, dim = X.shape num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes if self.W is None: # lazily initialize W self.W = 0.001 * np.random.randn(dim, num_classes) # Run stochastic gradient descent to optimize W loss_history = [] for it in xrange(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: # # Sample batch_size elements from the training data and their # # corresponding labels to use in this round of gradient descent. # # Store the data in X_batch and their corresponding labels in # # y_batch; after sampling X_batch should have shape (dim, batch_size) # # and y_batch should have shape (batch_size,) # # # # Hint: Use np.random.choice to generate indices. Sampling with # # replacement is faster than sampling without replacement. # ######################################################################### batch_inx = np.random.choice(num_train,batch_size) X_batch = X[batch_inx,:] y_batch = y[batch_inx] ######################################################################### # END OF YOUR CODE # ######################################################################### # evaluate loss and gradient loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) # perform parameter update ######################################################################### # TODO: # # Update the weights using the gradient and the learning rate. # ######################################################################### self.W = self.W - learning_rate * grad ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print('iteration %d / %d: loss %f' % (it, num_iters, loss)) return loss_history 再编辑一下predict函数 12345678910111213141516171819202122232425def predict(self, X): \"\"\" Use the trained weights of this linear classifier to predict labels for data points. Inputs: - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D. Returns: - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional array of length N, and each element is an integer giving the predicted class. \"\"\" y_pred = np.zeros(X.shape[0]) ########################################################################### # TODO: # # Implement this method. Store the predicted labels in y_pred. # ########################################################################### score = X.dot(self.W) y_pred = np.argmax(score,axis=1) ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred 得到预测值 12training accuracy: 0.376633validation accuracy: 0.384000 然后调一调learning_rate和regularization: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of about 0.4 on the validation set.learning_rates = [1e-7, 3e-7,5e-7,9e-7]regularization_strengths = [2.5e4, 1e4,3e4,2e4]# results is dictionary mapping tuples of the form# (learning_rate, regularization_strength) to tuples of the form# (training_accuracy, validation_accuracy). The accuracy is simply the fraction# of data points that are correctly classified.results = &#123;&#125;best_val = -1 # The highest validation accuracy that we have seen so far.best_svm = None # The LinearSVM object that achieved the highest validation rate.################################################################################# TODO: ## Write code that chooses the best hyperparameters by tuning on the validation ## set. For each combination of hyperparameters, train a linear SVM on the ## training set, compute its accuracy on the training and validation sets, and ## store these numbers in the results dictionary. In addition, store the best ## validation accuracy in best_val and the LinearSVM object that achieves this ## accuracy in best_svm. ## ## Hint: You should use a small value for num_iters as you develop your ## validation code so that the SVMs don't take much time to train; once you are ## confident that your validation code works, you should rerun the validation ## code with a larger value for num_iters. #################################################################################for learning_rate in learning_rates: for regularization_strength in regularization_strengths: svm = LinearSVM() loss_hist = svm.train(X_train, y_train, learning_rate=learning_rate, reg=regularization_strength, num_iters=1500, verbose=True) y_train_pred = svm.predict(X_train) y_val_pred = svm.predict(X_val) y_train_acc = np.mean(y_train_pred==y_train) y_val_acc = np.mean(y_val_pred==y_val) results[(learning_rate,regularization_strength)] = [y_train_acc, y_val_acc] if y_val_acc &gt; best_val: best_val = y_val_acc best_svm = svm################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val) 小结 多看看cs231n的note文档 多学习学习grad的推倒","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://fangzh.top/tags/cs231n/"},{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"}]},{"title":"cs231n作业：assignment1 - knn","slug":"cs231n-1h-1","date":"2018-09-26T04:41:15.000Z","updated":"2018-09-30T12:00:35.681Z","comments":true,"path":"2018/cs231n-1h-1/","link":"","permalink":"http://fangzh.top/2018/cs231n-1h-1/","excerpt":"GitHub地址：https://github.com/ZJUFangzh/cs231n 使用KNN算法来完成图像识别，数据集用的是cifar10。","text":"GitHub地址：https://github.com/ZJUFangzh/cs231n 使用KNN算法来完成图像识别，数据集用的是cifar10。 首先看一下数据集的维度 123456789# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print('Training data shape: ', X_train.shape)print('Training labels shape: ', y_train.shape)print('Test data shape: ', X_test.shape)print('Test labels shape: ', y_test.shape) 可以看到，每一张图片是$32×32×3$，训练集有50000张，测试集有10000张 1234Training data shape: (50000, 32, 32, 3)Training labels shape: (50000,)Test data shape: (10000, 32, 32, 3)Test labels shape: (10000,) 为了更够更快的计算，就选5000张做训练，500张做测试就好了 12345678910# Subsample the data for more efficient code execution in this exercisenum_training = 5000mask = list(range(num_training))X_train = X_train[mask]y_train = y_train[mask]num_test = 500mask = list(range(num_test))X_test = X_test[mask]y_test = y_test[mask] 而后把像素拉成3072的行向量 1234# Reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1))X_test = np.reshape(X_test, (X_test.shape[0], -1))print(X_train.shape, X_test.shape) 因为knn不需要训练，所以先存入数据： 1234567from cs231n.classifiers import KNearestNeighbor# Create a kNN classifier instance. # Remember that training a kNN classifier is a noop: # the Classifier simply remembers the data and does no further processing classifier = KNearestNeighbor()classifier.train(X_train, y_train) 然后要修改k_nearest_neighbor.py中的compute_distances_two_loops 这里套了两层循环，也就是比较训练集和测试集的每一张图片的间距： 123456789101112131415161718192021222324252627282930def compute_distances_two_loops(self, X): \"\"\" Compute the distance between each test point in X and each training point in self.X_train using a nested loop over both the training data and the test data. Inputs: - X: A numpy array of shape (num_test, D) containing test data. Returns: - dists: A numpy array of shape (num_test, num_train) where dists[i, j] is the Euclidean distance between the ith test point and the jth training point. \"\"\" num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in xrange(num_test): for j in xrange(num_train): ##################################################################### # TODO: # # Compute the l2 distance between the ith test point and the jth # # training point, and store the result in dists[i, j]. You should # # not use a loop over dimension. # ##################################################################### dists[i][j] = np.sqrt(np.sum(np.square(X[i,:] - self.X_train[j,:]))) ##################################################################### # END OF YOUR CODE # ##################################################################### return dists 得到了一个$(500,5000)$的dists矩阵。 然后修改predict_labels函数 123456789101112131415161718192021222324252627282930313233343536373839404142def predict_labels(self, dists, k=1): \"\"\" Given a matrix of distances between test points and training points, predict a label for each test point. Inputs: - dists: A numpy array of shape (num_test, num_train) where dists[i, j] gives the distance betwen the ith test point and the jth training point. Returns: - y: A numpy array of shape (num_test,) containing predicted labels for the test data, where y[i] is the predicted label for the test point X[i]. \"\"\" num_test = dists.shape[0] y_pred = np.zeros(num_test) for i in xrange(num_test): # A list of length k storing the labels of the k nearest neighbors to # the ith test point. closest_y = [] ######################################################################### # TODO: # # Use the distance matrix to find the k nearest neighbors of the ith # # testing point, and use self.y_train to find the labels of these # # neighbors. Store these labels in closest_y. # # Hint: Look up the function numpy.argsort. # ######################################################################### #找到每一个测试图片中对应的5000张训练集图片，距离最近的前k个 closest_y = self.y_train[np.argsort(dists[i])[:k]] ######################################################################### # TODO: # # Now that you have found the labels of the k nearest neighbors, you # # need to find the most common label in the list closest_y of labels. # # Store this label in y_pred[i]. Break ties by choosing the smaller # # label. # ######################################################################### #然后将这K个图片进行投票，得票数最多的就是预测值 y_pred[i] = np.argmax(np.bincount(closest_y)) ######################################################################### # END OF YOUR CODE # ######################################################################### return y_pred 预测一下： 12345678# Now implement the function predict_labels and run the code below:# We use k = 1 (which is Nearest Neighbor).y_test_pred = classifier.predict_labels(dists, k=1)# Compute and print the fraction of correctly predicted examplesnum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint('Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)) 结果是0.274 再试试k=5的结果，是0.278 然后再修改compute_distances_one_loop函数，这次争取只用一个循环 12345678910111213141516171819202122def compute_distances_one_loop(self, X): \"\"\" Compute the distance between each test point in X and each training point in self.X_train using a single loop over the test data. Input / Output: Same as compute_distances_two_loops \"\"\" num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in xrange(num_test): ####################################################################### # TODO: # # Compute the l2 distance between the ith test point and all training # # points, and store the result in dists[i, :]. # ####################################################################### #利用python的广播，一次性算出每一张图片与5000张图片的距离 dists[i, :] = np.sqrt(np.sum(np.square(self.X_train - X[i, :]),axis=1)) ####################################################################### # END OF YOUR CODE # ####################################################################### return dists 验证一下间距是 12Difference was: 0.000000Good! The distance matrices are the same 然后争取不用循环compute_distances_no_loops，这一步比较难，想法是利用平方差公式$(x-y)^2 = x^2 + y^2 - 2xy$，使用矩阵乘法和二次广播，直接算出距离，注意矩阵的维度 12345678910111213141516171819202122232425262728293031def compute_distances_no_loops(self, X): \"\"\" Compute the distance between each test point in X and each training point in self.X_train using no explicit loops. Input / Output: Same as compute_distances_two_loops \"\"\" num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) ######################################################################### # TODO: # # Compute the l2 distance between all test points and all training # # points without using any explicit loops, and store the result in # # dists. # # # # You should implement this function using only basic array operations; # # in particular you should not use functions from scipy. # # # # HINT: Try to formulate the l2 distance using matrix multiplication # # and two broadcast sums. # ######################################################################### temp_2xy = np.dot(X,self.X_train.T) * (-2) temp_x2 = np.sum(np.square(X),axis=1,keepdims=True) temp_y2 = np.sum(np.square(self.X_train),axis=1) dists = temp_x2 + temp_2xy + temp_y2 dists = np.sqrt(dists) ######################################################################### # END OF YOUR CODE # ######################################################################### return dists 对比一下三种方法的时间，我这里不知道为什么two比one短，理论上是循环越少时间越短： 123Two loop version took 24.510484 secondsOne loop version took 56.412211 secondsNo loop version took 0.183508 seconds 交叉验证 用交叉验证来找到最好的k 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859num_folds = 5k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]X_train_folds = []y_train_folds = []################################################################################# TODO: ## Split up the training data into folds. After splitting, X_train_folds and ## y_train_folds should each be lists of length num_folds, where ## y_train_folds[i] is the label vector for the points in X_train_folds[i]. ## Hint: Look up the numpy array_split function. #################################################################################X_train_folds = np.array_split(X_train, num_folds)y_train_folds = np.array_split(y_train, num_folds)################################################################################# END OF YOUR CODE ################################################################################## A dictionary holding the accuracies for different values of k that we find# when running cross-validation. After running cross-validation,# k_to_accuracies[k] should be a list of length num_folds giving the different# accuracy values that we found when using that value of k.k_to_accuracies = &#123;&#125;################################################################################# TODO: ## Perform k-fold cross validation to find the best value of k. For each ## possible value of k, run the k-nearest-neighbor algorithm num_folds times, ## where in each case you use all but one of the folds as training data and the ## last fold as a validation set. Store the accuracies for all fold and all ## values of k in the k_to_accuracies dictionary. #################################################################################classifier = KNearestNeighbor()for k in k_choices: accuracies = [] for fold in range(num_folds): temp_X = X_train_folds[:] temp_y = y_train_folds[:] X_val_fold = temp_X.pop(fold) y_val_fold = temp_y.pop(fold) temp_X = np.array([y for x in temp_X for y in x]) temp_y = np.array([y for x in temp_y for y in x]) classifier.train(temp_X,temp_y) y_val_pred = classifier.predict(X_val_fold,k=k) num_correct = np.sum(y_val_fold == y_val_pred) accuracies.append(num_correct / y_val_fold.shape[0]) k_to_accuracies[k] = accuracies ################################################################################# END OF YOUR CODE ################################################################################## Print out the computed accuraciesfor k in sorted(k_to_accuracies): for accuracy in k_to_accuracies[k]: print('k = %d, accuracy = %f' % (k, accuracy)) 画个图： 12345678910111213# plot the raw observationsfor k in k_choices: accuracies = k_to_accuracies[k] plt.scatter([k] * len(accuracies), accuracies)# plot the trend line with error bars that correspond to standard deviationaccuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)plt.title('Cross-validation on k')plt.xlabel('k')plt.ylabel('Cross-validation accuracy')plt.show() 12345678910111213# Based on the cross-validation results above, choose the best value for k, # retrain the classifier using all the training data, and test it on the test# data. You should be able to get above 28% accuracy on the test data.best_k = 10classifier = KNearestNeighbor()classifier.train(X_train, y_train)y_test_pred = classifier.predict(X_test, k=best_k)# Compute and display the accuracynum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint('Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)) 得到最好的k=10，准确率是0.282 小结 cs231n的作业比DeepLearning.ai的难多了，不是一个档次的，关键是提示比较少，所以自己做起来比较费劲 主要要学会向量化的运算，少用loop循环 knn已经被淘汰了，这个作业只是让我们入门看看图像识别大概怎么做","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://fangzh.top/tags/cs231n/"},{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"}]},{"title":"DeepLearning.ai笔记:(3-2)-- 机器学习策略(2)(ML strategy)","slug":"dl-ai-3-2","date":"2018-09-20T09:59:04.000Z","updated":"2018-09-30T07:35:28.628Z","comments":true,"path":"2018/2018092017/","link":"","permalink":"http://fangzh.top/2018/2018092017/","excerpt":"这周继续讲了机器学习策略,包括误差分析、错误样本清楚、数据分布不同、迁移学习、多任务学习等。","text":"这周继续讲了机器学习策略,包括误差分析、错误样本清楚、数据分布不同、迁移学习、多任务学习等。 误差分析对于训练后的模型，如果不进行误差分析，那么很难提升精度。所以应该在验证集中，找到标记错误的那些样本，统计一下都是因为什么原因出现的错误，如是不是照片模糊，还是本来是猫把它标记成狗了等等。 清除错误标记样本如果是随机的误差，也就是人为标记样本出现了随机错误，那么没有关系，因为算法对随即误差还是很有鲁棒性的。 如果是系统误差，那没办法了。 比如说总体误差是10%，然后发现因为人工错误标记引起的误差是0.6%，那么其他原因造成的误差就是9.4%，这个时候应该集中精力去找那9.4%的误差原因，并进行修正。 快速搭建系统对于一个项目来说，我们一开始不要想得太复杂，先快速搭建一个基本的系统，进行迭代，然后在慢慢分析，逐步提高，不要想着一步到位，这样子往往会难以入手。 不同分布的训练和测试假设你在网上找到了20万张照片去分析，但是我们实际上要测试的是用户在手机拍摄情况下的准确度。但是问题是手机上拍摄的数据不足，假设只有1万张。也就是训练集和测试集不是在同一分布，那么怎么办呢？ 显然，如果把21万张照片加在一起，重新分配，是不合理的，因为这样子你验证集和测试集上的数据显然很少是手机拍摄的。 所以，应该用20万张照片，再加上5000张照片作为训练集，然后把剩下来的5000张照片对半分为验证集和测试集，那样子才更为符合实际情况。 不同分布的偏差和方差如上述情况，你的训练集和验证测试集不同一分布的，假设training error：1%，dev error：10%，那么这个时候能说是方差太大吗，显然是不合理的，因为不是同一分布的。 那么这个时候应该重新定义一个集合，叫做训练验证集：train-dev 也就是在训练集中拿出一部分数据，跟验证集合在一起，不参与训练，这样我们就得到了：training error：1%，training-dev error：9%，dev error：10%，如果是这种情况，这样才能说是方差问题。 如果是training error：1%，training-dev error：1.5%，dev error：10%，那么，显然不是因为方差问题，而是因为分布不同而导致的。 如何解决呢？ 进行人工误差分析，看一看训练集和测试集的差别到底在哪里，比如是不是有噪音、照片模糊等等 然后把训练集搞得更像测试集，也就是多收集点类似于测试集的数据，或者通过人工合成技术，把噪声加上去。 迁移学习如果我们现在训练了一个猫的分类器，然后这个时候有了新任务，要识别红绿灯，问题是，我们没有那么多红绿灯的照片，没有那么多的数据，那怎么办？这时候就可以把这个猫分类器学习的参数迁移到红绿灯分类器中，只要输出层的微调就行了。因为图像识别的神经网络，在前面的网络大多是进行一些特征提取，所以如果进行图像识别的迁移，还是很有帮助的！ 但是迁移学习有限制： 必须是相关的类型，比如都是图像识别，都是语音识别 A的数据远大于B，如果B的数据够多，那自己从头开始学不就好了 Muti-task多任务学习假设在自动驾驶中，需要同时检测很多物体，比如人、红绿灯，汽车等等。 那么就可以把这些都写到一个向量中： 如图，$y = [0 1 1 0]$即表示同时有车和停车标志。 这个又和softmax不同，softmax一次只识别一种物体，而多任务学习一次可以识别多种物体。 这个时候的loss funtion 和logistic是一样的： 如果在标注样本中，只标注了每张图片的一部分，比如说图片中有行人和车，只标注的行人，有没有车是不知道的，那么可以设为问号$y = [1 0 ? 0]$，这样也是可以训练的，但是在计算loss的时候，要把这个未标记的部分扣除，不要计算在内。 端到端学习假如我们进行公司门禁，需要刷脸进入，那么这时候算法需要分成两步， 首先检测到你这个人，然后找到人脸的位置 把人脸图像方法，然后在放入模型中计算是否匹配 而端到端学习则直接忽略的这个过程，直接拍一张照片放入模型，输出结果。 再比如说语音识别的时候，在数据少的情况下，我们可能需要 提取声音 分析语法 切分成一个个发声字母 组成句子 翻译 而端到端学习直接是：提取声音—&gt;翻译 就不需要人为的过多干预了，因为机器可以学到的比人为规定的还要好。 但是注意一点是，需要很大量的数据的时候才能进行端到端学习；如果数据很少，那么还是手动干预，设计一些组件效果会好一点。","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai笔记:(3-1)-- 机器学习策略(1)(ML strategy)","slug":"dl-ai-3-1","date":"2018-09-20T08:48:41.000Z","updated":"2018-09-30T07:35:04.197Z","comments":true,"path":"2018/2018092016/","link":"","permalink":"http://fangzh.top/2018/2018092016/","excerpt":"第三门课主要讲了机器学习的一些策略，也就是在你做项目的时候，应该要具体根据什么来改进你的模型。","text":"第三门课主要讲了机器学习的一些策略，也就是在你做项目的时候，应该要具体根据什么来改进你的模型。 正交化在设计过程中，最好是保证几个变量相互独立，也就是正交。就好比你在开车的时候，油门和方向盘是相互独立的。如果方向盘和油门不独立，当你调整方向盘的时候速度也在变化，就很难受了。 所以在监督学习中，以下几个应该正交： 损失函数应该在训练集上表现很好 否则，就使用更大的神经网络，或者使用更好的优化算法 在验证集上表现很好 否则，就用正则化或者训练集上要更多的数据 在测试机上表现很好 否则，就使用更大的验证集 现实中表现很好 否则，就检查一下验证集是不是对的，损失函数是不是好的 单一数字评估指标在训练模型中，当然需要一种指标来评估一下模型是不是好的。 一般使用两个参数： 准确率p：在预测的数据中，是正确的概率 召回率r：在真实数据中，预测是正确的概率 一般用F1 Score把两个指标给统一起来： $$F1-Score = \\frac{2}{\\frac{1}{p} + \\frac{1}{r}}$$ 满足和优化指标一般，满足指标都是一个区间范围，比如时间上只要小于100ms就可以，这样子，就在满足满足指标的情况下，选择最优指标（如精确度最高）最好的那个模型。 训练/验证/测试集的划分应该使验证集和测试集的数据满足统一分布。 与人类表现比较可避免的偏差 我们训练出来的结果，应该和人类表现作比较，如果差距比较小，那么说明很接近了，如果差距比较大，应该着重优化缩小这个可避免的偏差。 如图，左边说明应该着重减小bias，右边应该着重减小variance 改善模型的表现减少bias： 训练更大的模型 更长的时间，更优化的算法（Momentum，RMSprop，Adam） 寻找更好的网络架构、更好的参数 减少variance： 收集更多的数据 正则化 更好的架构和参数","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(2-3)-- 超参数调试（Hyperparameter tuning）","slug":"dl-ai-2-3h","date":"2018-09-18T02:35:32.000Z","updated":"2018-09-30T07:33:54.479Z","comments":true,"path":"2018/2018091810/","link":"","permalink":"http://fangzh.top/2018/2018091810/","excerpt":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 本周主要是TensorFlow的简单教程，没什么好说的，可以去看看更详细一点的教程。","text":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 本周主要是TensorFlow的简单教程，没什么好说的，可以去看看更详细一点的教程。 1234567891011121314151617181920212223242526272829303132# GRADED FUNCTION: linear_functiondef linear_function(): \"\"\" Implements a linear function: Initializes W to be a random tensor of shape (4,3) Initializes X to be a random tensor of shape (3,1) Initializes b to be a random tensor of shape (4,1) Returns: result -- runs the session for Y = WX + b \"\"\" np.random.seed(1) ### START CODE HERE ### (4 lines of code) X = tf.constant(np.random.randn(3,1), name = \"X\") W = tf.constant(np.random.randn(4,3), name = \"W\") b = tf.constant(np.random.randn(4,1), name = \"b\") Y = tf.matmul(W,X) + b ### END CODE HERE ### # Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate ### START CODE HERE ### sess = tf.Session() result = sess.run(Y) ### END CODE HERE ### # close the session sess.close() return result 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: sigmoiddef sigmoid(z): \"\"\" Computes the sigmoid of z Arguments: z -- input value, scalar or vector Returns: results -- the sigmoid of z \"\"\" ### START CODE HERE ### ( approx. 4 lines of code) # Create a placeholder for x. Name it 'x'. x = tf.placeholder(tf.float32,name=\"x\") # compute sigmoid(x) sigmoid = tf.sigmoid(x) # Create a session, and run it. Please use the method 2 explained above. # You should use a feed_dict to pass z's value to x. with tf.Session() as sess: # Run session and call the output \"result\" result = sess.run(sigmoid,feed_dict=&#123;x:z&#125;) ### END CODE HERE ### return result 1234567891011121314151617181920212223242526272829303132333435363738# GRADED FUNCTION: costdef cost(logits, labels): \"\"\" Computes the cost using the sigmoid cross entropy Arguments: logits -- vector containing z, output of the last linear unit (before the final sigmoid activation) labels -- vector of labels y (1 or 0) Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" in the TensorFlow documentation. So logits will feed into z, and labels into y. Returns: cost -- runs the session of the cost (formula (2)) \"\"\" ### START CODE HERE ### # Create the placeholders for \"logits\" (z) and \"labels\" (y) (approx. 2 lines) z = tf.placeholder(tf.float32,name=\"z\") y = tf.placeholder(tf.float32,name=\"y\") # Use the loss function (approx. 1 line) cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=y) # Create a session (approx. 1 line). See method 1 above. sess = tf.Session() # Run the session (approx. 1 line). cost = sess.run(cost,feed_dict=&#123;z:logits,y:labels&#125;) # Close the session (approx. 1 line). See method 1 above. sess.close() ### END CODE HERE ### return cost 123456789101112131415161718192021222324252627282930313233343536# GRADED FUNCTION: one_hot_matrixdef one_hot_matrix(labels, C): \"\"\" Creates a matrix where the i-th row corresponds to the ith class number and the jth column corresponds to the jth training example. So if example j had a label i. Then entry (i,j) will be 1. Arguments: labels -- vector containing the labels C -- number of classes, the depth of the one hot dimension Returns: one_hot -- one hot matrix \"\"\" ### START CODE HERE ### # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line) C = tf.constant(C) # Use tf.one_hot, be careful with the axis (approx. 1 line) one_hot_matrix = tf.one_hot(labels, C, axis = 0) # Create the session (approx. 1 line) sess = tf.Session() # Run the session (approx. 1 line) one_hot = sess.run(one_hot_matrix) # Close the session (approx. 1 line). See method 1 above. sess.close() ### END CODE HERE ### return one_hot 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: onesdef ones(shape): \"\"\" Creates an array of ones of dimension shape Arguments: shape -- shape of the array you want to create Returns: ones -- array containing only ones \"\"\" ### START CODE HERE ### # Create \"ones\" tensor using tf.ones(...). (approx. 1 line) ones = tf.ones(shape) # Create the session (approx. 1 line) sess = tf.Session() # Run the session to compute 'ones' (approx. 1 line) ones = sess.run(ones) # Close the session (approx. 1 line). See method 1 above. sess.close() ### END CODE HERE ### return ones Building neural network12345678910111213141516171819202122232425# GRADED FUNCTION: create_placeholdersdef create_placeholders(n_x, n_y): \"\"\" Creates the placeholders for the tensorflow session. Arguments: n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288) n_y -- scalar, number of classes (from 0 to 5, so -&gt; 6) Returns: X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\" Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\" Tips: - You will use None because it let's us be flexible on the number of examples you will for the placeholders. In fact, the number of examples during test/train is different. \"\"\" ### START CODE HERE ### (approx. 2 lines) X = tf.placeholder(tf.float32,[n_x,None]) Y = tf.placeholder(tf.float32,[n_y,None]) ### END CODE HERE ### return X, Y 1234567891011121314151617181920212223242526272829303132333435# GRADED FUNCTION: initialize_parametersdef initialize_parameters(): \"\"\" Initializes parameters to build a neural network with tensorflow. The shapes are: W1 : [25, 12288] b1 : [25, 1] W2 : [12, 25] b2 : [12, 1] W3 : [6, 12] b3 : [6, 1] Returns: parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3 \"\"\" tf.set_random_seed(1) # so that your \"random\" numbers match ours ### START CODE HERE ### (approx. 6 lines of code) W1 = tf.get_variable(\"W1\", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1)) b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer()) W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1)) b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer()) W3 = tf.get_variable(\"W3\", [6,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1)) b3 = tf.get_variable(\"b3\", [6,1], initializer = tf.zeros_initializer()) ### END CODE HERE ### parameters = &#123;\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3&#125; return parameters 1234567891011121314151617181920212223242526272829303132# GRADED FUNCTION: forward_propagationdef forward_propagation(X, parameters): \"\"\" Implements the forward propagation for the model: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX Arguments: X -- input dataset placeholder, of shape (input size, number of examples) parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\" the shapes are given in initialize_parameters Returns: Z3 -- the output of the last LINEAR unit \"\"\" # Retrieve the parameters from the dictionary \"parameters\" W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] W3 = parameters['W3'] b3 = parameters['b3'] ### START CODE HERE ### (approx. 5 lines) # Numpy Equivalents: Z1 = tf.matmul(W1,X) + b1 # Z1 = np.dot(W1, X) + b1 A1 = tf.nn.relu(Z1) # A1 = relu(Z1) Z2 = tf.matmul(W2,A1) + b2 # Z2 = np.dot(W2, a1) + b2 A2 = tf.nn.relu(Z2) # A2 = relu(Z2) Z3 = tf.matmul(W3,A2) + b3 # Z3 = np.dot(W3,Z2) + b3 ### END CODE HERE ### return Z3 1234567891011121314151617181920212223# GRADED FUNCTION: compute_cost def compute_cost(Z3, Y): \"\"\" Computes the cost Arguments: Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples) Y -- \"true\" labels vector placeholder, same shape as Z3 Returns: cost - Tensor of the cost function \"\"\" # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...) logits = tf.transpose(Z3) labels = tf.transpose(Y) ### START CODE HERE ### (1 line of code) cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =logits, labels = labels)) ### END CODE HERE ### return cost 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 32, print_cost = True): \"\"\" Implements a three-layer tensorflow neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX. Arguments: X_train -- training set, of shape (input size = 12288, number of training examples = 1080) Y_train -- test set, of shape (output size = 6, number of training examples = 1080) X_test -- training set, of shape (input size = 12288, number of training examples = 120) Y_test -- test set, of shape (output size = 6, number of test examples = 120) learning_rate -- learning rate of the optimization num_epochs -- number of epochs of the optimization loop minibatch_size -- size of a minibatch print_cost -- True to print the cost every 100 epochs Returns: parameters -- parameters learnt by the model. They can then be used to predict. \"\"\" ops.reset_default_graph() # to be able to rerun the model without overwriting tf variables tf.set_random_seed(1) # to keep consistent results seed = 3 # to keep consistent results (n_x, m) = X_train.shape # (n_x: input size, m : number of examples in the train set) n_y = Y_train.shape[0] # n_y : output size costs = [] # To keep track of the cost # Create Placeholders of shape (n_x, n_y) ### START CODE HERE ### (1 line) X, Y = create_placeholders(n_x,n_y) ### END CODE HERE ### # Initialize parameters ### START CODE HERE ### (1 line) parameters = initialize_parameters() ### END CODE HERE ### # Forward propagation: Build the forward propagation in the tensorflow graph ### START CODE HERE ### (1 line) Z3 = forward_propagation(X, parameters) ### END CODE HERE ### # Cost function: Add cost function to tensorflow graph ### START CODE HERE ### (1 line) cost = compute_cost(Z3, Y) ### END CODE HERE ### # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer. ### START CODE HERE ### (1 line) optimizer = optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost) ### END CODE HERE ### # Initialize all the variables init = tf.global_variables_initializer() # Start the session to compute the tensorflow graph with tf.Session() as sess: # Run the initialization sess.run(init) # Do the training loop for epoch in range(num_epochs): epoch_cost = 0. # Defines a cost related to an epoch num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set seed = seed + 1 minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed) for minibatch in minibatches: # Select a minibatch (minibatch_X, minibatch_Y) = minibatch # IMPORTANT: The line that runs the graph on a minibatch. # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y). ### START CODE HERE ### (1 line) _ , minibatch_cost = sess.run([optimizer, cost], feed_dict = &#123;X: minibatch_X, Y: minibatch_Y&#125;) ### END CODE HERE ### epoch_cost += minibatch_cost / num_minibatches # Print the cost every epoch if print_cost == True and epoch % 100 == 0: print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost)) if print_cost == True and epoch % 5 == 0: costs.append(epoch_cost) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() # lets save the parameters in a variable parameters = sess.run(parameters) print (\"Parameters have been trained!\") # Calculate the correct predictions correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y)) # Calculate accuracy on the test set accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) print (\"Train Accuracy:\", accuracy.eval(&#123;X: X_train, Y: Y_train&#125;)) print (\"Test Accuracy:\", accuracy.eval(&#123;X: X_test, Y: Y_test&#125;)) return parameters","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"},{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai笔记:(2-3)-- 超参数调试（Hyperparameter tuning）","slug":"dl-ai-2-3","date":"2018-09-17T12:19:55.000Z","updated":"2018-09-30T07:33:32.709Z","comments":true,"path":"2018/2018091720/","link":"","permalink":"http://fangzh.top/2018/2018091720/","excerpt":"这周主要讲了这些超参数调试的方法以及batch norm，还有softmax多分类函数的使用。","text":"这周主要讲了这些超参数调试的方法以及batch norm，还有softmax多分类函数的使用。 调试处理之前提到的超参数有： $\\alpha$ hidden units minibatch size $\\beta$(Momentum) layers learning rate decay $\\beta_1,\\beta_2,\\epsilon$ 颜色代表重要性。 在调参中，常用的方式是在网格中取不同的点，然后计算这些点中的最佳值， 但是左边是均匀的选点，这样有可能导致在某一个参数上变化很小，浪费计算时间，所以应该更推荐右边的选点方法，即随机选点。 而后，当随机选点选到几个结果比较好的点时，逐步缩小范围，进行更精细的选取。 超参数的合适范围当然，随机采样并不是在轴上均匀的采样。 比如说$\\alpha = 0.001 — 1$，这样子，那么在$0.1-1$的部分占了90%的概率，显然是不合理的，所以应该将区间对数化，转化成$[0.001,0.01],[0.01,0.1],[0.1,1]$的区间，这样更为合理。思路是：$10^{-3} = 0.001$，所以取值从$[10^{-3},10^{0}]$，我们只要将指数随机就可以了。 12r = -3*np.random.rand() # rand()表示在 [0，1]随机取样，再乘以系数，就可以得到[-3,0]a = 10**r 同理,$\\beta = 0.9 ,…..,0.999$ 通过$1-\\beta = 0.1,….,0.001$，所以$1-\\beta = 10^{r}$，$\\beta = 1-10^{r}$ 归一化网络的激活函数我们之前是将输入的数据X归一化，可以加速训练，其实在神经网络中，也可以同样归一化，一般是对$z^{[l]}$归一化。 这个方法叫做 batch norm 公式是： 加上$\\epsilon$是为了不至于除以0 而一般标准化后还会加上两个参数，来表示新的方差$\\gamma$和均值$\\beta$： $\\gamma$和$\\beta$也是参数，和$w,b$一样，可以在学习中进行更新。 将batch norm 放入神经网络 可以看到， 先求的$z^{[1]}$，再进行batch norm，加上参数$\\beta^{[1]},\\gamma^{[1]}$，得到${\\tilde{z}}^{[1]}$,再根据activation function得到$a^{[1]}$ batch norm同样适用于Momentum、RMSprop 、Adam的梯度下降法来进行更新。 Batch Norm为什么有用？如果我们的图片中训练的都是黑猫，这个时候给你一些橘猫的图片，那么大概率是训练不好的。因为相当于样本集合的分布改变了，batch norm就可以解决这个问题。 如果这个时候要计算第三层，那么很显然计算结果是依赖第二层的数据的。但是如果我们对第二层的数据进行了归一化，那么就可以将第二层的均值和方差都限制在同一分布，而且这两个参数是自动学习的。也就是归一化后的数据可以减弱前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习。 还有就是batch norm在某种程度上有正则化的效果，因为归一化会使各个层之间的依赖性降低，而且归一化有带来一定的噪声，有点像dropout。 测试集的batch normbatch norm是在训练集上得到的，那么怎么把它应用在测试集呢？ 这个时候可以直接从训练集中拿到$\\mu$和$\\sigma^{2}$ 使用指数加权平均，在每一步中保留$\\mu$和$\\sigma^{2}$，就可以得到训练后的$\\mu$和$\\sigma^{2}$ softmax之前说的都是二分类问题，如何解决多分类问题呢？ 可以用softmax算法来解决。 前面的步骤都一样，而到了最后一层output layer，你想要分为多少类，就用多少个神经元。 这个时候，最后一层的activation function就变成了： $a^{[l]}_i$就表示了每一个分类的概率。 计算例子如图： 而它的损失函数用的也是cross-entropy： 最终得到一个关于Y的矩阵： 其实是可以证明，当分类为2时，softmax就是logistic regression","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(2-2)-- 优化算法（Optimization algorithms）","slug":"dl-ai-2-2h","date":"2018-09-17T03:06:06.000Z","updated":"2018-09-30T07:26:21.628Z","comments":true,"path":"2018/2018091711/","link":"","permalink":"http://fangzh.top/2018/2018091711/","excerpt":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 本周作业实践了课上的各种优化算法： mini-batch momentum Adam","text":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 本周作业实践了课上的各种优化算法： mini-batch momentum Adam 首先是标准的gradient descent： 123456789101112131415161718192021222324252627def update_parameters_with_gd(parameters, grads, learning_rate): \"\"\" Update parameters using one step of gradient descent Arguments: parameters -- python dictionary containing your parameters to be updated: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients to update each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl learning_rate -- the learning rate, scalar. Returns: parameters -- python dictionary containing your updated parameters \"\"\" L = len(parameters) // 2 # number of layers in the neural networks # Update rule for each parameter for l in range(L): ### START CODE HERE ### (approx. 2 lines) parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads['dW' + str(l+1)] parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads['db' + str(l+1)] ### END CODE HERE ### return parameters mini-batch步骤是： shuffle：将数据随机打乱，使用np.random.permutation(m)函数可以把m个样本的顺序重新映射，变成一个len为m的列表，里面的值就是映射原本的顺序。 再根据size大小进行分区，需要注意的是最后的数据有可能小于size大小的，因为可能无法整除，要单独考虑 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: random_mini_batchesdef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0): \"\"\" Creates a list of random minibatches from (X, Y) Arguments: X -- input data, of shape (input size, number of examples) Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples) mini_batch_size -- size of the mini-batches, integer Returns: mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y) \"\"\" np.random.seed(seed) # To make your \"random\" minibatches the same as ours m = X.shape[1] # number of training examples mini_batches = [] # Step 1: Shuffle (X, Y) permutation = list(np.random.permutation(m)) print(permutation) shuffled_X = X[:, permutation] shuffled_Y = Y[:, permutation].reshape((1,m)) # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case. num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning for k in range(0, num_complete_minibatches): ### START CODE HERE ### (approx. 2 lines) mini_batch_X = shuffled_X[:,k * mini_batch_size:(k+1)* mini_batch_size] mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k+1)* mini_batch_size] ### END CODE HERE ### mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) # Handling the end case (last mini-batch &lt; mini_batch_size) if m % mini_batch_size != 0: ### START CODE HERE ### (approx. 2 lines) mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:] mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:] ### END CODE HERE ### mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) return mini_batches Momentum先初始化为0， 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: initialize_velocitydef initialize_velocity(parameters): \"\"\" Initializes the velocity as a python dictionary with: - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters. Arguments: parameters -- python dictionary containing your parameters. parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl Returns: v -- python dictionary containing the current velocity. v['dW' + str(l)] = velocity of dWl v['db' + str(l)] = velocity of dbl \"\"\" L = len(parameters) // 2 # number of layers in the neural networks v = &#123;&#125; # Initialize velocity for l in range(L): ### START CODE HERE ### (approx. 2 lines) v[\"dW\" + str(l+1)] = np.zeros((parameters['W' + str(l+1) ].shape[0],parameters['W' + str(l+1) ].shape[1])) v[\"db\" + str(l+1)] = np.zeros((parameters['b' + str(l+1) ].shape[0],parameters['b' + str(l+1) ].shape[1])) ### END CODE HERE ### return v 再按公式进行迭代，因为指数加权平均不需要知道前面n个数据，只要一步一步进行迭代，知道当前的数据就行，节省空间。 123456789101112131415161718192021222324252627282930313233343536373839# GRADED FUNCTION: update_parameters_with_momentumdef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate): \"\"\" Update parameters using Momentum Arguments: parameters -- python dictionary containing your parameters: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients for each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl v -- python dictionary containing the current velocity: v['dW' + str(l)] = ... v['db' + str(l)] = ... beta -- the momentum hyperparameter, scalar learning_rate -- the learning rate, scalar Returns: parameters -- python dictionary containing your updated parameters v -- python dictionary containing your updated velocities \"\"\" L = len(parameters) // 2 # number of layers in the neural networks # Momentum update for each parameter for l in range(L): ### START CODE HERE ### (approx. 4 lines) # compute velocities v[\"dW\" + str(l+1)] = beta * v[\"dW\" + str(l+1)] + (1 - beta) * grads[\"dW\" + str(l+1)] v[\"db\" + str(l+1)] = beta * v[\"db\" + str(l+1)] + (1 - beta) * grads[\"db\" + str(l+1)] # update parameters parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)] parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)] ### END CODE HERE ### return parameters, v Adam没什么好说的，先初始化，根据公式来就行了。 1234567891011121314151617181920212223242526272829303132333435def initialize_adam(parameters) : \"\"\" Initializes v and s as two python dictionaries with: - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters. Arguments: parameters -- python dictionary containing your parameters. parameters[\"W\" + str(l)] = Wl parameters[\"b\" + str(l)] = bl Returns: v -- python dictionary that will contain the exponentially weighted average of the gradient. v[\"dW\" + str(l)] = ... v[\"db\" + str(l)] = ... s -- python dictionary that will contain the exponentially weighted average of the squared gradient. s[\"dW\" + str(l)] = ... s[\"db\" + str(l)] = ... \"\"\" L = len(parameters) // 2 # number of layers in the neural networks v = &#123;&#125; s = &#123;&#125; # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\". for l in range(L): ### START CODE HERE ### (approx. 4 lines) v[\"dW\" + str(l+1)] = np.zeros((parameters['W'+str(l+1)].shape[0],parameters['W'+str(l+1)].shape[1])) v[\"db\" + str(l+1)] = np.zeros((parameters['b'+str(l+1)].shape[0],parameters['b'+str(l+1)].shape[1])) s[\"dW\" + str(l+1)] = np.zeros((parameters['W'+str(l+1)].shape[0],parameters['W'+str(l+1)].shape[1])) s[\"db\" + str(l+1)] = np.zeros((parameters['b'+str(l+1)].shape[0],parameters['b'+str(l+1)].shape[1])) ### END CODE HERE ### return v, s 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8): \"\"\" Update parameters using Adam Arguments: parameters -- python dictionary containing your parameters: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients for each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl v -- Adam variable, moving average of the first gradient, python dictionary s -- Adam variable, moving average of the squared gradient, python dictionary learning_rate -- the learning rate, scalar. beta1 -- Exponential decay hyperparameter for the first moment estimates beta2 -- Exponential decay hyperparameter for the second moment estimates epsilon -- hyperparameter preventing division by zero in Adam updates Returns: parameters -- python dictionary containing your updated parameters v -- Adam variable, moving average of the first gradient, python dictionary s -- Adam variable, moving average of the squared gradient, python dictionary \"\"\" L = len(parameters) // 2 # number of layers in the neural networks v_corrected = &#123;&#125; # Initializing first moment estimate, python dictionary s_corrected = &#123;&#125; # Initializing second moment estimate, python dictionary # Perform Adam update on all parameters for l in range(L): # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\". ### START CODE HERE ### (approx. 2 lines) v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1-beta1) * grads['dW' + str(l+1)] v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1-beta1) * grads['db' + str(l+1)] ### END CODE HERE ### # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\". ### START CODE HERE ### (approx. 2 lines) v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1 ** t) v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1 ** t) ### END CODE HERE ### # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\". ### START CODE HERE ### (approx. 2 lines) s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1-beta2) * (grads['dW' + str(l+1)]**2) s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1-beta2) * (grads['db' + str(l+1)]**2) ### END CODE HERE ### # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\". ### START CODE HERE ### (approx. 2 lines) s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t) s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t) ### END CODE HERE ### # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\". ### START CODE HERE ### (approx. 2 lines) parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / (s_corrected[\"dW\" + str(l+1)]**0.5 + epsilon) parameters[\"b\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (s_corrected[\"db\" + str(l+1)]**0.5 + epsilon) ### END CODE HERE ### return parameters, v, s 最后代入模型函数，根据关键字选择需要的优化算法就行了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, num_epochs = 10000, print_cost = True): \"\"\" 3-layer neural network model which can be run in different optimizer modes. Arguments: X -- input data, of shape (2, number of examples) Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples) layers_dims -- python list, containing the size of each layer learning_rate -- the learning rate, scalar. mini_batch_size -- the size of a mini batch beta -- Momentum hyperparameter beta1 -- Exponential decay hyperparameter for the past gradients estimates beta2 -- Exponential decay hyperparameter for the past squared gradients estimates epsilon -- hyperparameter preventing division by zero in Adam updates num_epochs -- number of epochs print_cost -- True to print the cost every 1000 epochs Returns: parameters -- python dictionary containing your updated parameters \"\"\" L = len(layers_dims) # number of layers in the neural networks costs = [] # to keep track of the cost t = 0 # initializing the counter required for Adam update seed = 10 # For grading purposes, so that your \"random\" minibatches are the same as ours # Initialize parameters parameters = initialize_parameters(layers_dims) # Initialize the optimizer if optimizer == \"gd\": pass # no initialization required for gradient descent elif optimizer == \"momentum\": v = initialize_velocity(parameters) elif optimizer == \"adam\": v, s = initialize_adam(parameters) # Optimization loop for i in range(num_epochs): # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch seed = seed + 1 minibatches = random_mini_batches(X, Y, mini_batch_size, seed) for minibatch in minibatches: # Select a minibatch (minibatch_X, minibatch_Y) = minibatch # Forward propagation a3, caches = forward_propagation(minibatch_X, parameters) # Compute cost cost = compute_cost(a3, minibatch_Y) # Backward propagation grads = backward_propagation(minibatch_X, minibatch_Y, caches) # Update parameters if optimizer == \"gd\": parameters = update_parameters_with_gd(parameters, grads, learning_rate) elif optimizer == \"momentum\": parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate) elif optimizer == \"adam\": t = t + 1 # Adam counter parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon) # Print the cost every 1000 epoch if print_cost and i % 1000 == 0: print (\"Cost after epoch %i: %f\" %(i, cost)) if print_cost and i % 100 == 0: costs.append(cost) # plot the cost plt.plot(costs) plt.ylabel('cost') plt.xlabel('epochs (per 100)') plt.title(\"Learning rate = \" + str(learning_rate)) plt.show() return parameters 效果gradient descent gradient descent with momentum Adam mode 效果还是很明显的：","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"},{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai笔记:(2-2)-- 优化算法（Optimization algorithms）","slug":"dl-ai-2-2","date":"2018-09-16T13:42:33.000Z","updated":"2018-09-30T07:24:35.706Z","comments":true,"path":"2018/2018091621/","link":"","permalink":"http://fangzh.top/2018/2018091621/","excerpt":"这周学习了优化算法，可以让神经网络运行的更快。","text":"这周学习了优化算法，可以让神经网络运行的更快。 主要有: mini-batch 动量梯度下降(momentum) RMSprop Adam优化算法 学习率衰减 mini-batch(小批量)原本的梯度下降算法，在每一次的迭代中，要把所有的数据都进行计算再取平均，那如果你的数据量特别大的话，每进行一次迭代就会耗费大量的时间。 所以就有了mini-batch，做小批量的计算迭代。也就是把训练集划分成n等分，比如数据量有500万个的时候，以1000为单位，将数据集划分为5000份，$$x = {x^{\\lbrace 1 \\rbrace},x^{\\lbrace 2 \\rbrace},x^{\\lbrace 3 \\rbrace},…..,x^{\\lbrace 5000 \\rbrace}}$$ 用大括弧表示每一份的mini-batch，其中每一份$x^{\\lbrace t \\rbrace}$都是1000个样本。 这个时候引入epoch的概念，1个epoch相当于是遍历了一次数据集，比如用mini-batch，1个epoch就可以进行5000次迭代，而传统的batch把数据集都一起计算，相当于1个epoch只进行了1次迭代。 具体计算步骤是： 先划分好每一个mini-batch for t in range(5000)，循环每次迭代 循环里面和之前的计算过程一样，前向传播，但每次计算量是1000个样本 计算损失函数 反向传播 更新参数 batch和mini-batch的对比如图： 如果mini-batch的样本为m的话，其实就是batch gradient descent，缺点是如果样本量太大的话，每一次迭代的时间会比较长，但是优点是每一次迭代的损失函数都是下降的，比较平稳。 mini-batch样本为1的话，那就是随机梯度下降（Stochastic gradient descent）,也就是每次迭代只选择其中一个样本进行迭代，但是这样会失去了样本向量化带来的计算加速效果，损失函数总体是下降的，但是局部会很抖动，很可能无法达到全局最小点。 所以选择一个合适的size很重要，$1 &lt; size &lt; m$，可以实现快速的计算效果，也能够享受向量化带来的加速。 mini-batch size的选择 因为电脑的内存和使用方式都是二进制的，而且是2的n次方，所以之前选1000也不太合理，可以选1024，但是1024也比较少见，一般是从64到512。也就是$64、128、256、512$ 指数加权平均(Exponentially weighted averages ) 蓝色的点是每一天的气温，可以看到是非常抖动的，那如果可以把它平均一下，比如把10天内的气温平均一下，就可以得到如红色的曲线。 但是如果是单纯的把前面的10天气温一起平均的话，那么这样你就需要把前10天的气温全部储存记录下来，这样子虽然会更准一点，但是很浪费储存空间，所以就有了指数加权平均这样的概念。方法如下： $$V_0 = 0$$ $$V_1 = \\beta * V_0 + (1 - \\beta) \\theta_1$$ $……$ $$V_t = \\beta * V_{t-1} + (1 - \\beta) \\theta_t$$ 其中，$\\theta_t$表示第t天的温度，而$V_t$表示指数加权平均后的第t天温度，$\\beta$这个参数表示$\\frac{1}{1-\\beta}$天的平均，也就是，$\\beta = 0.9$，表示10天内的平均，$\\beta = 0.98$，表示50天内的平均。 理解指数加权平均我们再来看一下公式： $$v_t = \\beta v_{t-1} + (1 - \\beta) \\theta_t$$ 假设$\\beta = 0.9$，那么 $$v_{100} = 0.9v_{99} + 0.1\\theta_{100}$$ $$v_{99} = 0.9v_{98} + 0.1\\theta_{99}$$ $$v_{98} = 0.9v_{97} + 0.1\\theta_{98}$$ 展开一下，得到： $$ v_{100} = 0.1 \\theta_{100} + 0.1 \\times 0.9 \\times \\theta_{99} + 0.1 \\times 0.9^2 \\times \\theta_{98} + ……$$ 看到没有，每一项都会乘以0.9，这样就是指数加权的意思了，那么为什么表示的是10天内的平均值呢？明明是10天以前的数据都有加进去的才对，其实是因为$0.9^{10} \\approx 0.35 \\approx \\frac{1}{e}$，也就是10天以前的权重只占了三分之一左右，已经很小了，所以我们就可以认为这个权重就是10天内的温度平均，其实有详细的数学证明的，这里就不要证明了，反正理解了$(1-\\epsilon)^{\\frac{1}{\\epsilon}} \\approx \\frac{1}{e}$，$\\epsilon$为0.02的时候，就代表了50天内的数据。 因为指数加权平均不需要知道前面n个数据，只要一步一步进行迭代，知道当前的数据就行，所以非常节省空间。 指数加权平均的偏差修正如果你细心一点，你就会发现其实这个公式有问题， $$V_0 = 0$$ $$V_1 = \\beta * V_0 + (1 - \\beta) \\theta_1$$ $……$ $$V_t = \\beta * V_{t-1} + (1 - \\beta) \\theta_t$$ 如果第一天的温度是40摄氏度，那么$V_1 = 0.1 * 40 = 4$，显然是不合理的。因为初始值$V_0 = 0$，也就是前面几天的数据都会普遍偏低。所以特别是在估测初期，需要进行一些修正，这个时候就不要用$v_t$了，而是用$\\frac{v_t}{1-\\beta^t}$来代表第t天的温度平均，你会发现随着t的增加，$\\beta^t$接近于0，所以偏差修正几乎就没有用了，而t比较小的时候，就非常有效果。 不过在大部分机器学习中，一般也不需要修正，因为只是前面的初始时期比较有偏差而已，到后面就基本不会有偏差了，所以也不太用。 动量梯度下降法 (Gradient descent with Momentum )用动量梯度下降法运行速度总是比标准的梯度下降法要来的快。它的基本思想是计算梯度的指数加权平均数，然后用该梯度来更新权重。 效果如图： 使用动量梯度下降法后，在竖直方向上的抖动减少了，而在水平方向上的运动反而加速了。 算法公式： 可以发现，就是根据指数平均计算出了$v_{dW}$，然后更新参数时把$dW$换成了$v_{dw}$，$\\beta$一般的取值是0.9。可以发现，在纵向的波动经过平均以后，变得非常小了，而因为在横向上，每一次的微分分量都是指向低点，所以平均后的值一直朝着低点前进。 物理意义： 个人的理解是大概这个公式也很像动量的公式$m v = m_1 v_1 + m_2 v_2$，也就是把两个物体合并了得到新物体的质量和速度的意思 理解成速度和加速度，把$v_{dW}$看成速度，$dW$看成加速度，这样每次因为有速度的存在，加速度只能影响到速度的大小而不能够立刻改变速度的方向。 RMSprop（root mean square prop）均方根传播。这是另一种梯度下降的优化算法。 顾名思义，先平方再开根号。 其实和动量梯度下降法公式差不多： 在更新参数的分母项加了一项$\\epsilon = 10^{-8}$,来确保算法不会除以0 Adam算法Adam算法其实就是结合了Momentum和RMSprop ，注意这个时候要加上偏差修正： 初始化参数：$v_{dW} = 0$，$S_{dW} =0$，$v_{db} = 0$，$S_{db} =0$ 在第$t$次迭代中， 计算mini-batch的dW,db Momentum: $v_{dW}= \\beta_{1}v_{dW} + ( 1 - \\beta_{1})dW$，$v_{db}= \\beta_{1}v_{db} + ( 1 -\\beta_{1} ){db}$ RMSprop:$S_{dW}=\\beta_{2}S_{dW} + ( 1 - \\beta_{2}){(dW)}^{2}$，$S_{db} =\\beta_{2}S_{db} + \\left( 1 - \\beta_{2} \\right){(db)}^{2}$ $v_{dW}^{\\text{corrected}}= \\frac{v_{dW}}{1 - \\beta_{1}^{t}}$，$v_{db}^{\\text{corrected}} =\\frac{v_{db}}{1 -\\beta_{1}^{t}}$ $S_{dW}^{\\text{corrected}} =\\frac{S_{dW}}{1 - \\beta_{2}^{t}}$，$S_{db}^{\\text{corrected}} =\\frac{S_{db}}{1 - \\beta_{2}^{t}}$ $W:= W - \\frac{a v_{dW}^{\\text{corrected}}}{\\sqrt{S_{dW}^{\\text{corrected}}} +\\varepsilon}$ 超参数有$\\alpha,\\beta_1,\\beta_2,\\epsilon$，一般$\\beta_1 = 0.9,\\beta_2 = 0.999,\\epsilon = 10^{-8}$ 学习率衰减在梯度下降时，如果是固定的学习率$\\alpha$，在到达最小值附近的时候，可能不会精确收敛，会很抖动，因此很难达到最小值，所以可以考虑学习率衰减，在迭代过程中，逐渐减小$\\alpha$，这样一开始比较快，后来慢慢的变慢。 常用的是： $$a= \\frac{1}{1 + decayrate * \\text{epoch_num}} a_{0}$$ $$a =\\frac{k}{\\sqrt{\\text{epoch_num}}}a_{0}$$ $$a =\\frac{k}{\\sqrt{t}}a_{0}$$","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(2-1)-- 深度学习的实践层面（Practical aspects of Deep Learning）","slug":"dl-ai-2-1h","date":"2018-09-15T07:58:33.000Z","updated":"2018-09-30T07:31:00.143Z","comments":true,"path":"2018/2018091515/","link":"","permalink":"http://fangzh.top/2018/2018091515/","excerpt":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！","text":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 本周的作业分了3部分： 初始化参数 正则化（L2、dropout） 梯度检验 part1：Initialization主要说明的不同的初始化对迭代的影响。 首先，模型函数是这样的： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"): \"\"\" Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- input data, of shape (2, number of examples) Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples) learning_rate -- learning rate for gradient descent num_iterations -- number of iterations to run gradient descent print_cost -- if True, print the cost every 1000 iterations initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\") Returns: parameters -- parameters learnt by the model \"\"\" grads = &#123;&#125; costs = [] # to keep track of the loss m = X.shape[1] # number of examples layers_dims = [X.shape[0], 10, 5, 1] # Initialize parameters dictionary. if initialization == \"zeros\": parameters = initialize_parameters_zeros(layers_dims) elif initialization == \"random\": parameters = initialize_parameters_random(layers_dims) elif initialization == \"he\": parameters = initialize_parameters_he(layers_dims) # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. a3, cache = forward_propagation(X, parameters) # Loss cost = compute_loss(a3, Y) # Backward propagation. grads = backward_propagation(X, Y, cache) # Update parameters. parameters = update_parameters(parameters, grads, learning_rate) # Print the loss every 1000 iterations if print_cost and i % 1000 == 0: print(\"Cost after iteration &#123;&#125;: &#123;&#125;\".format(i, cost)) costs.append(cost) # plot the loss plt.plot(costs) plt.ylabel('cost') plt.xlabel('iterations (per hundreds)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() return parameters 1. Zero Initialization 把参数全都置位0，结果是显而易见的，就是没有任何变化。 2. Random initialization 把W参数随机化了，但是乘以10倍系数，所以导致初始化的参数太大，收敛速度很慢 12345678910111213141516171819202122232425def initialize_parameters_random(layers_dims): \"\"\" Arguments: layer_dims -- python array (list) containing the size of each layer. Returns: parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\": W1 -- weight matrix of shape (layers_dims[1], layers_dims[0]) b1 -- bias vector of shape (layers_dims[1], 1) ... WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1]) bL -- bias vector of shape (layers_dims[L], 1) \"\"\" np.random.seed(3) # This seed makes sure your \"random\" numbers will be the as ours parameters = &#123;&#125; L = len(layers_dims) # integer representing the number of layers for l in range(1, L): ### START CODE HERE ### (≈ 2 lines of code) parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 10 parameters['b' + str(l)] = np.zeros((layers_dims[l], 1)) ### END CODE HERE ### return parameters 结果一般般 3. He initialization 把W参数随机化，但是乘上系数 sqrt(2./layers_dims[l-1]) 12345678910111213141516171819202122232425def initialize_parameters_he(layers_dims): \"\"\" Arguments: layer_dims -- python array (list) containing the size of each layer. Returns: parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\": W1 -- weight matrix of shape (layers_dims[1], layers_dims[0]) b1 -- bias vector of shape (layers_dims[1], 1) ... WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1]) bL -- bias vector of shape (layers_dims[L], 1) \"\"\" np.random.seed(3) parameters = &#123;&#125; L = len(layers_dims) - 1 # integer representing the number of layers for l in range(1, L + 1): ### START CODE HERE ### (≈ 2 lines of code) parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1]) * np.sqrt(2./layers_dims[l-1]) parameters['b' + str(l)] = np.zeros((layers_dims[l], 1)) ### END CODE HERE ### return parameters 结果非常理想。 Part 2：Regularization数据集： 模型函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1): \"\"\" Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- input data, of shape (input size, number of examples) Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples) learning_rate -- learning rate of the optimization num_iterations -- number of iterations of the optimization loop print_cost -- If True, print the cost every 10000 iterations lambd -- regularization hyperparameter, scalar keep_prob - probability of keeping a neuron active during drop-out, scalar. Returns: parameters -- parameters learned by the model. They can then be used to predict. \"\"\" grads = &#123;&#125; costs = [] # to keep track of the cost m = X.shape[1] # number of examples layers_dims = [X.shape[0], 20, 3, 1] # Initialize parameters dictionary. parameters = initialize_parameters(layers_dims) # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. if keep_prob == 1: a3, cache = forward_propagation(X, parameters) elif keep_prob &lt; 1: a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob) # Cost function if lambd == 0: cost = compute_cost(a3, Y) else: cost = compute_cost_with_regularization(a3, Y, parameters, lambd) # Backward propagation. assert(lambd==0 or keep_prob==1) # it is possible to use both L2 regularization and dropout, # but this assignment will only explore one at a time if lambd == 0 and keep_prob == 1: grads = backward_propagation(X, Y, cache) elif lambd != 0: grads = backward_propagation_with_regularization(X, Y, cache, lambd) elif keep_prob &lt; 1: grads = backward_propagation_with_dropout(X, Y, cache, keep_prob) # Update parameters. parameters = update_parameters(parameters, grads, learning_rate) # Print the loss every 10000 iterations if print_cost and i % 10000 == 0: print(\"Cost after iteration &#123;&#125;: &#123;&#125;\".format(i, cost)) if print_cost and i % 1000 == 0: costs.append(cost) # plot the cost plt.plot(costs) plt.ylabel('cost') plt.xlabel('iterations (x1,000)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() return parameters 没有使用正则化时，效果： L2 正则计算代价函数 $$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L] (i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} $$ 公式已经给了，只要加上后面那一项就可以了 使用np.sum(np.square(Wl))来计算$\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$ 12345678910111213141516171819202122232425262728# GRADED FUNCTION: compute_cost_with_regularizationdef compute_cost_with_regularization(A3, Y, parameters, lambd): \"\"\" Implement the cost function with L2 regularization. See formula (2) above. Arguments: A3 -- post-activation, output of forward propagation, of shape (output size, number of examples) Y -- \"true\" labels vector, of shape (output size, number of examples) parameters -- python dictionary containing parameters of the model Returns: cost - value of the regularized loss function (formula (2)) \"\"\" m = Y.shape[1] W1 = parameters[\"W1\"] W2 = parameters[\"W2\"] W3 = parameters[\"W3\"] cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost ### START CODE HERE ### (approx. 1 line) L2_regularization_cost = lambd / (m * 2) * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) ### END CODER HERE ### cost = cross_entropy_cost + L2_regularization_cost return cost 计算反向传播函数 在$dW$上加上了正则项$\\frac{\\lambda}{m} W$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: backward_propagation_with_regularizationdef backward_propagation_with_regularization(X, Y, cache, lambd): \"\"\" Implements the backward propagation of our baseline model to which we added an L2 regularization. Arguments: X -- input dataset, of shape (input size, number of examples) Y -- \"true\" labels vector, of shape (output size, number of examples) cache -- cache output from forward_propagation() lambd -- regularization hyperparameter, scalar Returns: gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables \"\"\" m = X.shape[1] (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache dZ3 = A3 - Y ### START CODE HERE ### (approx. 1 line) dW3 = 1./m * np.dot(dZ3, A2.T) + lambd / m * W3 ### END CODE HERE ### db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True) dA2 = np.dot(W3.T, dZ3) dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0)) ### START CODE HERE ### (approx. 1 line) dW2 = 1./m * np.dot(dZ2, A1.T) + lambd / m * W2 ### END CODE HERE ### db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True) dA1 = np.dot(W2.T, dZ2) dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0)) ### START CODE HERE ### (approx. 1 line) dW1 = 1./m * np.dot(dZ1, X.T) + lambd / m * W1 ### END CODE HERE ### db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True) gradients = &#123;\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1&#125; return gradients 加上L2正则项后，效果很明显： dropout在每一次迭代中，都随机删除一定概率的neurons。 1. Forward propagation with dropout 分4步: 每一层的$d^{[l]}$对应每一层的$a^{[l]}$,因为有m个样本，所以就有$D^{[1]} = [d^{1} d^{1} … d^{1}] $of the same dimension as $A^{[1]}$.使用np.random.rand(n,m) 将$D^{[l]}$布尔化， $ &lt; keepprob$ 分为 1和0 Set $A^{[1]}$ to $A^{[1]} * D^{[1]}$. Divide $A^{[1]}$ by keep_prob. 记得用cache把每一层的D都记录下来 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# GRADED FUNCTION: forward_propagation_with_dropoutdef forward_propagation_with_dropout(X, parameters, keep_prob = 0.5): \"\"\" Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID. Arguments: X -- input dataset, of shape (2, number of examples) parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\": W1 -- weight matrix of shape (20, 2) b1 -- bias vector of shape (20, 1) W2 -- weight matrix of shape (3, 20) b2 -- bias vector of shape (3, 1) W3 -- weight matrix of shape (1, 3) b3 -- bias vector of shape (1, 1) keep_prob - probability of keeping a neuron active during drop-out, scalar Returns: A3 -- last activation value, output of the forward propagation, of shape (1,1) cache -- tuple, information stored for computing the backward propagation \"\"\" np.random.seed(1) # retrieve parameters W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] W3 = parameters[\"W3\"] b3 = parameters[\"b3\"] # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID Z1 = np.dot(W1, X) + b1 A1 = relu(Z1) ### START CODE HERE ### (approx. 4 lines) # Steps 1-4 below correspond to the Steps 1-4 described above. D1 = np.random.rand(A1.shape[0], A1.shape[1]) # Step 1: initialize matrix D1 = np.random.rand(..., ...) D1 = D1 &lt; keep_prob # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold) A1 = A1 * D1 # Step 3: shut down some neurons of A1 A1 = A1 / keep_prob # Step 4: scale the value of neurons that haven't been shut down ### END CODE HERE ### Z2 = np.dot(W2, A1) + b2 A2 = relu(Z2) ### START CODE HERE ### (approx. 4 lines) D2 = np.random.rand(A2.shape[0], A2.shape[1]) # Step 1: initialize matrix D2 = np.random.rand(..., ...) D2 = D2 &lt; keep_prob # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold) A2 = A2 * D2 # Step 3: shut down some neurons of A2 A2 = A2 / keep_prob # Step 4: scale the value of neurons that haven't been shut down ### END CODE HERE ### Z3 = np.dot(W3, A2) + b3 A3 = sigmoid(Z3) cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) return A3, cache 2. Backward propagation with dropout reapplying the same mask $D^{[1]}$ to dA1. divide dA1 by keep_prob 反向传播的时候，让之前的删除的neurons依旧归0，然后也要除以keepprob，因为dA = np.dot(W.T, dZ)，并没有重复除以过系数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: backward_propagation_with_dropoutdef backward_propagation_with_dropout(X, Y, cache, keep_prob): \"\"\" Implements the backward propagation of our baseline model to which we added dropout. Arguments: X -- input dataset, of shape (2, number of examples) Y -- \"true\" labels vector, of shape (output size, number of examples) cache -- cache output from forward_propagation_with_dropout() keep_prob - probability of keeping a neuron active during drop-out, scalar Returns: gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables \"\"\" m = X.shape[1] (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache dZ3 = A3 - Y dW3 = 1./m * np.dot(dZ3, A2.T) db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True) dA2 = np.dot(W3.T, dZ3) ### START CODE HERE ### (≈ 2 lines of code) dA2 = dA2 * D2 # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation dA2 = dA2 / keep_prob # Step 2: Scale the value of neurons that haven't been shut down ### END CODE HERE ### dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0)) dW2 = 1./m * np.dot(dZ2, A1.T) db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True) dA1 = np.dot(W2.T, dZ2) ### START CODE HERE ### (≈ 2 lines of code) dA1 = dA1 * D1 # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation dA1 = dA1 / keep_prob # Step 2: Scale the value of neurons that haven't been shut down ### END CODE HERE ### dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0)) dW1 = 1./m * np.dot(dZ1, X.T) db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True) gradients = &#123;\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1&#125; return gradients 最终结果,也还不错： 注意： dropout也是正则化的一种 训练的时候用，测试的时候不要用 在正向传播和反向传播中都要用 Part3:Gradient Checking首先写了一维的checking 12345678910111213141516171819# GRADED FUNCTION: forward_propagationdef forward_propagation(x, theta): \"\"\" Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x) Arguments: x -- a real-valued input theta -- our parameter, a real number as well Returns: J -- the value of function J, computed using the formula J(theta) = theta * x \"\"\" ### START CODE HERE ### (approx. 1 line) J = np.dot(theta,x) ### END CODE HERE ### return J 12345678910111213141516171819# GRADED FUNCTION: backward_propagationdef backward_propagation(x, theta): \"\"\" Computes the derivative of J with respect to theta (see Figure 1). Arguments: x -- a real-valued input theta -- our parameter, a real number as well Returns: dtheta -- the gradient of the cost with respect to theta \"\"\" ### START CODE HERE ### (approx. 1 line) dtheta = x ### END CODE HERE ### return dtheta 根据公式： $$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} $$ 步骤是： $\\theta^{+} = \\theta + \\varepsilon$ $\\theta^{-} = \\theta - \\varepsilon$ $J^{+} = J(\\theta^{+})$ $J^{-} = J(\\theta^{-})$ $gradapprox = \\frac{J^{+} - J^{-}}{2 \\varepsilon}$ 123456789101112131415161718192021222324252627282930313233343536373839def gradient_check(x, theta, epsilon = 1e-7): \"\"\" Implement the backward propagation presented in Figure 1. Arguments: x -- a real-valued input theta -- our parameter, a real number as well epsilon -- tiny shift to the input to compute approximated gradient with formula(1) Returns: difference -- difference (2) between the approximated gradient and the backward propagation gradient \"\"\" # Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit. ### START CODE HERE ### (approx. 5 lines) thetaplus = theta + epsilon # Step 1 thetaminus = theta - epsilon # Step 2 J_plus = forward_propagation(x, thetaplus) # Step 3 J_minus = forward_propagation(x, thetaminus) # Step 4 gradapprox = (J_plus - J_minus) / (2 * epsilon) # Step 5 ### END CODE HERE ### # Check if gradapprox is close enough to the output of backward_propagation() ### START CODE HERE ### (approx. 1 line) grad = backward_propagation(x, theta) ### END CODE HERE ### ### START CODE HERE ### (approx. 1 line) numerator = np.linalg.norm(grad - gradapprox) # Step 1' denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox) # Step 2' difference = numerator / denominator # Step 3' ### END CODE HERE ### if difference &lt; 1e-7: print (\"The gradient is correct!\") else: print (\"The gradient is wrong!\") return difference 在N维的空间中， 12345678910111213141516171819202122232425262728293031323334353637383940414243def forward_propagation_n(X, Y, parameters): \"\"\" Implements the forward propagation (and computes the cost) presented in Figure 3. Arguments: X -- training set for m examples Y -- labels for m examples parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\": W1 -- weight matrix of shape (5, 4) b1 -- bias vector of shape (5, 1) W2 -- weight matrix of shape (3, 5) b2 -- bias vector of shape (3, 1) W3 -- weight matrix of shape (1, 3) b3 -- bias vector of shape (1, 1) Returns: cost -- the cost function (logistic cost for one example) \"\"\" # retrieve parameters m = X.shape[1] W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] W3 = parameters[\"W3\"] b3 = parameters[\"b3\"] # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID Z1 = np.dot(W1, X) + b1 A1 = relu(Z1) Z2 = np.dot(W2, A1) + b2 A2 = relu(Z2) Z3 = np.dot(W3, A2) + b3 A3 = sigmoid(Z3) # Cost logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y) cost = 1./m * np.sum(logprobs) cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) return cost, cache 1234567891011121314151617181920212223242526272829303132333435def backward_propagation_n(X, Y, cache): \"\"\" Implement the backward propagation presented in figure 2. Arguments: X -- input datapoint, of shape (input size, 1) Y -- true \"label\" cache -- cache output from forward_propagation_n() Returns: gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables. \"\"\" m = X.shape[1] (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache dZ3 = A3 - Y dW3 = 1./m * np.dot(dZ3, A2.T) db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True) dA2 = np.dot(W3.T, dZ3) dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0)) dW2 = 1./m * np.dot(dZ2, A1.T) * 2 db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True) dA1 = np.dot(W2.T, dZ2) dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0)) dW1 = 1./m * np.dot(dZ1, X.T) db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True) gradients = &#123;\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1&#125; return gradients 这个时候，给了两个函数，可以在字典和向量结构相互转换，也就是要计算$\\theta^{+}$时，把字典转为向量会比较好计算。 12dictionary_to_vector()vector_to_dictionary() J_plus[i]就是向量中的每一个元素，也就是W,b展开之后的每一项元素 To compute J_plus[i]: Set $\\theta^{+}$ to np.copy(parameters_values) Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$ Calculate $J^{+}_i$ using to forward_propagation_n(x, y, vector_to_dictionary($\\theta^{+}$ )). To compute J_minus[i]: do the same thing with $\\theta^{-}$ Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$ 代码如下，记住 thetaplus是一个(n,1)的向量，循环计算每一个参数的gradapprox，再和原本的grad比较： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# GRADED FUNCTION: gradient_check_ndef gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7): \"\"\" Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n Arguments: parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\": grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. x -- input datapoint, of shape (input size, 1) y -- true \"label\" epsilon -- tiny shift to the input to compute approximated gradient with formula(1) Returns: difference -- difference (2) between the approximated gradient and the backward propagation gradient \"\"\" # Set-up variables parameters_values, _ = dictionary_to_vector(parameters) grad = gradients_to_vector(gradients) num_parameters = parameters_values.shape[0] J_plus = np.zeros((num_parameters, 1)) J_minus = np.zeros((num_parameters, 1)) gradapprox = np.zeros((num_parameters, 1)) # Compute gradapprox for i in range(num_parameters): # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\". # \"_\" is used because the function you have to outputs two parameters but we only care about the first one ### START CODE HERE ### (approx. 3 lines) thetaplus = np.copy(parameters_values) # Step 1 thetaplus[i][0] = thetaplus[i][0] + epsilon # Step 2 J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus)) # Step 3 ### END CODE HERE ### # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\". ### START CODE HERE ### (approx. 3 lines) thetaminus = np.copy(parameters_values) # Step 1 thetaminus[i][0] = thetaminus[i][0] - epsilon # Step 2 J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus)) # Step 3 ### END CODE HERE ### # Compute gradapprox[i] ### START CODE HERE ### (approx. 1 line) gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon) ### END CODE HERE ### # Compare gradapprox to backward propagation gradients by computing difference. ### START CODE HERE ### (approx. 1 line) numerator = np.linalg.norm(grad - gradapprox) # Step 1' denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox) # Step 2' difference = numerator / denominator # Step 3' ### END CODE HERE ### if difference &gt; 2e-7: print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\") else: print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\") return difference 注意： 梯度检验太慢，不要在训练的时候运行，你运行只是为了保证你的算法是正确的。","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"},{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai笔记:(2-1)-- 深度学习的实践层面（Practical aspects of Deep Learning）","slug":"dl-ai-2-1","date":"2018-09-15T05:37:15.000Z","updated":"2018-09-30T07:28:53.410Z","comments":true,"path":"2018/20180901513/","link":"","permalink":"http://fangzh.top/2018/20180901513/","excerpt":"第二门课主要讲的是如何改善神经网络，通过超参数的调试、正则化以及优化。 第一周主要是说了一些之前机器学习里面涉及到的数据集的划分，以及初始化，正则化的方法，还有梯度的验证。","text":"第二门课主要讲的是如何改善神经网络，通过超参数的调试、正则化以及优化。 第一周主要是说了一些之前机器学习里面涉及到的数据集的划分，以及初始化，正则化的方法，还有梯度的验证。 训练、验证、测试集的划分这些在之前的机器学习课程中都讲过了，这里简单说一下。 训练集也就是你训练的样本；验证集是你训练之后的参数放到这些数据中做验证；而最后做的测试集则是相当于用来最终的测试。 一般来说，划分比例为60%/20%/20%就可以了，但是当数据越来越大，变成上百万，上千万的时候，那么验证集和测试集就没必要占那么大比重了，因为太过浪费，一般在0.5%-3%左右就可以。 需要注意的是，验证集和测试集的数据要来源相同，同分布，也就是同一类的数据，不能验证集是网上的，测试集是你自己拍的照片，这样误差会很大。 bias and variance（偏差和方差） high bias 表示的是高偏差，一般出现在欠拟合(under fitting)的情况下， high variance表示高方差，一般出现在overfitting情况下。 如何解决呢： high bias 更多的隐藏层 每一层更多的神经元 high variance 增加数据 正则化 从左到右4种情况即是： high variance ; high bias ; high bias and high variance ; low bias and low variance regularization（正则化）high variance可以使用正则化来解决。 我们知道，在logistic regression中的正则化项，是在损失函数后面加上： L2 正则：$\\frac{\\lambda}{2m}||w||^{2}{2} = \\frac{\\lambda}{2m}\\sum{j=1}^{n_{x}}{|w|} = \\frac{\\lambda}{2m} w^T w$ L1正则：$\\frac{\\lambda}{2m}||w||{1} = \\frac{\\lambda}{2m}\\sum{j=1}^{n_{x}}{|w|}$ 一般用L2正则来做。 在neural network中， 可以看到后面的正则式是从第1层累加到了第L层的所有神经网络的权重$||W^{[l]}||_{F}$的平方。 而我们知道这个W是一个$n^{[l]} * n^{[l-1]}$的矩阵，那么 它表示矩阵中所有元素的平方和。也就这一项嵌套了3层的$\\sum$。 那么，如何实现这个范数的梯度下降呢？ 在原本的backprop中,加上的正则项的导数，$dJ / dW$ $$dW^{[l]} = (form backprop) + \\frac{\\lambda}{m}W^{[l]}$$ 代入 $$W^{[l]} = W^{[l]} - \\alpha dW^{[l]}$$ 得到： 可以看到，$(1 - \\frac{\\alpha \\lambda}{m}) &lt; 1$，所以每一次都会让W变小，因此L2范数正则化也成为“权重衰减” 正则化如何防止过拟合？直观理解是在代价函数加入正则项后，如果$\\lambda$非常大，为了满足代价函数最小化，那么$w^{[l]}$这一项必须非常接近于0，所以就等价于很多神经元都没有作用了，从原本的非线性结构变成了近似的线性结构，自然就不会过拟合了。 我们再来直观感受一下， 假设是一个tanh()函数，那么$z = wx + b$，当w非常接近于0时，z也接近于0，也就是在坐标轴上0附近范围内，这个时候斜率接近于线性，那么整个神经网络也非常接近于线性的网络，那么就不会发生过拟合了。 dropout 正则化dropout(随机失活)，也是正则化的一种，顾名思义，是让神经网络中的神经元按照一定的概率随机失活。 实现dropout：inverted dropout（反向随机失活） 实现dropout有好几种，但是最常用的还是这个inverted dropout 假设是一个3层的神经网络，keepprob表示保留节点的概率 12345keepprob = 0.8#d3是矩阵，每个元素有true,false,在python中代表1和0d3 = np.random.rand(a3.shape[0],a3.shape[1]) &lt; keepproba3 = np.multiply(a3,d3)a3 /= keepprob 其中第4式 $a3 /= keepprob$ 假设第三层有50个神经元 a3.shape[0] = 50，一共有 $50 * m$维，m是样本数，这样子就会有平均10个神经元被删除，因为$z^{[4]} = w^{[4]} a^{[3]} + b^{[4]}$，那么这个时候$z^{[4]}$的期望值就少了20%,所以在每个神经元上都除以keepprob的值，刚好弥补的之前的损失。 注意 在test阶段，就不需要再使用dropout了，而是像之前一样，直接乘以各个层的权重，得出预测值就可以。 理解dropout直观上，因为神经元有可能会被随机清除，这样子在训练中，就不会过分依赖某一个神经元或者特征的权重。 当然可以设置不同层有不同的dropout概率。 计算机视觉领域非常喜欢用这个dropout。 但是这个东西的一大缺点就是代价函数J不能再被明确定义，每次都会随机移除一些节点，所以很难进行复查。如果需要调试的话，通常会关闭dropout，设置为1，这样再来debug。 归一化归一化数据可以加速神经网络的训练速度。 一般有两个步骤： 零均值 归一化方差 这样子在gradient的时候就会走的顺畅一点： 参数初始化合理的参数初始化可以有效的加快神经网络的训练速度。 一般呢$z = w_1 x_1 + w_2 x_2 + … + w_n x_n$，一般希望z不要太大也不要太小。所以呢，希望n越大，w越小才好。最合理的就是方差 $w = \\frac{1}{n}$，所以： 1WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(1/n) 这个$n$即$n^{[l-1]}$ 如果是relu函数， 那么 $w = \\frac{2}{n}$比较好，也就是np.sqrt(2/n) 梯度的数值逼近$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} $$ 微积分的常识，用$\\varepsilon$来逼近梯度。 梯度检验用梯度检验可以来检查在反向传播中的算法有没有错误。 这个时候，可以把$W^{[1]},b^{[1]},……W^{[l]},b^{[l]}$变成一个向量，这样可以得到一个代价函数$J(\\theta)$，然后$dW,db$也可以转换成一个向量，用$d\\theta$表示，和$\\theta$有相同的维度。 再对每一个$d\\theta_{approx}[i]$求上面的双边梯度逼近，然后也用导数求得每一个$d\\theta[i]$，然后根据图上的cheak公式。求梯度逼近的时候，设置两边的$\\varepsilon = 10^{-7}$，最终求得的值如果是$10^{-7}$，那么很正常，$10^{-3}$就是错了的，如果是$10^{-5}$，那么就需要斟酌一下了。 注意 不要在训练中用梯度检验，因为很慢 如果发现有问题，那么定位到误差比较大的那一层查看 如果有正则化，记得加入正则项 不要和dropout一起使用，因为dropout本来就不容易计算梯度。","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(1-4)-- 深层神经网络（Deep neural networks）","slug":"dl-ai-1-4h","date":"2018-09-13T09:59:43.000Z","updated":"2018-09-30T07:22:02.969Z","comments":true,"path":"2018/2018091318/","link":"","permalink":"http://fangzh.top/2018/2018091318/","excerpt":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！","text":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 本周的作业分了两个部分，第一部分先构建神经网络的基本函数，第二部分才是构建出模型并预测。 Part1构建的函数有： Initialize the parameters two-layer L-layer forworad propagation Linear part 先构建一个线性的计算函数 linear-&gt;activation 在构建某一个神经元的线性和激活函数 L_model_forward funciton 再融合 L-1次的Relu 和 一次 的 sigmoid最后一层 Compute loss backward propagation Linear part linear-&gt;activation L_model_backward funciton Initialization初始化使用： w : np.random.randn(shape)*0.01 b : np.zeros(shape) 1. two-layer 先写了个两层的初始化函数，上周已经写过了。 1234567891011121314151617181920212223242526272829303132333435def initialize_parameters(n_x, n_h, n_y): \"\"\" Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: parameters -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) \"\"\" np.random.seed(1) ### START CODE HERE ### (≈ 4 lines of code) W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h,1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y,1)) ### END CODE HERE ### assert(W1.shape == (n_h, n_x)) assert(b1.shape == (n_h, 1)) assert(W2.shape == (n_y, n_h)) assert(b2.shape == (n_y, 1)) parameters = &#123;\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2&#125; return parameters 2. L-layer 然后写了个L层的初始化函数，其中，输入的参数是一个列表，如[12,4,3,1]，表示一共4层： 1234567891011121314151617181920212223242526def initialize_parameters_deep(layer_dims): \"\"\" Arguments: layer_dims -- python array (list) containing the dimensions of each layer in our network Returns: parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\": Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1]) bl -- bias vector of shape (layer_dims[l], 1) \"\"\" np.random.seed(3) parameters = &#123;&#125; L = len(layer_dims) # number of layers in the network for l in range(1, L): ### START CODE HERE ### (≈ 2 lines of code) parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01 parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) ### END CODE HERE ### assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1])) assert(parameters['b' + str(l)].shape == (layer_dims[l], 1)) return parameters Forward propagation module1. Linear Forward 利用公式： $$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$ where $A^{[0]} = X$. 这个时候，输入的参数是 A,W,b,输出是计算得到的Z，以及cache=（A， W， b）保存起来 12345678910111213141516171819202122def linear_forward(A, W, b): \"\"\" Implement the linear part of a layer's forward propagation. Arguments: A -- activations from previous layer (or input data): (size of previous layer, number of examples) W -- weights matrix: numpy array of shape (size of current layer, size of previous layer) b -- bias vector, numpy array of shape (size of the current layer, 1) Returns: Z -- the input of the activation function, also called pre-activation parameter cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently \"\"\" ### START CODE HERE ### (≈ 1 line of code) Z = np.dot(W, A) + b ### END CODE HERE ### assert(Z.shape == (W.shape[0], A.shape[1])) cache = (A, W, b) return Z, cache 2. Linear-Activation Forward 在这里就是把刚才得到的Z，通过$A = g(Z)$激活函数，合并成一个 这个时候，notebook已经给了我们现成的sigmoid和relu函数了，只要调用就行，不过在里面好像没有说明源代码，输出都是A和cache=Z，这里贴出来： 12345678910111213141516def sigmoid(Z): \"\"\" Implements the sigmoid activation in numpy Arguments: Z -- numpy array of any shape Returns: A -- output of sigmoid(z), same shape as Z cache -- returns Z as well, useful during backpropagation \"\"\" A = 1/(1+np.exp(-Z)) cache = Z return A, cache 123456789101112131415161718def relu(Z): \"\"\" Implement the RELU function. Arguments: Z -- Output of the linear layer, of any shape Returns: A -- Post-activation parameter, of the same shape as Z cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently \"\"\" A = np.maximum(0,Z) assert(A.shape == Z.shape) cache = Z return A, cache 而后利用之前的linear_forward，可以写出某层神经元的前向函数了，输入是$A^{[l-1]},W,b$，还有一个是说明sigmoid还是relu的字符串activation。 输出是$A^{[l]}$和cache，这里的cache已经包含的4个参数了，分别是$A^{[l-1]},W^{[l]},b^{[l]},Z^{[l]}$ 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: linear_activation_forwarddef linear_activation_forward(A_prev, W, b, activation): \"\"\" Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer Arguments: A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples) W -- weights matrix: numpy array of shape (size of current layer, size of previous layer) b -- bias vector, numpy array of shape (size of the current layer, 1) activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" Returns: A -- the output of the activation function, also called the post-activation value cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\"; stored for computing the backward pass efficiently \"\"\" if activation == \"sigmoid\": # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\". ### START CODE HERE ### (≈ 2 lines of code) Z, linear_cache = linear_forward(A_prev, W, b) A, activation_cache = sigmoid(Z) ### END CODE HERE ### elif activation == \"relu\": # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\". ### START CODE HERE ### (≈ 2 lines of code) Z, linear_cache = linear_forward(A_prev, W, b) A, activation_cache = relu(Z) ### END CODE HERE ### assert (A.shape == (W.shape[0], A_prev.shape[1])) cache = (linear_cache, activation_cache) # print(cache) return A, cache 3. L-Layer Model 这一步就把多层的神经网络从头到尾串起来了。前面有L-1层的Relu，第L层是sigmoid。 输入是X，也就是$A^{[0]}$，和 parameters包含了各个层的W,b 输出是最后一层的$A^{[L]}$，也就是预测结果$Y_hat$，以及每一层的caches : $A^{[l-1]},W^{[l]},b^{[l]},Z^{[l]}$ 1234567891011121314151617181920212223242526272829303132333435def L_model_forward(X, parameters): \"\"\" Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation Arguments: X -- data, numpy array of shape (input size, number of examples) parameters -- output of initialize_parameters_deep() Returns: AL -- last post-activation value caches -- list of caches containing: every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1) \"\"\" caches = [] A = X L = len(parameters) // 2 # number of layers in the neural network # Implement [LINEAR -&gt; RELU]*(L-1). Add \"cache\" to the \"caches\" list. for l in range(1, L): A_prev = A ### START CODE HERE ### (≈ 2 lines of code) A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], 'relu') caches.append(cache) ### END CODE HERE ### # Implement LINEAR -&gt; SIGMOID. Add \"cache\" to the \"caches\" list. ### START CODE HERE ### (≈ 2 lines of code) AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)],'sigmoid') caches.append(cache) ### END CODE HERE ### # print(AL.shape) assert(AL.shape == (1,X.shape[1])) return AL, caches Cost function$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{L}\\right)) $$ 利用np.multiply and np.sum求得交叉熵 123456789101112131415161718192021222324def compute_cost(AL, Y): \"\"\" Implement the cost function defined by equation (7). Arguments: AL -- probability vector corresponding to your label predictions, shape (1, number of examples) Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples) Returns: cost -- cross-entropy cost \"\"\" m = Y.shape[1] # Compute loss from aL and y. ### START CODE HERE ### (≈ 1 lines of code) cost = - np.sum(np.multiply(Y,np.log(AL)) + np.multiply(1-Y,np.log(1-AL))) / m print(cost) ### END CODE HERE ### cost = np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17). assert(cost.shape == ()) return cost Backward propagation module1. Linear backward 首先假设知道 $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$，然后想要求得的是$(dW^{[l]}, db^{[l]} dA^{[l-1]})$. 公式已经给你了：$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$ $$db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l] (i)}$$ $$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$ cache是linear cache: A_prev,W,b 1234567891011121314151617181920212223242526272829def linear_backward(dZ, cache): \"\"\" Implement the linear portion of backward propagation for a single layer (layer l) Arguments: dZ -- Gradient of the cost with respect to the linear output (of current layer l) cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer Returns: dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev dW -- Gradient of the cost with respect to W (current layer l), same shape as W db -- Gradient of the cost with respect to b (current layer l), same shape as b \"\"\" A_prev, W, b = cache m = A_prev.shape[1] ### START CODE HERE ### (≈ 3 lines of code) dW = 1 / m * np.dot(dZ, A_prev.T) db = 1 / m * np.sum(dZ, axis=1,keepdims=True) #print(db.shape) #print(b.shape) dA_prev = np.dot(W.T, dZ) ### END CODE HERE ### assert (dA_prev.shape == A_prev.shape) assert (dW.shape == W.shape) assert (db.shape == b.shape) return dA_prev, dW, db 2. Linear-Activation backward dA通过激活函数的导数可以求得dZ，再由上面的函数，最终： 输入$dA^{[l]} , cache$ 输出$dA^{[l-1]} ,dW,db$ 这个时候它有给了两个现成的函数dZ = sigmoid_backward(dA, activation_cache)、dZ = relu_backward(dA, activation_cache) 源代码如下,输入的都是dA，和 cache=Z，输出是dZ： $$dZ^{[l]} = dA^{[l]} * g’(Z^{[l]})$$ 1234567891011121314151617181920def sigmoid_backward(dA, cache): \"\"\" Implement the backward propagation for a single SIGMOID unit. Arguments: dA -- post-activation gradient, of any shape cache -- 'Z' where we store for computing backward propagation efficiently Returns: dZ -- Gradient of the cost with respect to Z \"\"\" Z = cache s = 1/(1+np.exp(-Z)) dZ = dA * s * (1-s) assert (dZ.shape == Z.shape) return dZ 123456789101112131415161718192021def relu_backward(dA, cache): \"\"\" Implement the backward propagation for a single RELU unit. Arguments: dA -- post-activation gradient, of any shape cache -- 'Z' where we store for computing backward propagation efficiently Returns: dZ -- Gradient of the cost with respect to Z \"\"\" Z = cache dZ = np.array(dA, copy=True) # just converting dz to a correct object. # When z &lt;= 0, you should set dz to 0 as well. dZ[Z &lt;= 0] = 0 assert (dZ.shape == Z.shape) return dZ 然后得到了函数如下,注意这里面的cache已经是4个元素了linear_cache=A_prev,W,b、activation_cache=Z： 12345678910111213141516171819202122232425262728293031# GRADED FUNCTION: linear_activation_backwarddef linear_activation_backward(dA, cache, activation): \"\"\" Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer. Arguments: dA -- post-activation gradient for current layer l cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" Returns: dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev dW -- Gradient of the cost with respect to W (current layer l), same shape as W db -- Gradient of the cost with respect to b (current layer l), same shape as b \"\"\" linear_cache, activation_cache = cache if activation == \"relu\": ### START CODE HERE ### (≈ 2 lines of code) dZ = relu_backward(dA, activation_cache) dA_prev, dW, db = linear_backward(dZ, linear_cache) ### END CODE HERE ### elif activation == \"sigmoid\": ### START CODE HERE ### (≈ 2 lines of code) dZ = sigmoid_backward(dA, activation_cache) dA_prev, dW, db = linear_backward(dZ, linear_cache) ### END CODE HERE ### return dA_prev, dW, db 3. L-Model Backward 可以把前面的函数穿起来，从后面往前面传播了，先算最后一层的sigmoid，然后往前算L-1的循环relu。其中，dAL是损失函数的导数，这个是预先求得知道的，也就是 $$-\\frac{y}{a}-\\frac{1-y}{1-a}$$ numpy表示为： 1dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) 整个backward中，我们的输入只有AL,Y和caches， 输出则是每一层的grads，包括了$dA,dW,db$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# GRADED FUNCTION: L_model_backwarddef L_model_backward(AL, Y, caches): \"\"\" Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group Arguments: AL -- probability vector, output of the forward propagation (L_model_forward()) Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) caches -- list of caches containing: every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2) the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1]) Returns: grads -- A dictionary with the gradients grads[\"dA\" + str(l)] = ... grads[\"dW\" + str(l)] = ... grads[\"db\" + str(l)] = ... \"\"\" grads = &#123;&#125; L = len(caches) # the number of layers m = AL.shape[1] Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL # Initializing the backpropagation ### START CODE HERE ### (1 line of code) dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) ### END CODE HERE ### # Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"] ### START CODE HERE ### (approx. 2 lines) current_cache = caches[L-1] grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid') ### END CODE HERE ### # Loop from l=L-2 to l=0 for l in reversed(range(L-1)): # lth layer: (RELU -&gt; LINEAR) gradients. # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] ### START CODE HERE ### (approx. 5 lines) current_cache = caches[l] dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA'+str(l+1)], current_cache, 'relu') grads[\"dA\" + str(l)] = dA_prev_temp grads[\"dW\" + str(l + 1)] = dW_temp grads[\"db\" + str(l + 1)] = db_temp ### END CODE HERE ### return grads Update Parameters12345678910111213141516171819202122232425# GRADED FUNCTION: update_parametersdef update_parameters(parameters, grads, learning_rate): \"\"\" Update parameters using gradient descent Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients, output of L_model_backward Returns: parameters -- python dictionary containing your updated parameters parameters[\"W\" + str(l)] = ... parameters[\"b\" + str(l)] = ... \"\"\" L = len(parameters) // 2 # number of layers in the neural network # Update rule for each parameter. Use a for loop. ### START CODE HERE ### (≈ 3 lines of code) for l in range(L): parameters[\"W\" + str(l+1)] -= learning_rate * grads['dW'+str(l+1)] parameters[\"b\" + str(l+1)] -= learning_rate * grads['db'+str(l+1)] ### END CODE HERE ### return parameters Part2有了part1中的函数，就很容易在part2中搭建模型和训练了。 依旧是识别猫猫的图片。 开始先用两层的layer做训练，得到了精确度是72%，这里贴代码就好了，L层再详细说说 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899### CONSTANTS DEFINING THE MODEL ####n_x = 12288 # num_px * num_px * 3n_h = 7n_y = 1layers_dims = (n_x, n_h, n_y)# GRADED FUNCTION: two_layer_modeldef two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False): \"\"\" Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- input data, of shape (n_x, number of examples) Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) layers_dims -- dimensions of the layers (n_x, n_h, n_y) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- If set to True, this will print the cost every 100 iterations Returns: parameters -- a dictionary containing W1, W2, b1, and b2 \"\"\" np.random.seed(1) grads = &#123;&#125; costs = [] # to keep track of the cost m = X.shape[1] # number of examples (n_x, n_h, n_y) = layers_dims # Initialize parameters dictionary, by calling one of the functions you'd previously implemented ### START CODE HERE ### (≈ 1 line of code) parameters = initialize_parameters(n_x, n_h, n_y) ### END CODE HERE ### # Get W1, b1, W2 and b2 from the dictionary parameters. W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\". ### START CODE HERE ### (≈ 2 lines of code) A1, cache1 = linear_activation_forward(X, W1, b1, 'relu') A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid') ### END CODE HERE ### # Compute cost ### START CODE HERE ### (≈ 1 line of code) cost = compute_cost(A2, Y) ### END CODE HERE ### # Initializing backward propagation dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2)) # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\". ### START CODE HERE ### (≈ 2 lines of code) dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid') dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu') ### END CODE HERE ### # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2 grads['dW1'] = dW1 grads['db1'] = db1 grads['dW2'] = dW2 grads['db2'] = db2 # Update parameters. ### START CODE HERE ### (approx. 1 line of code) parameters = update_parameters(parameters, grads, learning_rate) ### END CODE HERE ### # Retrieve W1, b1, W2, b2 from parameters W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] # Print the cost every 100 training example if print_cost and i % 100 == 0: print(\"Cost after iteration &#123;&#125;: &#123;&#125;\".format(i, np.squeeze(cost))) if print_cost and i % 100 == 0: costs.append(cost) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() return parameters L-layer Neural Network使用之前的函数： 123456789101112131415def initialize_parameters_deep(layers_dims): ... return parameters def L_model_forward(X, parameters): ... return AL, cachesdef compute_cost(AL, Y): ... return costdef L_model_backward(AL, Y, caches): ... return gradsdef update_parameters(parameters, grads, learning_rate): ... return parameters 这里一共4层： 1layers_dims = [12288, 20, 7, 5, 1] # 4-layer model 思路是： 初始化参数 进入for的n次迭代循环： L_model_forward(X, parameters) 得到 AL,caches 计算cost L_model_backward(AL, Y, caches)计算grads update_parameters(parameters, grads, learning_rate)更新参数 每100层记录一下cost的值 画出cost梯度下降图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# GRADED FUNCTION: L_layer_modeldef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009 \"\"\" Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- data, numpy array of shape (number of examples, num_px * num_px * 3) Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) layers_dims -- list containing the input size and each layer size, of length (number of layers + 1). learning_rate -- learning rate of the gradient descent update rule num_iterations -- number of iterations of the optimization loop print_cost -- if True, it prints the cost every 100 steps Returns: parameters -- parameters learnt by the model. They can then be used to predict. \"\"\" np.random.seed(1) costs = [] # keep track of cost # Parameters initialization. (≈ 1 line of code) ### START CODE HERE ### parameters = initialize_parameters_deep(layers_dims) ### END CODE HERE ### # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID. ### START CODE HERE ### (≈ 1 line of code) AL, caches = L_model_forward(X, parameters) ### END CODE HERE ### # Compute cost. ### START CODE HERE ### (≈ 1 line of code) cost = compute_cost(AL, Y) ### END CODE HERE ### # Backward propagation. ### START CODE HERE ### (≈ 1 line of code) grads = L_model_backward(AL, Y, caches) ### END CODE HERE ### # Update parameters. ### START CODE HERE ### (≈ 1 line of code) parameters = update_parameters(parameters, grads, learning_rate) ### END CODE HERE ### # Print the cost every 100 training example if print_cost and i % 100 == 0: print (\"Cost after iteration %i: %f\" %(i, cost)) if print_cost and i % 100 == 0: costs.append(cost) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() return parameters 2500的迭代次数，精度达到了80%！ 小结 过程其实是很清晰的，就是先初始化参数；再开始循环，循环中先计算前向传播，得到最后一层的AL，以及每一层的cache，其中cache包括了 A_prev，W，b，Z；然后计算一下每一次迭代的cost；再进行反向传播，得到每一层的梯度dA,dW,db;记得每100次迭代记录一下cost值，这样就可以画出cost是如何下降的了。 part1构建的那些函数，一步步来是比较简单的，但是如果自己要一下子想出来的话，也很难想得到。所以思路要清晰，一步一步来，才能构建好函数！","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"},{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai笔记:(1-4)-- 深层神经网络（Deep neural networks）","slug":"dl-ai-1-4","date":"2018-09-13T08:54:18.000Z","updated":"2018-09-30T07:21:27.458Z","comments":true,"path":"2018/2018091316/","link":"","permalink":"http://fangzh.top/2018/2018091316/","excerpt":"这一周主要讲了深层的神经网络搭建。","text":"这一周主要讲了深层的神经网络搭建。 深层神经网络的符号表示 在深层的神经网络中， $L$表示神经网络的层数 $L = 4$ $n^{[l]}$表示第$l$层的神经网络个数 $W^{[l]}: (n^{[l]},n^{l-1})$ $dW^{[l]}: (n^{[l]},n^{l-1})$ $b^{[l]}: (n^{[l]},1)$ $db^{[l]}: (n^{[l]},1)$ $z^{[l]}:(n^{[l]},1)$ $a^{[l]}:(n^{[l]},1)$ 前向传播和反向传播前向传播 input $a^{[l-1]}$ output $a^{[l]},cache (z^{[l]})$ ，其中cache也顺便把 $W^{[l]}, b^{[l]}$也保存下来了 所以，前向传播的公式可以写作： $$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$$ $$A^{[l]} = g^{[l]}(Z^{[l]})$$ 维度 假设有m个样本，那么$Z^{[l]}$ 维度就是 $(n^{[l]}, m)$ ，$A^{[l]}$的维度和$Z^{[l]}$一样。 那么 $ W^{[l]} A^{[l-1]}$维度就是 $(n^{[l]},n^{l-1}) * (n^{[l-1]},m)$ 也就是 $(n^{[l]}, m)$，这个时候，还需要加上$b^{[l]}$，而$b^{[l]}$本身的维度是$(n^{[l]},1)$，借助python的广播，扩充到了m个维度。 反向传播 input $da^{[l]}$ output $da^{[l-1]} , dW^{[l]} , db^{[l]}$ 公式： 向量化： 正向传播和反向传播如图： 具体过程为，第一层和第二层用Relu函数，第三层输出用sigmoid，这个时候的输出值是$a^{[3]}$ 而首先进行反向传播的时候先求得$da^{[3]} = - \\frac{y}{a} - \\frac{1-y}{1-a}$，然后再包括之前存在cache里面的$z^{[3]}$,反向传播可以得到$dw^{[3]}, db^{[3]},da^{[2]}$，然后继续反向，直到得到了$dw^{[1]},db^{[1]}$后，更新一下w，b的参数，然后继续做前向传播、反向传播，不断循环。 Why Deep？ 如图直观上感觉，比如第一层，它会先识别出一些边缘信息；第二层则将这些边缘进行整合，得到一些五官信息，如眼睛、嘴巴等；到了第三层，就可以将这些信息整合起来，输出一张人脸了。 如果网络层数不够深的话，可以组合的情况就很少，或者需要类似门电路那样，用单层很多个特征才能得到和深层神经网络类似的效果。 搭建深层神经网络块 和之前说的一样，一个网络块中包含了前向传播和反向传播。 前向输入$a^{[l-1]}$，经过神经网络的计算，$g^{[l]}(w^{[l]}a^{[l-1]} + b^{[l]})$得到$a^{[l]}$ 反向传播，输入$da^{[l]}$，再有之前在cache的$z^{[l]}$,即可得到$dw^{[l]},db^{[l]}$还有上一层的$da^{[l-1]}$ 参数与超参数超参数就是你自己调的，玄学参数： learning_rate iterations L = len(hidden layer) $n^{[l]}$ activation function mini batch size（最小的计算批） regularization（正则） momentum（动量）","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai作业:(1-3)-- 浅层神经网络（Shallow neural networks）","slug":"dl-ai-1-3h","date":"2018-09-12T07:49:22.000Z","updated":"2018-09-30T07:18:54.285Z","comments":true,"path":"2018/2018091216/","link":"","permalink":"http://fangzh.top/2018/2018091216/","excerpt":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！","text":"不要抄作业！ 我只是把思路整理了，供个人学习。 不要抄作业！ 数据集数据集是一个类似花的数据集。 而如果用传统的logistic regression，做出来的就是一个二分类问题，简单粗暴的划出了一条线， 可以看见，准确率只有47%。 所以就需要构建神经网络模型了。 神经网络模型Reminder: The general methodology to build a Neural Network is to: 12345671. Define the neural network structure ( # of input units, # of hidden units, etc). 2. Initialize the model&apos;s parameters3. Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent) 已经给出思路了： 定义神经网络的结构 初始化模型参数 循环： 计算正向传播 计算损失函数 计算反向传播来得到grad 更新参数 1. 定义神经网络结构12345678910111213141516171819# GRADED FUNCTION: layer_sizesdef layer_sizes(X, Y): \"\"\" Arguments: X -- input dataset of shape (input size, number of examples) Y -- labels of shape (output size, number of examples) Returns: n_x -- the size of the input layer n_h -- the size of the hidden layer n_y -- the size of the output layer \"\"\" ### START CODE HERE ### (≈ 3 lines of code) n_x = X.shape[0] # size of input layer n_h = 4 n_y = Y.shape[0] # size of output layer ### END CODE HERE ### return (n_x, n_h, n_y) 2. 初始化参数来初始化w和b的参数 w: np.random.rand(a,b) * 0.01 b: np.zeros((a,b)) 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: initialize_parametersdef initialize_parameters(n_x, n_h, n_y): \"\"\" Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) \"\"\" np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random. ### START CODE HERE ### (≈ 4 lines of code) W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y, 1)) ### END CODE HERE ### assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = &#123;\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2&#125; return parameters 3. loop在这里可以使用sigmoid()来做输出层的函数，np.tanh()来做hidden layer的激活函数。 3.1 forward propagation在这个函数中，输入的是X，和parameters，然后就可以根据 $$z^{[1] (i)} = W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$ 得到每一层的Z和A了。 123456789101112131415161718192021222324252627282930313233343536# GRADED FUNCTION: forward_propagationdef forward_propagation(X, parameters): \"\"\" Argument: X -- input data of size (n_x, m) parameters -- python dictionary containing your parameters (output of initialization function) Returns: A2 -- The sigmoid output of the second activation cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\" \"\"\" # Retrieve each parameter from the dictionary \"parameters\" ### START CODE HERE ### (≈ 4 lines of code) W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] ### END CODE HERE ### # Implement Forward Propagation to calculate A2 (probabilities) ### START CODE HERE ### (≈ 4 lines of code) Z1 = np.dot(W1,X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2,A1) + b2 A2 = sigmoid(Z2) ### END CODE HERE ### assert(A2.shape == (1, X.shape[1])) cache = &#123;\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2&#125; return A2, cache 3.2 cost接下来，在得到A2的值后，就可以根据公式来计算损失函数了。 $$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small$$ 在这里需要注意的是交叉熵的计算，交叉熵使用np.multiply()来计算，然后用np.sum()，求和。 而单单计算logprobs = np.multiply(np.log(A2),Y)是不够的，因为这个只得到了公式的前一半的部分，Y=0的部分在元素相乘中就相当于没有了，所以还要再后面加一项np.multiply(np.log(1-A2),1-Y) 123456789101112131415161718192021222324252627# GRADED FUNCTION: compute_costdef compute_cost(A2, Y, parameters): \"\"\" Computes the cross-entropy cost given in equation (13) Arguments: A2 -- The sigmoid output of the second activation, of shape (1, number of examples) Y -- \"true\" labels vector of shape (1, number of examples) parameters -- python dictionary containing your parameters W1, b1, W2 and b2 Returns: cost -- cross-entropy cost given equation (13) \"\"\" m = Y.shape[1] # number of example # Compute the cross-entropy cost ### START CODE HERE ### (≈ 2 lines of code) logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),1-Y) cost = -1 / m * np.sum(logprobs) ### END CODE HERE ### cost = np.squeeze(cost) # makes sure cost is the dimension we expect. # E.g., turns [[17]] into 17 assert(isinstance(cost, float)) return cost 3.3 backworad propagationNG说神经网络中最难理解的是这个，但是现在公式已经帮我们推倒好了。 其中， $g^{[1]’}(Z^{[1]})$ using (1 - np.power(A1, 2)) 可以看到，公式中需要的变量有X,Y,A,W,然后输出一个字典结构的grads 12345678910111213141516171819202122232425262728293031323334353637383940414243def backward_propagation(parameters, cache, X, Y): \"\"\" Implement the backward propagation using the instructions above. Arguments: parameters -- python dictionary containing our parameters cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\". X -- input data of shape (2, number of examples) Y -- \"true\" labels vector of shape (1, number of examples) Returns: grads -- python dictionary containing your gradients with respect to different parameters \"\"\" m = X.shape[1] # First, retrieve W1 and W2 from the dictionary \"parameters\". ### START CODE HERE ### (≈ 2 lines of code) W1 = parameters['W1'] W2 = parameters['W2'] ### END CODE HERE ### # Retrieve also A1 and A2 from dictionary \"cache\". ### START CODE HERE ### (≈ 2 lines of code) A1 = cache['A1'] A2 = cache['A2'] ### END CODE HERE ### # Backward propagation: calculate dW1, db1, dW2, db2. ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above) dZ2 = A2 - Y dW2 = 1 / m * np.dot(dZ2, A1.T) db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True) dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) dW1 = 1 / m * np.dot(dZ1, X.T) db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True) ### END CODE HERE ### grads = &#123;\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2&#125; return grads 3.4 update parameters最后根据得到的grads，乘上学习速率，就可以更新参数了。 12345678910111213141516171819202122232425262728293031323334353637383940414243# GRADED FUNCTION: update_parametersdef update_parameters(parameters, grads, learning_rate = 1.2): \"\"\" Updates parameters using the gradient descent update rule given above Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients Returns: parameters -- python dictionary containing your updated parameters \"\"\" # Retrieve each parameter from the dictionary \"parameters\" ### START CODE HERE ### (≈ 4 lines of code) W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] ### END CODE HERE ### # Retrieve each gradient from the dictionary \"grads\" ### START CODE HERE ### (≈ 4 lines of code) dW1 = grads['dW1'] db1 = grads['db1'] dW2 = grads['dW2'] db2 = grads['db2'] ## END CODE HERE ### # Update rule for each parameter ### START CODE HERE ### (≈ 4 lines of code) W1 = W1 - learning_rate * dW1 b1 = b1 - learning_rate * db1 W2 = W2 - learning_rate * dW2 b2 = b2 - learning_rate * db2 ### END CODE HERE ### parameters = &#123;\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2&#125; return parameters 然后把更新完的参数再传入前面的循环中，不断循环，直到达到循环的次数。 nn_model把前面的函数都调用过来。 模型中传入的参数是，X,Y，和迭代次数 首先需要得到你要设计的神经网络结构，调用layer_sizes()得到了n_x,n_y，也就是输入层和输出层。 初始化参数initialize_parameters(n_x, n_h, n_y),得到初始化的 W1, b1, W2, b2 然后开始循环 使用forward_propagation(X, parameters),先得到各个神经元的计算值。 然后compute_cost(A2, Y, parameters),得到cost backward_propagation(parameters, cache, X, Y)计算出每一步的梯度 update_parameters(parameters, grads)更新一下参数 返回训练完的parameters 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# GRADED FUNCTION: nn_modeldef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False): \"\"\" Arguments: X -- dataset of shape (2, number of examples) Y -- labels of shape (1, number of examples) n_h -- size of the hidden layer num_iterations -- Number of iterations in gradient descent loop print_cost -- if True, print the cost every 1000 iterations Returns: parameters -- parameters learnt by the model. They can then be used to predict. \"\"\" np.random.seed(3) n_x = layer_sizes(X, Y)[0] n_y = layer_sizes(X, Y)[2] # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\". ### START CODE HERE ### (≈ 5 lines of code) parameters = initialize_parameters(n_x, n_h, n_y) W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] ### END CODE HERE ### # Loop (gradient descent) for i in range(0, num_iterations): ### START CODE HERE ### (≈ 4 lines of code) # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\". A2, cache = forward_propagation(X, parameters) # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\". cost = compute_cost(A2, Y, parameters) # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\". grads = backward_propagation(parameters, cache, X, Y) # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\". parameters = update_parameters(parameters, grads) ### END CODE HERE ### # Print the cost every 1000 iterations if print_cost and i % 1000 == 0: print (\"Cost after iteration %i: %f\" %(i, cost)) return parameters 预测得到训练后的parameters，再用forward_propagation(X, parameters)计算出输出层最终的值A2，以0.5为分界，分为0和1。 123456789101112131415161718192021# GRADED FUNCTION: predictdef predict(parameters, X): \"\"\" Using the learned parameters, predicts a class for each example in X Arguments: parameters -- python dictionary containing your parameters X -- input data of size (n_x, m) Returns predictions -- vector of predictions of our model (red: 0 / blue: 1) \"\"\" # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold. ### START CODE HERE ### (≈ 2 lines of code) A2, cache = forward_propagation(X, parameters) predictions = (A2 &gt; 0.5) ### END CODE HERE ### return predictions 123456# Build a model with a n_h-dimensional hidden layerparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)# Plot the decision boundaryplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)plt.title(\"Decision Boundary for hidden layer size \" + str(4)) 可以看到，训练后神经网络得到的分界线更为合理。 123# Print accuracypredictions = predict(parameters, X)print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%') 准确率高达90% 优化参数这个时候就可以设置不同的hidden_layer的维度大小[1, 2, 3, 4, 5, 20, 50] 123456789101112# This may take about 2 minutes to runplt.figure(figsize=(16, 32))hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]for i, n_h in enumerate(hidden_layer_sizes): plt.subplot(5, 2, i+1) plt.title('Hidden Layer of size %d' % n_h) parameters = nn_model(X, Y, n_h, num_iterations = 5000) plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y) predictions = predict(parameters, X) accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) print (\"Accuracy for &#123;&#125; hidden units: &#123;&#125; %\".format(n_h, accuracy)) 1234567Accuracy for 1 hidden units: 67.5 %Accuracy for 2 hidden units: 67.25 %Accuracy for 3 hidden units: 90.75 %Accuracy for 4 hidden units: 90.5 %Accuracy for 5 hidden units: 91.25 %Accuracy for 20 hidden units: 90.0 %Accuracy for 50 hidden units: 90.25 % 得到的结果在n_h = 5时有最大值。","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"homework","slug":"homework","permalink":"http://fangzh.top/tags/homework/"},{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"DeepLearning.ai笔记:(1-3)-- 浅层神经网络（Shallow neural networks）","slug":"dl-ai-1-3","date":"2018-09-12T07:34:23.000Z","updated":"2018-09-30T07:17:02.644Z","comments":true,"path":"2018/2018091215/","link":"","permalink":"http://fangzh.top/2018/2018091215/","excerpt":"前面两周讲的是一些logistic和向量化的内容，以及numpy的基本使用，在他之前的机器学习课程中已经讲过了，这里就不再赘述。Week3主要讲了如何搭建两层的神经网络。","text":"前面两周讲的是一些logistic和向量化的内容，以及numpy的基本使用，在他之前的机器学习课程中已经讲过了，这里就不再赘述。Week3主要讲了如何搭建两层的神经网络。 神经网络的表示 这周的内容就围绕着这一张图来讲。 $$a_{j}^{[i]}$$ 这就是每一层神经元的表达方式，上标中括号[]，表示是第几层的神经元；下标表示这个是某一层的第几个神经元。 Input Layer：输入层，也用$a_{j}^{[0]}$，表示第0层 Hidden Layer：表示除了最后一层输出层以外的内部隐藏层 Output Layer：输出层，表示最后一层 而通常神经网络的层数一般不包括输入层。 $w^{[i]}$：每一层的参数$w$的维度是（该层神经元个数，前面一层神经元个数） $b^{[i]}$：为（每一层的神经元个数，1） 计算单个数据的神经网络 由此得到，计算单个数据的神经网络只需要4步： $$z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$$ $$a^{[1]} = \\sigma(z^{[1]})$$ $$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$ $$a^{[2]} = \\sigma(z^{[2]})$$ 多数据的向量化表示我们知道，多个数据的表示就是$x^{(i)}$，使用小括号的上标。神经元也是一样。 如$a^{[1] (i)}$表示第1层神经元的第i个样本。 那么如果有m个样本，一直做for循环来计算出这些神经元的值，实在是太慢了，所以跟logistic一样，可以直接用向量化来表示，这个时候用大写字母来表示。 $$Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]}$$ $$A^{[1]} = \\sigma(Z^{[1]})$$ $$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$$ $$A^{[2]} = \\sigma(Z^{[2]})$$ 这个时候，例如$A^{[1]}$是一个$(n,m)$的矩阵，m是样本数，每一列表示一个样本，n是该层的神经元个数。 从水平上看，矩阵 A代表了各个训练样本。竖直上看，A的不同索引对应不用的隐藏单元。 对矩阵Z和X也是类似，水平方向对应不同的样本，竖直方向上对应不同的输入特征，也就是神经网络输入层的各个节点。 激活函数 在此前都是用sigmoid作为激活函数的。但是激活函数不只有这一种，常用的有4种，分别是：sigmoid, tanh, ReLu, Leaky ReLu。 sigmoid: $a = \\frac{1}{1 + e^{-z}}$ 导数：$a^{\\prime} = a(1-a)$ tanh: $a = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ 导数：$a^{\\prime} = 1 - a^2$ ReLu(修正线性单元): $a = max(0, z)$ Leaky ReLu: $a = max(0.01z, z)$ tips: tanh函数在值域上处于-1和+1之间，所以均值更接近0，使用tanh比sigmoid更能够中心化数据，使得平均值接近0，而不是0.5。 tanh在大多数场合都是优于sigmoid的。 但是sigmoid和tanh有共同的缺点就是z在特别大或者特别小的时候，梯度很小，收敛速度很慢。 而ReLu弥补了两者的不足，在$z &gt; 0$时，梯度始终为1，提高了速度。 Leaky ReLu保证了$z &lt; 0$时，梯度不为0，但是实际上效果差不多。 结论： sigmoid：除了输出层是一个二分类问题的时候使用，不然基本不用 tanh：几乎适用于任何场合 ReLu：默认使用这个，如果不确定你要用哪个激活函数，那就选ReLu或者Leaky ReLu 为什么要使用非线性的激活函数如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与只有一个隐藏层效果相当，这种情况就是多层感知机（MLP）了。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。 梯度下降法公式 这里给出了浅层神经网络的梯度下降法公式。其中$g^{[1]’}(Z^{[1]})$表示你的激活函数的导数。 参数随机初始化在神经网络中,如果将参数全部初始化为0 会导致一个问题，例如对于上面的神经网络的例子，如果将参数全部初始化为0，在每轮参数更新的时候，与输入单元相关的两个隐藏单元的结果将是相同的。 所以初始化时，W要随机初始化，b不存在对称性问题，所以可以设置为0 12W = np.random.rand((2,2))* 0.01b = np.zero((2,1)) 将W乘以0.01是为了让W初始化足够小，因为如果很大的话，Z就很大，用sigmoid或者tanh时，所得到的梯度就会很小，训练过程会变慢。 ReLU和Leaky ReLU作为激活函数时，不存在这种问题，因为在大于0的时候，梯度均为1。 好好做作业，才能有更深的体会！","categories":[{"name":"AI","slug":"AI","permalink":"http://fangzh.top/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://fangzh.top/categories/AI/Deep-Learning/"}],"tags":[{"name":"dl.ai","slug":"dl-ai","permalink":"http://fangzh.top/tags/dl-ai/"}]},{"title":"hexo中输入数学公式","slug":"mathjax","date":"2018-09-12T05:39:33.000Z","updated":"2018-09-30T07:03:07.189Z","comments":true,"path":"2018/2018091212/","link":"","permalink":"http://fangzh.top/2018/2018091212/","excerpt":"hexo通过MathJax渲染Latex公式。","text":"hexo通过MathJax渲染Latex公式。 开启hueman主题比较简单，在主题配置文件中找到mathjax： 1mathjax: True 这样就可以了。 页面插入公式插入有两种形式，一种是在行内直接插入，不居中显示： 1$math$ 另一种是在行间插入公式，居中显示： 1$$math$$ 基本语法上下标 ^上标，_表示下标 1234$$a_&#123;1&#125; x^&#123;2&#125; $$$$e^&#123;-\\alpha t&#125; $$$$a^&#123;i&#125;_&#123;ij&#125;$$$$e^&#123;x^2&#125; \\neq &#123;e^x&#125;^2$$ $$a_{1} x^{2}$$$$e^{-\\alpha t}$$$$a^{i}_{ij}$$$$e^{x^2} \\neq {e^x}^2$$ 此外，如果左右两边都有上下标，则使用 \\sideset 命令，效果如下：1\\sideset&#123;^xy&#125;&#123;^xy&#125;\\bigotimes $$\\sideset{^xy}{^xy}\\bigotimes$$ 平方根 平方根输入命令为 \\sqrt，n次方根命令为 \\sqrt[n]，其符号大小由LaTeX 自动给定：12$$\\sqrt&#123;x&#125;$$ $$\\sqrt&#123;x^2+\\sqrt&#123;y&#125;$$ $$\\sqrt[3]&#123;2&#125;$$$$\\sqrt&#123;x&#125;$$ $$ \\sqrt{x^2+\\sqrt{y}}$$$$\\sqrt[3]{2}$$ 水平线使用 \\overline 和 \\underline 分别在表达式上下方画出水平线：12$$\\overline&#123;m + n&#125;$$$$\\underline&#123;m + n&#125;$$ $$\\overline{m + n}$$$$\\underline{m + n}$$ 水平大括号命令 \\overbrace 和 \\underrace，效果如下：12$$\\underbrace&#123;a+b+\\cdots+z&#125;$$$$\\overbrace&#123;a+b+\\cdots+z&#125;$$ $$\\overbrace{a+b+\\cdots+z}$$$$\\underbrace{a+b+\\cdots+z}$$ 矢量矢量的命令是 \\vec，用于单个字母的向量表示。\\overrightarrow 和\\overleftarrow 分别表示向右和向左的向量箭头：123$$\\vec&#123;a&#125;$$$$\\overrightarrow&#123;AB&#125;$$$$\\overleftarrow&#123;BA&#125;$$ $$\\vec{a}$$$$\\overrightarrow{AB}$$$$\\overleftarrow{BA}$$ 分数分数使用 \\frac{…}{…} 进行排版：123$$1\\frac&#123;1&#125;&#123;2&#125;$$$$\\frac&#123;x^2&#125;&#123;k+1&#125;$$$$x^&#123;1/2&#125;$$ $$1\\frac{1}{2}$$$$\\frac{x^2}{k+1}$$$$x^{1/2}$$ 积分运算符积分运算符使用 \\int 生成。求和运算符使用 \\sum 生成。乘积运算符使用 \\prod 生成。上下限使用^ 和_ 命令，类似 上下标：123$$\\sum_&#123;i=1&#125;^&#123;n&#125;$$$$\\int_&#123;0&#125;^&#123;\\frac&#123;\\pi&#125;&#123;2&#125;&#125;$$$$\\prod_\\epsilon$$ $$\\sum_{i=1}^{n}$$$$\\int_{0}^{\\frac{\\pi}{2}}$$$$\\prod_\\epsilon$$ 希腊字母 $\\alpha$ \\alpha $\\beta$ \\beta $\\gamma$ \\gamma $\\delta$ \\delta $\\epsilon$ \\epsilon 字体转换要对公式的某一部分字符进行字体转换，可以用{\\rm需转换的部分字符}命令，其中\\rm可以参照下表选择合适的字体。一般情况下，公式默认为意大利体。 123456789\\rm 罗马体 \\rm test \\it 意大利体 \\it test\\bf 黑体 \\bf test \\cal 花体 \\cal test\\sl 倾斜体 \\sl test \\sf 等线体 \\sf test\\mit 数学斜体 \\mit test \\tt 打字机字体 \\tt test\\sc 小体大写字母 \\sc test","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"博客搭建","slug":"日常技术/博客搭建","permalink":"http://fangzh.top/categories/日常技术/博客搭建/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://fangzh.top/tags/hexo/"},{"name":"blog","slug":"blog","permalink":"http://fangzh.top/tags/blog/"}]},{"title":"hexo教程:搜索SEO+阅读量统计+访问量统计+评论系统(3)","slug":"hexo-3","date":"2018-09-10T10:09:38.000Z","updated":"2018-09-30T07:08:10.753Z","comments":true,"path":"2018/2018090918/","link":"","permalink":"http://fangzh.top/2018/2018090918/","excerpt":"网站做完之后，可以为网站添加一些常用的功能，如能被搜索引擎收录的SEO优化，网站访问量和文章阅读量统计，以及评论系统。","text":"网站做完之后，可以为网站添加一些常用的功能，如能被搜索引擎收录的SEO优化，网站访问量和文章阅读量统计，以及评论系统。 本文参考了: visugar.com这里面说的很详细了。 1. SEO优化推广是很麻烦的事情，怎么样别人才能知道我们呢，首先需要让搜索引擎收录你的这个网站，别人才能搜索的到。那么这就需要SEO优化了。 SEO是由英文Search Engine Optimization缩写而来， 中文意译为“搜索引擎优化”。SEO是指通过站内优化比如网站结构调整、网站内容建设、网站代码优化等以及站外优化。 百度seo刚建站的时候是没有搜索引擎收录我们的网站的。可以在搜索引擎中输入site:&lt;域名&gt; 来查看一下。 1. 登录百度站长平台添加网站 登录百度站长平台，在站点管理中添加你自己的网站。 验证网站有三种方式：文件验证、HTML标签验证、CNAME验证。 第三种方式最简单，只要将它提供给你的那个xxxxx使用CNAME解析到xxx.baidu.com就可以了。也就是登录你的阿里云，把这个解析填进去就OK了。 2. 提交链接 我们需要使用npm自动生成网站的sitemap，然后将生成的sitemap提交到百度和其他搜索引擎 12npm install hexo-generator-sitemap --save npm install hexo-generator-baidu-sitemap --save 这时候你需要在你的根目录下_config.xml中看看url有没有改成你自己的： 重新部署后，就可以在public文件夹下看到生成的sitemap.xml和baidusitemap.xml了。 然后就可以向百度提交你的站点地图了。 这里建议使用自动提交。 自动提交又分为三种：主动推送、自动推送、sitemap。 可以三个一起提交不要紧，我选择的是后两种。 自动推送：把百度生成的自动推送代码，放在主题文件/layout/common/head.ejs的适当位置，然后验证一下就可以了。 sitemap：把两个sitemap地址，提交上去，看到状态正常就OK了。 ps: 百度收录比较慢，慢慢等个十天半个月再去site:&lt;域名&gt;看看有没有被收录。 google的SEO流程一样，google更简单，而且收录更快，进入google站点地图，提交网站和sitemap.xml，就可以了。 如果你这个域名在google这里出了问题，那你就提交 yourname.github.io，这个链接，效果是一样的。 不出意外的话一天内google就能收录你的网站了。 其他的搜索，如搜狗搜索，360搜索，流程是一样的，这里就不再赘述。 2. 评论系统评论系统有很多，但是很多都是墙外的用不了，之前说过这个valine好像集成在hueman和next主题里面了，但是我还没有研究过，我看的是visugar这个博主用的来比力评论系统，感觉也还不错。 来比力官网，注册好后，点击管理页面，在代码管理中找到安装代码： 获取安装代码后，在主题的comment下新建一个文件放入刚刚那段代码，再找到article文件，找到如下代码，若没有则直接在footer后面添加即可。livebe即为刚刚所创文件名称。 1&lt;%- partial(&apos;comment/livebe&apos;) %&gt; 然后可以自己设置一些东西： 还可以设置评论提醒，这样别人评论你的时候就可以及时知道了。 3. 添加百度统计百度统计可以在后台上看到你网站的访问数，浏览量，浏览链接分布等很重要的信息。所以添加百度统计能更有效的让你掌握你的网站情况。 百度统计，注册一下，这里的账号好像和百度账号不是一起的。 照样把代码复制到head.ejs文件中，然后再进行一下安装检查，半小时左右就可以在百度统计里面看到自己的网站信息了。 4. 文章阅读量统计leanCloudleanCloud，进去后注册一下，进入后创建一个应用： 在存储中创建Class，命名为Counter, 然后在设置页面看到你的应用Key，在主题的配置文件中： 1234leancloud_visitors: enable: true app_id: 你的id app_key: 你的key 在article.ejs中适当的位置添加如下，这要看你让文章的阅读量统计显示在哪个地方了， 1阅读数量:&lt;span id=&quot;&lt;%= url_for(post.path) %&gt;&quot; class=&quot;leancloud_visitors&quot; data-flag-title=&quot;&lt;%- post.title %&gt;&quot;&gt;&lt;/span&gt;次 然后在footer.ejs的最后，添加： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;script src=&quot;//cdn1.lncld.net/static/js/2.5.0/av-min.js&quot;&gt;&lt;/script&gt;&lt;script&gt; var APP_ID = &apos;你的app id&apos;; var APP_KEY = &apos;你的app key&apos;; AV.init(&#123; appId: APP_ID, appKey: APP_KEY &#125;); // 显示次数 function showTime(Counter) &#123; var query = new AV.Query(&quot;Counter&quot;); if($(&quot;.leancloud_visitors&quot;).length &gt; 0)&#123; var url = $(&quot;.leancloud_visitors&quot;).attr(&apos;id&apos;).trim(); // where field query.equalTo(&quot;words&quot;, url); // count query.count().then(function (number) &#123; // There are number instances of MyClass where words equals url. $(document.getElementById(url)).text(number? number : &apos;--&apos;); &#125;, function (error) &#123; // error is an instance of AVError. &#125;); &#125; &#125; // 追加pv function addCount(Counter) &#123; var url = $(&quot;.leancloud_visitors&quot;).length &gt; 0 ? $(&quot;.leancloud_visitors&quot;).attr(&apos;id&apos;).trim() : &apos;icafebolger.com&apos;; var Counter = AV.Object.extend(&quot;Counter&quot;); var query = new Counter; query.save(&#123; words: url &#125;).then(function (object) &#123; &#125;) &#125; $(function () &#123; var Counter = AV.Object.extend(&quot;Counter&quot;); addCount(Counter); showTime(Counter); &#125;);&lt;/script&gt; 重新部署后就可以了。 5. 引入不蒜子访问量和访问人次统计不蒜子的添加非常非常方便，不蒜子 在footer.ejs中的合适位置，看你要显示在哪个地方，添加： 1234&lt;!--这一段是不蒜子的访问量统计代码--&gt;&lt;script async src=&quot;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;&lt;span id=&quot;busuanzi_container_site_pv&quot;&gt;本站总访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次 &amp;nbsp; &lt;/span&gt;&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt;访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人次&lt;/span&gt; 就可以了。 总结到这里就基本做完了。其实都是参考别的博主的设置的，不一定仅限于hueman主题，其他主题的设置也是大体相同的，所以如果你希望设置别的主题，那么仔细看一下这个主题的代码结构，也能够把上边的功能添加进去。 多看看别的博主的那些功能，如果有你能找到自己喜欢的功能，那么好好发动搜索技能，很快就能找到怎么做了。加油吧！","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"博客搭建","slug":"日常技术/博客搭建","permalink":"http://fangzh.top/categories/日常技术/博客搭建/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://fangzh.top/tags/hexo/"},{"name":"blog","slug":"blog","permalink":"http://fangzh.top/tags/blog/"},{"name":"教程","slug":"教程","permalink":"http://fangzh.top/tags/教程/"}]},{"title":"你见过什么样的云霄？","slug":"yx","date":"2018-09-10T07:21:32.000Z","updated":"2018-09-30T07:02:24.596Z","comments":true,"path":"2018/2018091015/","link":"","permalink":"http://fangzh.top/2018/2018091015/","excerpt":"","text":"花了三年时间做的家乡的航拍视频，有点生涩，顺便放上来试试hexo的视频嵌入。 你见过什么样的云霄？ 123456789&lt;iframe height=300 width=510 src=&apos;http://player.youku.com/embed/XMzc4NzA3Njg0MA==&apos; frameborder=0 allowfullscreen&gt;&lt;/iframe&gt;","categories":[{"name":"生活","slug":"生活","permalink":"http://fangzh.top/categories/生活/"},{"name":"旅行","slug":"生活/旅行","permalink":"http://fangzh.top/categories/生活/旅行/"}],"tags":[{"name":"航拍","slug":"航拍","permalink":"http://fangzh.top/tags/航拍/"},{"name":"旅行","slug":"旅行","permalink":"http://fangzh.top/tags/旅行/"},{"name":"视频","slug":"视频","permalink":"http://fangzh.top/tags/视频/"}]},{"title":"hexo教程:基本配置+更换主题+多终端工作+coding page部署分流(2)","slug":"hexo-2","date":"2018-09-07T07:18:31.000Z","updated":"2018-09-30T07:11:09.801Z","comments":true,"path":"2018/2018090715/","link":"","permalink":"http://fangzh.top/2018/2018090715/","excerpt":"上次介绍了hexo的基本搭建和部署。但是还有很多事情没有解决，这次先来看看hexo的基本配置文件，还有如何在多平台部署实现国内外分流，以及换电脑后如何无缝的衔接工作。","text":"上次介绍了hexo的基本搭建和部署。但是还有很多事情没有解决，这次先来看看hexo的基本配置文件，还有如何在多平台部署实现国内外分流，以及换电脑后如何无缝的衔接工作。 1. hexo基本配置在文件根目录下的_config.yml，就是整个hexo框架的配置文件了。可以在里面修改大部分的配置。详细可参考官方的配置描述。 网站 参数 描述 title 网站标题 subtitle 网站副标题 description 网站描述 author 您的名字 language 网站使用的语言 timezone 网站时区。Hexo 默认使用您电脑的时区。时区列表。比如说：America/New_York, Japan, 和 UTC 。 其中，description主要用于SEO，告诉搜索引擎一个关于您站点的简单描述，通常建议在其中包含您网站的关键词。author参数用于主题显示文章的作者。 网址 参数 描述 url 网址 root 网站根目录 permalink 文章的 永久链接 格式 permalink_defaults 永久链接中各部分的默认值 在这里，你需要把url改成你的网站域名。 permalink，也就是你生成某个文章时的那个链接格式。 比如我新建一个文章叫temp.md，那么这个时候他自动生成的地址就是http://yoursite.com/2018/09/05/temp。 以下是官方给出的示例，关于链接的变量还有很多，需要的可以去官网上查找 永久链接 。 参数 结果 :year/:month/:day/:title/ 2013/07/14/hello-world :year-:month-:day-:title.html 2013-07-14-hello-world.html :category/:title foo/bar/hello-world 再往下翻，中间这些都默认就好了。 12345678theme: landscape# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: &lt;repository url&gt; branch: [branch] theme就是选择什么主题，也就是在theme这个文件夹下，在官网上有很多个主题，默认给你安装的是lanscape这个主题。当你需要更换主题时，在官网上下载，把主题的文件放在theme文件夹下，再修改这个参数就可以了。 接下来这个deploy就是网站的部署的，repo就是仓库(Repository)的简写。branch选择仓库的哪个分支。这个在之前进行github page部署的时候已经修改过了，不再赘述。而这个在后面进行双平台部署的时候会再次用到。 Front-matterFront-matter 是文件最上方以 --- 分隔的区域，用于指定个别文件的变量，举例来说： 123title: Hello Worlddate: 2013/7/13 20:46:25--- 下是预先定义的参数，您可在模板中使用这些参数值并加以利用。 参数 描述 layout 布局 title 标题 date 建立日期 updated 更新日期 comments 开启文章的评论功能 tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章网址 其中，分类和标签需要区别一下，分类具有顺序性和层次性，也就是说 Foo, Bar 不等于 Bar, Foo；而标签没有顺序和层次。 12345categories:- Diarytags:- PS3- Games layout（布局）当你每一次使用代码 1hexo new paper 它其实默认使用的是post这个布局，也就是在source文件夹下的_post里面。 Hexo 有三种默认布局：post、page 和 draft，它们分别对应不同的路径，而您自定义的其他布局和 post 相同，都将储存到 source/_posts 文件夹。 布局 路径 post source/_posts page source draft source/_drafts 而new这个命令其实是： 1hexo new [layout] &lt;title&gt; 只不过这个layout默认是post罢了。 page如果你想另起一页，那么可以使用 1hexo new page board 系统会自动给你在source文件夹下创建一个board文件夹，以及board文件夹中的index.md，这样你访问的board对应的链接就是http://xxx.xxx/board draftdraft是草稿的意思，也就是你如果想写文章，又不希望被看到，那么可以 1hexo new draft newpage 这样会在source/_draft中新建一个newpage.md文件，如果你的草稿文件写的过程中，想要预览一下，那么可以使用 1hexo server --draft 在本地端口中开启服务预览。 如果你的草稿文件写完了，想要发表到post中， 1hexo publish draft newpage 就会自动把newpage.md发送到post中。 2. 更换主题到这一步，如果你觉得默认的landscape主题不好看，那么可以在官网的主题中，选择你喜欢的一个主题进行修改就可以啦。点这里 这里有200多个主题可以选。不过最受欢迎的就是那么几个，比如NexT主题，非常的简洁好看，大多数人都选择这个，关于这个的教程也比较多。不过我选择的是hueman这个主题，好像是从WordPress移植过来的，展示效果如下： 不管怎么样，至少是符合我个人的审美。 直接在github链接上下载下来，然后放到theme文件夹下就行了，然后再在刚才说的配置文件中把theme换成那个主题文件夹的名字，它就会自动在theme文件夹中搜索你配置的主题。 而后进入hueman这个文件夹，可以看到里面也有一个配置文件_config.xml，貌似它默认是_config.xml.example，把它复制一份，重命名为_config.xml就可以了。这个配置文件是修改你整个主题的配置文件。 menu（菜单栏）也就是上面菜单栏上的这些东西。 其中，About这个你是找不到网页的，因为你的文章中没有about这个东西。如果你想要的话，可以执行命令 1hexo new page about 它就会在根目录下source文件夹中新建了一个about文件夹，以及index.md，在index.md中写上你想要写的东西，就可以在网站上展示出来了。 如果你想要自己再自定义一个菜单栏的选项，那么就 1hexo new page yourdiy 然后在主题配置文件的menu菜单栏添加一个 Yourdiy : /yourdiy，注意冒号后面要有空格，以及前面的空格要和menu中默认的保持整齐。然后在languages文件夹中，找到zh-CN.yml，在index中添加yourdiy: &#39;中文意思&#39;就可以显示中文了。 customize(定制)在这里可以修改你的个人logo，默认是那个hueman，在source/css/images文件夹中放入自己要的logo，再改一下url的链接名字就可以了。 favicon是网站中出现的那个小图标的icon，找一张你喜欢的logo，然后转换成ico格式，放在images文件夹下，配置一下路径就行。 social_links ，可以显示你的社交链接，而且是有logo的。 tips: 在这里可以添加一个rss功能，也就是那个符号像wifi一样的东西。 添加RSS1. 什么是RSS？ RSS也就是订阅功能，你可以理解为类似与订阅公众号的功能，来订阅各种博客，杂志等等。 2. 为什么要用RSS？ 就如同订阅公众号一样，你对某个公众号感兴趣，你总不可能一直时不时搜索这个公众号来看它的文章吧。博客也是一样，如果你喜欢某个博主，或者某个平台的内容，你可以通过RSS订阅它们，然后在RSS阅读器上可以实时推送这些消息。现在网上的垃圾消息太多了，如果你每一天都在看这些消息中度过，漫无目的的浏览，只会让你的时间一点一点的流逝，太不值得了。如果你关注的博主每次都发的消息都是精华，而且不是每一天十几条几十条的轰炸你，那么这个博主就值得你的关注，你就可以通过RSS订阅他。 在我的理解中，如果你不想每天都被那些没有质量的消息轰炸，只想安安静静的关注几个博主，每天看一些有质量的内容也不用太多，那么RSS订阅值得你的拥有。 3. 添加RSS功能 先安装RSS插件 1npm i hexo-generator-feed 而后在你整个项目的_config.yml中找到Extensions，添加： 12345678910# Extensions## Plugins: https://hexo.io/plugins/#RSS订阅plugin:- hexo-generator-feed#Feed Atomfeed: type: atom path: atom.xml limit: 20 这个时候你的RSS链接就是 域名/atom.xml了。 所以，在主题配置文件中的这个social links，开启RSS的页面功能，这样你网站上就有那个像wifi一样符号的RSS logo了，注意空格。 1rss: /atom.xml 4. 如何关注RSS？ 首先，你需要一个RSS阅读器，在这里我推荐inoreader，宇宙第一RSS阅读器，而且中文支持的挺好。不过它没有PC端的程序，只有网页版，chrome上有插件。在官网上用google账号或者自己注册账号登录，就可以开始你的关注之旅了。 每次需要关注某个博主时，就点开他的RSS链接，把链接复制到inoreader上，就能关注了，当然，如果是比较大众化的很厉害的博主，你直接搜名字也可以的，比如每个人都非常佩服的阮一峰大师，直接在阅读器上搜索阮一峰，应该就能出来了。 我关注的比如，阮一峰的网络日志，月光博客，知乎精选等，都很不错。当然，还有我！！赶快关注我吧！你值得拥有：http://fangzh.top/atom.xml 在安卓端，inoreader也有下载，不过因为国内google是登录不了的，你需要在inoreader官网上把你的密码修改了，然后就可以用账户名和密码登录了。 在IOS端，没用过，好像是reader 3可以支持inoreader账户，还有个readon也不错，可以去试试。 widgets(侧边栏)侧边栏的小标签，如果你想自己增加一个，比如我增加了一个联系方式，那么我把communication写在上面，在zh-CN.yml中的sidebar，添加communication: &#39;中文&#39;。 然后在hueman/layout/widget中添加一个communicaiton.ejs，填入模板： 12345678&lt;% if (site.posts.length) &#123; %&gt; &lt;div class=\"widget-wrap widget-list\"&gt; &lt;h3 class=\"widget-title\"&gt;&lt;%= __('sidebar.communiation') %&gt;&lt;/h3&gt; &lt;div class=\"widget\"&gt; &lt;!--这里添加你要写的内容--&gt; &lt;/div&gt; &lt;/div&gt;&lt;% &#125; %&gt; search(搜索框)默认搜索框是不能够用的， you need to install hexo-generator-json-content before using Insight Search 它已经告诉你了，如果想要使用，就安装这个插件。 comment(评论系统)这里的多数都是国外的，基本用不了。这个valine好像不错，还能统计文章阅读量，可以自己试一试，链接。 miscellaneous(其他)这里我就改了一个links，可以添加友链。注意空格要对！不然会报错！ 总结：整个主题看起来好像很复杂的样子，但是仔细捋一捋其实也比较流畅， languages: 顾名思义 layout：布局文件，其实后期想要修改自定义网站上的东西，添加各种各样的信息，主要是在这里修改，其中comment是评论系统，common是常规的布局，最常修改的在这里面，比如修改页面head和footer的内容。 scripts：js脚本，暂时没什么用 source：里面放了一些css的样式，以及图片 3. git分支进行多终端工作问题来了，如果你现在在自己的笔记本上写的博客，部署在了网站上，那么你在家里用台式机，或者实验室的台式机，发现你电脑里面没有博客的文件，或者要换电脑了，最后不知道怎么移动文件，怎么办？ 在这里我们就可以利用git的分支系统进行多终端工作了，这样每次打开不一样的电脑，只需要进行简单的配置和在github上把文件同步下来，就可以无缝操作了。 机制机制是这样的，由于hexo d上传部署到github的其实是hexo编译后的文件，是用来生成网页的，不包含源文件。 也就是上传的是在本地目录里自动生成的.deploy_git里面。 其他文件 ，包括我们写在source 里面的，和配置文件，主题文件，都没有上传到github 所以可以利用git的分支管理，将源文件上传到github的另一个分支即可。 上传分支首先，先在github上新建一个hexo分支，如图： 然后在这个仓库的settings中，选择默认分支为hexo分支（这样每次同步的时候就不用指定分支，比较方便）。 然后在本地的任意目录下，打开git bash， 1git clone git@github.com:ZJUFangzh/ZJUFangzh.github.io.git 将其克隆到本地，因为默认分支已经设成了hexo，所以clone时只clone了hexo。 接下来在克隆到本地的ZJUFangzh.github.io中，把除了.git 文件夹外的所有文件都删掉 把之前我们写的博客源文件全部复制过来，除了.deploy_git。这里应该说一句，复制过来的源文件应该有一个.gitignore，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下，表示这些类型文件不需要git： 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 注意，如果你之前克隆过theme中的主题文件，那么应该把主题文件中的.git文件夹删掉，因为git不能嵌套上传，最好是显示隐藏文件，检查一下有没有，否则上传的时候会出错，导致你的主题文件无法上传，这样你的配置在别的电脑上就用不了了。 而后 123git add .git commit –m \"add branch\"git push 这样就上传完了，可以去你的github上看一看hexo分支有没有上传上去，其中node_modules、public、db.json已经被忽略掉了，没有关系，不需要上传的，因为在别的电脑上需要重新输入命令安装 。 这样就上传完了。 更换电脑操作一样的，跟之前的环境搭建一样， 安装git 1sudo apt-get install git 设置git全局邮箱和用户名 12git config --global user.name &quot;yourgithubname&quot;git config --global user.email &quot;yourgithubemail&quot; 设置ssh key 12345ssh-keygen -t rsa -C &quot;youremail&quot;#生成后填到github和coding上（有coding平台的话）#验证是否成功ssh -T git@github.comssh -T git@git.coding.net #(有coding平台的话) 安装nodejs 12sudo apt-get install nodejssudo apt-get install npm 安装hexo 1sudo npm install hexo-cli -g 但是已经不需要初始化了， 直接在任意文件夹下， 1git clone git@……………… 然后进入克隆到的文件夹： 123cd xxx.github.ionpm installnpm install hexo-deployer-git --save 生成，部署： 12hexo ghexo d 然后就可以开始写你的新博客了 1hexo new newpage Tips: 不要忘了，每次写完最好都把源文件上传一下 123git add .git commit –m &quot;xxxx&quot;git push 如果是在已经编辑过的电脑上，已经有clone文件夹了，那么，每次只要和远端同步一下就行了 1git pull 4. coding page上部署实现国内外分流之前我们已经把hexo托管在github了，但是github是国外的，而且百度的爬虫是不能够爬取github的，所以如果你希望你做的博客能够在百度引擎上被收录，而且想要更快的访问，那么可以在国内的coding page做一个托管，这样在国内访问就是coding page，国外就走github page。 1. 申请coding账户，新建项目 先申请一个账户，然后创建新的项目，这一步项目名称应该是随意的。 2. 添加ssh key 这一步跟github一样。 添加后，检查一下是不是添加成功 1ssh -T git@git.coding.net 3. 修改_config.yml hexo官方文档是这样的： 123456deploy: type: git message: [message] repo: github: &lt;repository url&gt;,[branch] coding: &lt;repository url&gt;,[branch] 那么，我们只需要： 12345deploy: type: git repo: coding: git@git.coding.net:ZJUFangzh/ZJUFangzh.git,master github: git@github.com:ZJUFangzh/ZJUFangzh.github.io.git,master 4. 部署 保存一下，直接 12hexo ghexo d 这样就可以在coding的项目上看到你部署的文件了。 5. 开启coding pages服务，绑定域名 如图： 6. 阿里云添加解析 这个时候就可以把之前github的解析改成境外，把coding的解析设为默认了。 7. 去除coding page的跳转广告 coding page的一个比较恶心人的地方就是，你只是银牌会员的话，访问会先跳转到一个广告，再到你自己的域名。那么它也给出了消除的办法。右上角切换到coding的旧版界面，默认新版是不行的。然后再来到pages服务这里。 这里： 只要你在页面上添加一行文字，写Hosted by Coding Pages，然后点下面的小勾勾，两个工作日内它就会审核通过了。 1&lt;p&gt;Hosted by &lt;a href=&quot;https://pages.coding.me&quot; style=&quot;font-weight: bold&quot;&gt;Coding Pages&lt;/a&gt;&lt;/p&gt; 我的选择是把这一行代码放在主题文件夹/layout/common/footer.ejs里面，也就是本来在页面中看到的页脚部分。 当然，为了统一，我又在后面加上了and Github哈哈，可以不加。 1&lt;p&gt;&lt;span&gt;Hosted by &lt;a href=&quot;https://pages.coding.me&quot; style=&quot;font-weight: bold&quot;&gt;Coding Pages&lt;/a&gt;&lt;/span&gt; and &lt;span&gt;&lt;a href=&quot;https://github.com&quot; style=&quot;font-weight: bold&quot;&gt;Github&lt;/a&gt;&lt;/span&gt;&lt;/p&gt; 这是最终加上去的代码。 至此，关于hexo的基本文件配置，主题更换，多终端同步，多平台部署已经介绍完了。 这一次就先到这里了，下回再讲讲如何优化网站的SEO、以及在主题中添加评论系统、阅读量统计等等，谢谢大家。","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"博客搭建","slug":"日常技术/博客搭建","permalink":"http://fangzh.top/categories/日常技术/博客搭建/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://fangzh.top/tags/hexo/"},{"name":"blog","slug":"blog","permalink":"http://fangzh.top/tags/blog/"},{"name":"教程","slug":"教程","permalink":"http://fangzh.top/tags/教程/"}]},{"title":"Linux安装shadowcocks","slug":"Linux安装shadowcocks","date":"2018-09-05T13:53:29.000Z","updated":"2018-09-30T07:04:28.914Z","comments":true,"path":"2018/2018090522/","link":"","permalink":"http://fangzh.top/2018/2018090522/","excerpt":"对于windows来说，只要下载一个shadowsocks的应用程序就行了。 github上一大堆shadowsocks-windows Linux上，可以用shell命令行解决的，绝不用GUI。","text":"对于windows来说，只要下载一个shadowsocks的应用程序就行了。 github上一大堆shadowsocks-windows Linux上，可以用shell命令行解决的，绝不用GUI。 123sudo apt-get install python-pippip install shadowsocks 接下来配置文件 shadowsocks.json，随便找个地方，你记得住的地方保存。123456789101112131415161718&#123; \"server\":\"my_server_ip\", \"local_address\": \"127.0.0.1\", \"local_port\":1080, \"server_port\":my_server_port, \"password\":\"my_password\", \"timeout\":300, \"method\":\"aes-256-cfb\"&#125; my_server_ip:你的账户ip my_server_port:你的账户端口 my_password:你的账户密码 method:输入你账户的加密方式 配置完成后，分前端启动和后端启动 前端启动就是你那个窗口得一直开着 后面这一段是你刚才建立的json文件地址1sudo sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json 后端启动在后端自己挂着（推荐）1sudo sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json -d start 后端停止1sudo sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json -d stop 重启（修改配置后要重启才能生效） 1sudo sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json -d restart 在此，建议把命令行做成一个.sh文件，放在桌面，想开的时候就可以随时执行shadowsocks.sh 123#! /bin/bashsudo sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json -d start 配置好后，还需要在chrome浏览器中配置switchomega（插件），如果没有，自己去下一个。因为我们肯定是希望在指定的国外网站进行科学上网，而在国内的网站，就不需要用shadowsocks做转发了，这样很慢。所以配置一个有一定规则的列表，是很有必要的。详细的switchomega配置过程网上一大堆，这里就不详细说明了。 当然，如果你嫌麻烦，觉得以上用shell配置shadowsocks的方法太复杂，那直接下一个linux下的shadowsocks-Qt5吧。 还有安卓版的： shadowsocks-android","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"Linux","slug":"日常技术/Linux","permalink":"http://fangzh.top/categories/日常技术/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://fangzh.top/tags/Linux/"},{"name":"install","slug":"install","permalink":"http://fangzh.top/tags/install/"},{"name":"shadowsocks","slug":"shadowsocks","permalink":"http://fangzh.top/tags/shadowsocks/"}]},{"title":"Linux安装anaconda","slug":"Linux安装anaconda","date":"2018-09-05T13:52:53.000Z","updated":"2018-09-30T07:05:14.003Z","comments":true,"path":"2018/2018090521/","link":"","permalink":"http://fangzh.top/2018/2018090521/","excerpt":"Anaconda是python的一个很好的发行版，安装了anaconda就可以解决很多python第三方库的问题。","text":"Anaconda是python的一个很好的发行版，安装了anaconda就可以解决很多python第三方库的问题。 首先，检查一下电脑中的python版本。 123$ which python3/usr/bin/python3 此时调用的python3版本在/usr/bin/中。 1. Download AnacondaDownload Anaconda 2. 安装 Anaconda这里选择你下载的那个文件（可以用tab自动补全） 1bash ~/Download/Anaconda3-5.2.0-Linux-x86_64.sh 3. 添加入path输入： 1source ~/.bashrc 自动添加完毕。 如果不行，可以手动添加（慎用） 12echo &apos;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&apos; &gt;&gt; ~/.bashrcsource ~/.bashrc 这个时候，pip已经可以使用了。用which pip可以显示在anaconda的pip。 输入 python3，也显示的是anaconda的python3。 这时候如果需要调用系统自带的python 则需要输入 12345sudo python3 # 3.6.5#或者sudo python # 2.7 具体可以查看anaconda的使用帮助。","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"Linux","slug":"日常技术/Linux","permalink":"http://fangzh.top/categories/日常技术/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://fangzh.top/tags/Linux/"},{"name":"install","slug":"install","permalink":"http://fangzh.top/tags/install/"},{"name":"python","slug":"python","permalink":"http://fangzh.top/tags/python/"}]},{"title":"Linux安装selenium+chromedriver","slug":"Linux安装selenium-chromedriver","date":"2018-09-05T13:51:41.000Z","updated":"2018-09-30T07:04:51.555Z","comments":true,"path":"2018/2018090520/","link":"","permalink":"http://fangzh.top/2018/2018090520/","excerpt":"Selenium是爬虫中用来模拟JS的利器。 下面介绍一下Linux安装selenium和chromedriver的具体做法。","text":"Selenium是爬虫中用来模拟JS的利器。 下面介绍一下Linux安装selenium和chromedriver的具体做法。 1. install selenium首先确保已经安装了pip命令，接下来： 1sudo pip install -U selenium 2. install chromedriver在Chromedriver网站上找到对应的版本，一般是最新版，如果你选的版本和电脑上的Chrome不互相匹配的话，在运行爬虫的时候会报错。（在网站里面的LATEST_RELEASE中可以找到最新版，不一定按那个序号来的） 找到后，把下面的2.41改成你要安装的版本。1wget -N http://chromedriver.storage.googleapis.com/2.41/chromedriver_linux64.zip 然后 1234567unzip chromedriver_linux64.zip #解压你下载的那个包chmod +x chromedriver #修改用户权限为可执行sudo mv -f chromedriver /usr/local/share/chromedriver #将解压后的文件移动到指定目录#在指定目录link到别的目录sudo ln -s /usr/local/share/chromedriver /usr/local/bin/chromedriver sudo ln -s /usr/local/share/chromedriver /usr/bin/chromedriver 一通操作后，你的selenium和chromedriver应该可以正常使用了。 123456from selenium import webdriverdriver = webdriver.Chrome()driver.get('https://www.baidu.com/')print('打开浏览器')print(driver.title)driver.quit()","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"Linux","slug":"日常技术/Linux","permalink":"http://fangzh.top/categories/日常技术/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://fangzh.top/tags/Linux/"},{"name":"install","slug":"install","permalink":"http://fangzh.top/tags/install/"},{"name":"python","slug":"python","permalink":"http://fangzh.top/tags/python/"}]},{"title":"hexo教程：github page+独立域名搭建(1)","slug":"hexo(1)","date":"2018-09-05T05:38:44.000Z","updated":"2018-09-30T07:13:41.820Z","comments":true,"path":"2018/2018090514/","link":"","permalink":"http://fangzh.top/2018/2018090514/","excerpt":"","text":"喜欢写Blog的人，会经历三个阶段。 第一阶段，刚接触Blog，觉得很新鲜，试着选择一个免费空间来写。 第二阶段，发现免费空间限制太多，就自己购买域名和空间，搭建独立博客。 第三阶段，觉得独立博客的管理太麻烦，最好在保留控制权的前提下，让别人来管，自己只负责写文章。 ——阮一峰 现在市面上的博客很多，如CSDN，博客园，简书等平台，可以直接在上面发表，用户交互做的好，写的文章百度也能搜索的到。缺点是比较不自由，会受到平台的各种限制和恶心的广告。 而自己购买域名和服务器，搭建博客的成本实在是太高了，不光是说这些购买成本，单单是花力气去自己搭这么一个网站，还要定期的维护它，对于我们大多数人来说，实在是没有这样的精力和时间。 那么就有第三种选择，直接在github page平台上托管我们的博客。这样就可以安心的来写作，又不需要定期维护，而且hexo作为一个快速简洁的博客框架，用它来搭建博客真的非常容易。 Hexo简介Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。大家可以进入hexo官网进行详细查看，因为Hexo的创建者是台湾人，对中文的支持很友好，可以选择中文进行查看。 Hexo搭建步骤 安装Git 安装Node.js 安装Hexo GitHub创建个人仓库 生成SSH添加到GitHub 将hexo部署到GitHub 设置个人域名 发布文章 1. 安装GitGit是目前世界上最先进的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。也就是用来管理你的hexo博客文章，上传到GitHub的工具。Git非常强大，我觉得建议每个人都去了解一下。廖雪峰老师的Git教程写的非常好，大家可以了解一下。Git教程 windows：到git官网上下载,Download git,下载后会有一个Git Bash的命令行工具，以后就用这个工具来使用git。 linux：对linux来说实在是太简单了，因为最早的git就是在linux上编写的，只需要一行代码 1sudo apt-get install git 安装好后，用git --version 来查看一下版本 2. 安装nodejsHexo是基于nodeJS编写的，所以需要安装一下nodeJs和里面的npm工具。 windows：nodejs选择LTS版本就行了。 linux：12sudo apt-get install nodejssudo apt-get install npm 安装完后，打开命令行12node -vnpm -v 检查一下有没有安装成功 顺便说一下，windows在git安装完后，就可以直接使用git bash来敲命令行了，不用自带的cmd，cmd有点难用。 3. 安装hexo前面git和nodejs安装好后，就可以安装hexo了，你可以先创建一个文件夹blog，然后cd到这个文件夹下（或者在这个文件夹下直接右键git bash打开）。 输入命令 1npm install -g hexo-cli 依旧用hexo -v查看一下版本 至此就全部安装完了。 接下来初始化一下hexo 1hexo init myblog 这个myblog可以自己取什么名字都行，然后12cd myblog //进入这个myblog文件夹npm install 新建完成后，指定文件夹目录下有： node_modules: 依赖包 public：存放生成的页面 scaffolds：生成文章的一些模板 source：用来存放你的文章 themes：主题 _config.yml: 博客的配置文件 12hexo ghexo server 打开hexo的服务，在浏览器输入localhost:4000就可以看到你生成的博客了。 大概长这样：使用ctrl+c可以把服务关掉。 4. GitHub创建个人仓库首先，你先要有一个GitHub账户，去注册一个吧。 注册完登录后，在GitHub.com中看到一个New repository，新建仓库 创建一个和你用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHub page的时候，才会被识别，也就是xxxx.github.io，其中xxx就是你注册GitHub的用户名。我这里是已经建过了。 点击create repository。 5. 生成SSH添加到GitHub回到你的git bash中，12git config --global user.name &quot;yourname&quot;git config --global user.email &quot;youremail&quot; 这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。 可以用以下两条，检查一下你有没有输对12git config user.namegit config user.email 然后创建SSH,一路回车1ssh-keygen -t rsa -C &quot;youremail&quot; 这个时候它会告诉你已经生成了.ssh的文件夹。在你的电脑中找到这个文件夹。 ssh，简单来讲，就是一个秘钥，其中，id_rsa是你这台电脑的私人秘钥，不能给别人看的，id_rsa.pub是公共秘钥，可以随便给别人看。把这个公钥放在GitHub上，这样当你链接GitHub自己的账户时，它就会根据公钥匹配你的私钥，当能够相互匹配时，才能够顺利的通过git上传你的文件到GitHub上。 而后在GitHub的setting中，找到SSH keys的设置选项，点击New SSH key把你的id_rsa.pub里面的信息复制进去。 在gitbash中，查看是否成功1ssh -T git@github.com 6. 将hexo部署到GitHub这一步，我们就可以将hexo和GitHub关联起来，也就是将hexo生成的文章部署到GitHub上，打开站点配置文件 _config.yml，翻到最后，修改为YourgithubName就是你的GitHub账户1234deploy: type: git repo: https://github.com/YourgithubName/YourgithubName.github.io.git branch: master 这个时候需要先安装deploy-git ，也就是部署的命令,这样你才能用命令部署到GitHub。1npm install hexo-deployer-git --save 然后123hexo cleanhexo generatehexo deploy 其中 hexo clean清除了你之前生成的东西，也可以不加。hexo generate 顾名思义，生成静态文章，可以用 hexo g缩写hexo deploy 部署文章，可以用hexo d缩写 注意deploy时可能要你输入username和password。 得到下图就说明部署成功了，过一会儿就可以在http://yourname.github.io 这个网站看到你的博客了！！ 7. 设置个人域名现在你的个人网站的地址是 yourname.github.io，如果觉得这个网址逼格不太够，这就需要你设置个人域名了。但是需要花钱。 注册一个阿里云账户,在阿里云上买一个域名，我买的是 fangzh.top，各个后缀的价格不太一样，比如最广泛的.com就比较贵，看个人喜好咯。 你需要先去进行实名认证,然后在域名控制台中，看到你购买的域名。 点解析进去，添加解析。 其中，192.30.252.153 和 192.30.252.154 是GitHub的服务器地址。注意，解析线路选择默认，不要像我一样选境外。这个境外是后面来做国内外分流用的,在后面的博客中会讲到。记得现在选择默认！！ 登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入你的域名fangzh.top 然后在你的博客文件source中创建一个名为CNAME文件，不要后缀。写上你的域名。 最后，在gitbash中，输入123hexo cleanhexo ghexo d 过不了多久，再打开你的浏览器，输入你自己的域名，就可以看到搭建的网站啦！ 接下来你就可以正式开始写文章了。 1hexo new newpapername 然后在source/_post中打开markdown文件，就可以开始编辑了。当你写完的时候，再123hexo cleanhexo ghexo d 就可以看到更新了。 至于更换网站主题，还有添加各种各样的功能等等，在往后的系列博客中，再进行介绍。","categories":[{"name":"日常技术","slug":"日常技术","permalink":"http://fangzh.top/categories/日常技术/"},{"name":"博客搭建","slug":"日常技术/博客搭建","permalink":"http://fangzh.top/categories/日常技术/博客搭建/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://fangzh.top/tags/hexo/"},{"name":"blog","slug":"blog","permalink":"http://fangzh.top/tags/blog/"},{"name":"教程","slug":"教程","permalink":"http://fangzh.top/tags/教程/"}]},{"title":"开篇博客","slug":"开篇博客","date":"2018-09-04T05:38:44.000Z","updated":"2018-09-30T07:02:41.652Z","comments":true,"path":"2018/2018090413/","link":"","permalink":"http://fangzh.top/2018/2018090413/","excerpt":"开个博客，写写东西。","text":"开个博客，写写东西。 很早之前就想建一个博客来写点东西，毕竟已经很久没有写过文章了，所以现在再提笔显得十分生涩，不像几年前那样能随意自在。 花了三天时间搭建了这个博客，此前原以为很复杂的，没有想到这么快就能搭好，一方面是自己的技术水平提升，很多以前不懂的技术，在这半年的自学以来都慢慢入门，另一方面，其实这就是一层窗户纸，对于有勇气的人来说，很快就能搭好了，因为困难总是可以解决的，怕的是畏惧这些困难。 中国的教育总是让你好好学习，其他的都不要管，好好高考，考个好大学。然而并没有教你应该做一个什么样的人，长大了应该做什么。所以我们大多数人在高考填志愿的时候并不知道要填什么专业，就这样稀里糊涂的跟着大部队走了。再者上了大学，也没有人告诉你接下来要做什么，所以中国的大学教育是极其失败的，很多人都是白白浪费了4年的光阴。 有很长一段时间，我都不知道自己要做什么。看着眼前的一个个项目，啥事也不想干，只能水水手机，水水b站，根本就没有想要干活的欲望。加上老师也不管事，就这样又水过了很多时间。不过人生总是有这么些时候的，能够及时纠正自己，就能更远的前行。 自学人工智能，机器学习等知识也有半年了，我知道很多人都在往这方向转，我是觉得人工智能挺有趣的，能够拯救世界。而且很多半路转行的人，今年找工作的时候都被刷了，算法岗一抓一大把，每个人都想进来，就连我那些很厉害的CS同学也在为找工作而焦虑。 不过不管怎么样，好好提高自己的技术水平和能力，总是不错的，不管之后能找到什么样的工作，慢慢变成一个更厉害的人，不要惶惶然无所事事，就能对得起自己。 搭建博客是很简单的，但是想坚持下来就没那么容易了。立个flag，希望自己能够坚持写博客，发文章，不管是技术教程，还是解决了哪一些问题，亦或是旅途中的美景、人生感悟都可以，希望能坚持到毕业。 一起加油吧。年轻的时候，看到一座山，总想知道山后面是什么。其实翻过山后，山的后面并没有什么特别的，但是我依然会去越过山丘，因为在越过山丘的过程中看到的风景，才是最重要的。","categories":[{"name":"生活","slug":"生活","permalink":"http://fangzh.top/categories/生活/"},{"name":"随笔","slug":"生活/随笔","permalink":"http://fangzh.top/categories/生活/随笔/"}],"tags":[{"name":"生活","slug":"生活","permalink":"http://fangzh.top/tags/生活/"}]}]}